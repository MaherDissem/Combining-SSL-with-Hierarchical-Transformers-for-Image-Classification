{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeUZnG_A79Gu",
        "outputId": "6522cef8-dd60-4e99-88ca-ebc065b4f2af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XotYh46j8wGg",
        "outputId": "c4fd80ad-d943-4041-bdbb-c7416e12c8f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Jan  8 14:52:46 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#Runtime -> Change runtime type-> GPU\n",
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yywKJFbS85-p",
        "outputId": "943f0874-d3f7-4145-b8f7-8be50df4ae65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'BYOL-ViT'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 69 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (69/69), done.\n",
            "Cloning into 'BYOL'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
            "remote: Total 85 (delta 26), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (85/85), done.\n"
          ]
        }
      ],
      "source": [
        "# code\n",
        "!git clone https://github.com/SafwenNaimi/BYOL-ViT.git\n",
        "!git clone https://github.com/SafwenNaimi/BYOL.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset downloading and preprocessing"
      ],
      "metadata": {
        "id": "IjXgMAZ5PSzE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kbQjRYnkY7p"
      },
      "outputs": [],
      "source": [
        "# STL dataset download\n",
        "!git clone https://github.com/mttk/STL10.git # run stl10_input.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fojM7wMhkeZ2"
      },
      "outputs": [],
      "source": [
        "!python /content/drive/MyDrive/BYOL-ViT-Hourglass/STL10/stl10_input.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBA-swEnnnfD"
      },
      "outputs": [],
      "source": [
        "!python /content/drive/MyDrive/BYOL-ViT-Hourglass/data_preprocessing.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BYOL training"
      ],
      "metadata": {
        "id": "0-nHvh01Poco"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aMhzQunrhtS",
        "outputId": "ee75acac-c74f-4950-b9b5-8eda5b06a7fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Step [2/275]:\tLoss: 1.5143251419067383\n",
            "Step [3/275]:\tLoss: 2.1860415935516357\n",
            "Step [4/275]:\tLoss: 1.4601454734802246\n",
            "Step [5/275]:\tLoss: 1.5380306243896484\n",
            "Step [6/275]:\tLoss: 1.5735058784484863\n",
            "Step [7/275]:\tLoss: 1.160517930984497\n",
            "Step [8/275]:\tLoss: 1.0158894062042236\n",
            "Step [9/275]:\tLoss: 1.897087812423706\n",
            "Step [10/275]:\tLoss: 0.776131272315979\n",
            "Step [11/275]:\tLoss: 2.1397247314453125\n",
            "Step [12/275]:\tLoss: 1.9997661113739014\n",
            "Step [13/275]:\tLoss: 2.0151329040527344\n",
            "Step [14/275]:\tLoss: 1.8773854970932007\n",
            "Step [15/275]:\tLoss: 1.2764887809753418\n",
            "Step [16/275]:\tLoss: 1.0323164463043213\n",
            "Step [17/275]:\tLoss: 1.667266845703125\n",
            "Step [18/275]:\tLoss: 1.629448652267456\n",
            "Step [19/275]:\tLoss: 1.3195356130599976\n",
            "Step [20/275]:\tLoss: 1.538228988647461\n",
            "Step [21/275]:\tLoss: 1.7641196250915527\n",
            "Step [22/275]:\tLoss: 1.3963326215744019\n",
            "Step [23/275]:\tLoss: 1.4958974123001099\n",
            "Step [24/275]:\tLoss: 1.3914896249771118\n",
            "Step [25/275]:\tLoss: 1.5699737071990967\n",
            "Step [26/275]:\tLoss: 1.0496615171432495\n",
            "Step [27/275]:\tLoss: 0.9022349119186401\n",
            "Step [28/275]:\tLoss: 1.4243707656860352\n",
            "Step [29/275]:\tLoss: 0.9482274651527405\n",
            "Step [30/275]:\tLoss: 1.1492695808410645\n",
            "Step [31/275]:\tLoss: 1.3637531995773315\n",
            "Step [32/275]:\tLoss: 1.1815037727355957\n",
            "Step [33/275]:\tLoss: 1.3530540466308594\n",
            "Step [34/275]:\tLoss: 1.1690576076507568\n",
            "Step [35/275]:\tLoss: 0.5783519744873047\n",
            "Step [36/275]:\tLoss: 1.3132160902023315\n",
            "Step [37/275]:\tLoss: 1.2404329776763916\n",
            "Step [38/275]:\tLoss: 1.0269317626953125\n",
            "Step [39/275]:\tLoss: 1.3650119304656982\n",
            "Step [40/275]:\tLoss: 0.8531084060668945\n",
            "Step [41/275]:\tLoss: 1.472978115081787\n",
            "Step [42/275]:\tLoss: 0.6816725134849548\n",
            "Step [43/275]:\tLoss: 1.4887752532958984\n",
            "Step [44/275]:\tLoss: 0.47754794359207153\n",
            "Step [45/275]:\tLoss: 1.2338403463363647\n",
            "Step [46/275]:\tLoss: 1.1887340545654297\n",
            "Step [47/275]:\tLoss: 0.8446443676948547\n",
            "Step [48/275]:\tLoss: 0.8477331399917603\n",
            "Step [49/275]:\tLoss: 1.5233319997787476\n",
            "Step [50/275]:\tLoss: 1.0457252264022827\n",
            "Step [51/275]:\tLoss: 0.8619924187660217\n",
            "Step [52/275]:\tLoss: 0.7086213827133179\n",
            "Step [53/275]:\tLoss: 1.3405085802078247\n",
            "Step [54/275]:\tLoss: 0.9265098571777344\n",
            "Step [55/275]:\tLoss: 1.2726507186889648\n",
            "Step [56/275]:\tLoss: 0.539230465888977\n",
            "Step [57/275]:\tLoss: 1.7357230186462402\n",
            "Step [58/275]:\tLoss: 1.4289965629577637\n",
            "Step [59/275]:\tLoss: 2.0119268894195557\n",
            "Step [60/275]:\tLoss: 1.1041336059570312\n",
            "Step [61/275]:\tLoss: 0.5975139141082764\n",
            "Step [62/275]:\tLoss: 1.4429171085357666\n",
            "Step [63/275]:\tLoss: 1.0450940132141113\n",
            "Step [64/275]:\tLoss: 0.7368737459182739\n",
            "Step [65/275]:\tLoss: 1.1648472547531128\n",
            "Step [66/275]:\tLoss: 1.448261022567749\n",
            "Step [67/275]:\tLoss: 0.699386715888977\n",
            "Step [68/275]:\tLoss: 0.6697287559509277\n",
            "Step [69/275]:\tLoss: 1.0919227600097656\n",
            "Step [70/275]:\tLoss: 1.428408145904541\n",
            "Step [71/275]:\tLoss: 1.3747777938842773\n",
            "Step [72/275]:\tLoss: 0.7972285151481628\n",
            "Step [73/275]:\tLoss: 0.5215364694595337\n",
            "Step [74/275]:\tLoss: 1.389618158340454\n",
            "Step [75/275]:\tLoss: 1.5653870105743408\n",
            "Step [76/275]:\tLoss: 1.4167273044586182\n",
            "Step [77/275]:\tLoss: 1.0116748809814453\n",
            "Step [78/275]:\tLoss: 1.6556816101074219\n",
            "Step [79/275]:\tLoss: 0.8355505466461182\n",
            "Step [80/275]:\tLoss: 1.0518403053283691\n",
            "Step [81/275]:\tLoss: 1.3337301015853882\n",
            "Step [82/275]:\tLoss: 1.4863837957382202\n",
            "Step [83/275]:\tLoss: 1.4015893936157227\n",
            "Step [84/275]:\tLoss: 1.27261483669281\n",
            "Step [85/275]:\tLoss: 0.676179051399231\n",
            "Step [86/275]:\tLoss: 1.5439984798431396\n",
            "Step [87/275]:\tLoss: 0.7032173871994019\n",
            "Step [88/275]:\tLoss: 0.8347469568252563\n",
            "Step [89/275]:\tLoss: 0.534981369972229\n",
            "Step [90/275]:\tLoss: 0.8924391269683838\n",
            "Step [91/275]:\tLoss: 1.5192780494689941\n",
            "Step [92/275]:\tLoss: 0.44231221079826355\n",
            "Step [93/275]:\tLoss: 1.182157278060913\n",
            "Step [94/275]:\tLoss: 1.0115399360656738\n",
            "Step [95/275]:\tLoss: 0.7690001726150513\n",
            "Step [96/275]:\tLoss: 1.738486647605896\n",
            "Step [97/275]:\tLoss: 1.4285476207733154\n",
            "Step [98/275]:\tLoss: 0.8147991299629211\n",
            "Step [99/275]:\tLoss: 1.4628353118896484\n",
            "Step [100/275]:\tLoss: 0.8595395088195801\n",
            "Step [101/275]:\tLoss: 0.8315403461456299\n",
            "Step [102/275]:\tLoss: 0.26425519585609436\n",
            "Step [103/275]:\tLoss: 1.4241740703582764\n",
            "Step [104/275]:\tLoss: 0.5999524593353271\n",
            "Step [105/275]:\tLoss: 0.7232788801193237\n",
            "Step [106/275]:\tLoss: 0.5838278532028198\n",
            "Step [107/275]:\tLoss: 1.3934340476989746\n",
            "Step [108/275]:\tLoss: 0.7979018688201904\n",
            "Step [109/275]:\tLoss: 1.1942646503448486\n",
            "Step [110/275]:\tLoss: 1.8915443420410156\n",
            "Step [111/275]:\tLoss: 1.6522293090820312\n",
            "Step [112/275]:\tLoss: 2.136216878890991\n",
            "Step [113/275]:\tLoss: 2.032122850418091\n",
            "Step [114/275]:\tLoss: 2.556269645690918\n",
            "Step [115/275]:\tLoss: 1.6737966537475586\n",
            "Step [116/275]:\tLoss: 1.9000180959701538\n",
            "Step [117/275]:\tLoss: 2.4719462394714355\n",
            "Step [118/275]:\tLoss: 1.951155185699463\n",
            "Step [119/275]:\tLoss: 0.9422856569290161\n",
            "Step [120/275]:\tLoss: 1.4000768661499023\n",
            "Step [121/275]:\tLoss: 0.7468776106834412\n",
            "Step [122/275]:\tLoss: 1.706525206565857\n",
            "Step [123/275]:\tLoss: 1.6629688739776611\n",
            "Step [124/275]:\tLoss: 2.423621654510498\n",
            "Step [125/275]:\tLoss: 1.7169063091278076\n",
            "Step [126/275]:\tLoss: 1.2835350036621094\n",
            "Step [127/275]:\tLoss: 1.779679775238037\n",
            "Step [128/275]:\tLoss: 0.9711305499076843\n",
            "Step [129/275]:\tLoss: 2.648677349090576\n",
            "Step [130/275]:\tLoss: 2.027968168258667\n",
            "Step [131/275]:\tLoss: 1.4661880731582642\n",
            "Step [132/275]:\tLoss: 1.4090712070465088\n",
            "Step [133/275]:\tLoss: 2.33722186088562\n",
            "Step [134/275]:\tLoss: 1.3790074586868286\n",
            "Step [135/275]:\tLoss: 1.6485176086425781\n",
            "Step [136/275]:\tLoss: 1.8531262874603271\n",
            "Step [137/275]:\tLoss: 2.268667221069336\n",
            "Step [138/275]:\tLoss: 1.3423510789871216\n",
            "Step [139/275]:\tLoss: 0.8257668018341064\n",
            "Step [140/275]:\tLoss: 0.9732123613357544\n",
            "Step [141/275]:\tLoss: 0.8834248781204224\n",
            "Step [142/275]:\tLoss: 0.526849091053009\n",
            "Step [143/275]:\tLoss: 1.283517599105835\n",
            "Step [144/275]:\tLoss: 1.1522800922393799\n",
            "Step [145/275]:\tLoss: 1.662168025970459\n",
            "Step [146/275]:\tLoss: 1.0096704959869385\n",
            "Step [147/275]:\tLoss: 0.51952064037323\n",
            "Step [148/275]:\tLoss: 0.6098053455352783\n",
            "Step [149/275]:\tLoss: 1.1443473100662231\n",
            "Step [150/275]:\tLoss: 1.3578418493270874\n",
            "Step [151/275]:\tLoss: 1.1328601837158203\n",
            "Step [152/275]:\tLoss: 1.2922333478927612\n",
            "Step [153/275]:\tLoss: 0.7387942671775818\n",
            "Step [154/275]:\tLoss: 1.1032028198242188\n",
            "Step [155/275]:\tLoss: 1.1235259771347046\n",
            "Step [156/275]:\tLoss: 1.3049112558364868\n",
            "Step [157/275]:\tLoss: 0.7822927236557007\n",
            "Step [158/275]:\tLoss: 0.9669432044029236\n",
            "Step [159/275]:\tLoss: 1.0978400707244873\n",
            "Step [160/275]:\tLoss: 0.48499220609664917\n",
            "Step [161/275]:\tLoss: 1.12548828125\n",
            "Step [162/275]:\tLoss: 0.8205090761184692\n",
            "Step [163/275]:\tLoss: 0.6116518378257751\n",
            "Step [164/275]:\tLoss: 1.2502363920211792\n",
            "Step [165/275]:\tLoss: 1.4949569702148438\n",
            "Step [166/275]:\tLoss: 1.6609443426132202\n",
            "Step [167/275]:\tLoss: 1.6228554248809814\n",
            "Step [168/275]:\tLoss: 1.194415807723999\n",
            "Step [169/275]:\tLoss: 0.6329572200775146\n",
            "Step [170/275]:\tLoss: 1.279952049255371\n",
            "Step [171/275]:\tLoss: 0.9318990111351013\n",
            "Step [172/275]:\tLoss: 1.3843705654144287\n",
            "Step [173/275]:\tLoss: 0.5585090517997742\n",
            "Step [174/275]:\tLoss: 1.3439178466796875\n",
            "Step [175/275]:\tLoss: 2.1786530017852783\n",
            "Step [176/275]:\tLoss: 1.3279838562011719\n",
            "Step [177/275]:\tLoss: 0.4581700265407562\n",
            "Step [178/275]:\tLoss: 1.2574021816253662\n",
            "Step [179/275]:\tLoss: 0.9592468738555908\n",
            "Step [180/275]:\tLoss: 0.7999756336212158\n",
            "Step [181/275]:\tLoss: 0.6809059381484985\n",
            "Step [182/275]:\tLoss: 0.8123872876167297\n",
            "Step [183/275]:\tLoss: 0.6628338098526001\n",
            "Step [184/275]:\tLoss: 0.6550185680389404\n",
            "Step [185/275]:\tLoss: 1.6426596641540527\n",
            "Step [186/275]:\tLoss: 1.4107444286346436\n",
            "Step [187/275]:\tLoss: 1.1079301834106445\n",
            "Step [188/275]:\tLoss: 0.6877697706222534\n",
            "Step [189/275]:\tLoss: 1.119689702987671\n",
            "Step [190/275]:\tLoss: 1.4020665884017944\n",
            "Step [191/275]:\tLoss: 1.4154361486434937\n",
            "Step [192/275]:\tLoss: 2.503533363342285\n",
            "Step [193/275]:\tLoss: 2.1411542892456055\n",
            "Step [194/275]:\tLoss: 1.9505648612976074\n",
            "Step [195/275]:\tLoss: 1.3336646556854248\n",
            "Step [196/275]:\tLoss: 1.93967604637146\n",
            "Step [197/275]:\tLoss: 1.7048684358596802\n",
            "Step [198/275]:\tLoss: 1.345656394958496\n",
            "Step [199/275]:\tLoss: 1.907168984413147\n",
            "Step [200/275]:\tLoss: 2.3952465057373047\n",
            "Step [201/275]:\tLoss: 2.0629403591156006\n",
            "Step [202/275]:\tLoss: 1.988293170928955\n",
            "Step [203/275]:\tLoss: 2.227813243865967\n",
            "Step [204/275]:\tLoss: 1.7197718620300293\n",
            "Step [205/275]:\tLoss: 1.8863780498504639\n",
            "Step [206/275]:\tLoss: 2.0084004402160645\n",
            "Step [207/275]:\tLoss: 1.515976905822754\n",
            "Step [208/275]:\tLoss: 1.772862195968628\n",
            "Step [209/275]:\tLoss: 1.6983145475387573\n",
            "Step [210/275]:\tLoss: 1.823807716369629\n",
            "Step [211/275]:\tLoss: 1.5645197629928589\n",
            "Step [212/275]:\tLoss: 1.9003281593322754\n",
            "Step [213/275]:\tLoss: 1.4276762008666992\n",
            "Step [214/275]:\tLoss: 1.3673796653747559\n",
            "Step [215/275]:\tLoss: 1.8419032096862793\n",
            "Step [216/275]:\tLoss: 1.0325819253921509\n",
            "Step [217/275]:\tLoss: 1.8433960676193237\n",
            "Step [218/275]:\tLoss: 2.132514238357544\n",
            "Step [219/275]:\tLoss: 1.4504917860031128\n",
            "Step [220/275]:\tLoss: 1.8421579599380493\n",
            "Step [221/275]:\tLoss: 1.6643962860107422\n",
            "Step [222/275]:\tLoss: 1.7222859859466553\n",
            "Step [223/275]:\tLoss: 1.9048430919647217\n",
            "Step [224/275]:\tLoss: 1.4763848781585693\n",
            "Step [225/275]:\tLoss: 1.3416392803192139\n",
            "Step [226/275]:\tLoss: 1.6290373802185059\n",
            "Step [227/275]:\tLoss: 2.000706195831299\n",
            "Step [228/275]:\tLoss: 1.3219107389450073\n",
            "Step [229/275]:\tLoss: 1.353367567062378\n",
            "Step [230/275]:\tLoss: 1.3711154460906982\n",
            "Step [231/275]:\tLoss: 2.3972158432006836\n",
            "Step [232/275]:\tLoss: 1.4380296468734741\n",
            "Step [233/275]:\tLoss: 2.0634565353393555\n",
            "Step [234/275]:\tLoss: 2.1266844272613525\n",
            "Step [235/275]:\tLoss: 2.468222141265869\n",
            "Step [236/275]:\tLoss: 1.6138561964035034\n",
            "Step [237/275]:\tLoss: 1.6641855239868164\n",
            "Step [238/275]:\tLoss: 1.5829601287841797\n",
            "Step [239/275]:\tLoss: 2.3539316654205322\n",
            "Step [240/275]:\tLoss: 1.9854912757873535\n",
            "Step [241/275]:\tLoss: 1.4950523376464844\n",
            "Step [242/275]:\tLoss: 1.7617871761322021\n",
            "Step [243/275]:\tLoss: 2.1013152599334717\n",
            "Step [244/275]:\tLoss: 1.625565528869629\n",
            "Step [245/275]:\tLoss: 1.6177692413330078\n",
            "Step [246/275]:\tLoss: 1.7330694198608398\n",
            "Step [247/275]:\tLoss: 2.0040454864501953\n",
            "Step [248/275]:\tLoss: 1.5274235010147095\n",
            "Step [249/275]:\tLoss: 1.6632664203643799\n",
            "Step [250/275]:\tLoss: 1.3573663234710693\n",
            "Step [251/275]:\tLoss: 1.9266350269317627\n",
            "Step [252/275]:\tLoss: 1.7969366312026978\n",
            "Step [253/275]:\tLoss: 2.223602294921875\n",
            "Step [254/275]:\tLoss: 1.0777978897094727\n",
            "Step [255/275]:\tLoss: 1.4714627265930176\n",
            "Step [256/275]:\tLoss: 2.061544179916382\n",
            "Step [257/275]:\tLoss: 1.7650985717773438\n",
            "Step [258/275]:\tLoss: 1.5444679260253906\n",
            "Step [259/275]:\tLoss: 1.9346531629562378\n",
            "Step [260/275]:\tLoss: 1.365283727645874\n",
            "Step [261/275]:\tLoss: 1.5832781791687012\n",
            "Step [262/275]:\tLoss: 1.456512689590454\n",
            "Step [263/275]:\tLoss: 1.603114366531372\n",
            "Step [264/275]:\tLoss: 1.6875073909759521\n",
            "Step [265/275]:\tLoss: 1.685218095779419\n",
            "Step [266/275]:\tLoss: 1.7454888820648193\n",
            "Step [267/275]:\tLoss: 2.0450034141540527\n",
            "Step [268/275]:\tLoss: 1.0556566715240479\n",
            "Step [269/275]:\tLoss: 1.747688889503479\n",
            "Step [270/275]:\tLoss: 1.6880412101745605\n",
            "Step [271/275]:\tLoss: 1.4782114028930664\n",
            "Step [272/275]:\tLoss: 1.622771978378296\n",
            "Step [273/275]:\tLoss: 1.4766054153442383\n",
            "Step [274/275]:\tLoss: 1.7865875959396362\n",
            "epoch number 96\n",
            "Step [0/275]:\tLoss: 1.7310659885406494\n",
            "Step [1/275]:\tLoss: 2.0098984241485596\n",
            "Step [2/275]:\tLoss: 1.508232831954956\n",
            "Step [3/275]:\tLoss: 1.374145746231079\n",
            "Step [4/275]:\tLoss: 1.709254264831543\n",
            "Step [5/275]:\tLoss: 2.719496965408325\n",
            "Step [6/275]:\tLoss: 1.733105182647705\n",
            "Step [7/275]:\tLoss: 2.3604073524475098\n",
            "Step [8/275]:\tLoss: 1.3875991106033325\n",
            "Step [9/275]:\tLoss: 1.372974157333374\n",
            "Step [10/275]:\tLoss: 1.4731419086456299\n",
            "Step [11/275]:\tLoss: 1.9798126220703125\n",
            "Step [12/275]:\tLoss: 1.1987555027008057\n",
            "Step [13/275]:\tLoss: 1.6561609506607056\n",
            "Step [14/275]:\tLoss: 1.4131078720092773\n",
            "Step [15/275]:\tLoss: 1.7366136312484741\n",
            "Step [16/275]:\tLoss: 1.452770471572876\n",
            "Step [17/275]:\tLoss: 1.1825987100601196\n",
            "Step [18/275]:\tLoss: 1.845241904258728\n",
            "Step [19/275]:\tLoss: 1.5538537502288818\n",
            "Step [20/275]:\tLoss: 1.515345811843872\n",
            "Step [21/275]:\tLoss: 1.4768973588943481\n",
            "Step [22/275]:\tLoss: 1.3952596187591553\n",
            "Step [23/275]:\tLoss: 1.5729179382324219\n",
            "Step [24/275]:\tLoss: 1.6403898000717163\n",
            "Step [25/275]:\tLoss: 1.9810963869094849\n",
            "Step [26/275]:\tLoss: 1.9814502000808716\n",
            "Step [27/275]:\tLoss: 1.625298023223877\n",
            "Step [28/275]:\tLoss: 2.0516254901885986\n",
            "Step [29/275]:\tLoss: 1.6811573505401611\n",
            "Step [30/275]:\tLoss: 1.7002506256103516\n",
            "Step [31/275]:\tLoss: 2.225343942642212\n",
            "Step [32/275]:\tLoss: 1.928244709968567\n",
            "Step [33/275]:\tLoss: 1.6068698167800903\n",
            "Step [34/275]:\tLoss: 1.3655500411987305\n",
            "Step [35/275]:\tLoss: 2.3154871463775635\n",
            "Step [36/275]:\tLoss: 1.669919729232788\n",
            "Step [37/275]:\tLoss: 1.1284013986587524\n",
            "Step [38/275]:\tLoss: 1.36598539352417\n",
            "Step [39/275]:\tLoss: 0.6999838948249817\n",
            "Step [40/275]:\tLoss: 1.2577871084213257\n",
            "Step [41/275]:\tLoss: 1.4093793630599976\n",
            "Step [42/275]:\tLoss: 1.5761862993240356\n",
            "Step [43/275]:\tLoss: 1.7567402124404907\n",
            "Step [44/275]:\tLoss: 0.8227106332778931\n",
            "Step [45/275]:\tLoss: 0.8261076211929321\n",
            "Step [46/275]:\tLoss: 2.5600357055664062\n",
            "Step [47/275]:\tLoss: 1.8172610998153687\n",
            "Step [48/275]:\tLoss: 0.7659159302711487\n",
            "Step [49/275]:\tLoss: 0.5161296725273132\n",
            "Step [50/275]:\tLoss: 0.6507509350776672\n",
            "Step [51/275]:\tLoss: 1.084883213043213\n",
            "Step [52/275]:\tLoss: 2.1935806274414062\n",
            "Step [53/275]:\tLoss: 1.489650011062622\n",
            "Step [54/275]:\tLoss: 2.0094447135925293\n",
            "Step [55/275]:\tLoss: 1.1111524105072021\n",
            "Step [56/275]:\tLoss: 1.4290592670440674\n",
            "Step [57/275]:\tLoss: 1.4236093759536743\n",
            "Step [58/275]:\tLoss: 1.5369751453399658\n",
            "Step [59/275]:\tLoss: 1.077026605606079\n",
            "Step [60/275]:\tLoss: 1.0341322422027588\n",
            "Step [61/275]:\tLoss: 0.973788857460022\n",
            "Step [62/275]:\tLoss: 2.158392906188965\n",
            "Step [63/275]:\tLoss: 1.6576831340789795\n",
            "Step [64/275]:\tLoss: 0.8645338416099548\n",
            "Step [65/275]:\tLoss: 1.347635269165039\n",
            "Step [66/275]:\tLoss: 0.7234805226325989\n",
            "Step [67/275]:\tLoss: 1.466354250907898\n",
            "Step [68/275]:\tLoss: 1.356564998626709\n",
            "Step [69/275]:\tLoss: 1.4894434213638306\n",
            "Step [70/275]:\tLoss: 1.6630580425262451\n",
            "Step [71/275]:\tLoss: 2.479918956756592\n",
            "Step [72/275]:\tLoss: 1.1079572439193726\n",
            "Step [73/275]:\tLoss: 1.699847936630249\n",
            "Step [74/275]:\tLoss: 1.537516474723816\n",
            "Step [75/275]:\tLoss: 0.9215503334999084\n",
            "Step [76/275]:\tLoss: 1.632232666015625\n",
            "Step [77/275]:\tLoss: 1.5216820240020752\n",
            "Step [78/275]:\tLoss: 1.3122014999389648\n",
            "Step [79/275]:\tLoss: 0.827914834022522\n",
            "Step [80/275]:\tLoss: 1.7788797616958618\n",
            "Step [81/275]:\tLoss: 1.3606218099594116\n",
            "Step [82/275]:\tLoss: 1.8335824012756348\n",
            "Step [83/275]:\tLoss: 0.6644776463508606\n",
            "Step [84/275]:\tLoss: 0.6760191917419434\n",
            "Step [85/275]:\tLoss: 1.0129916667938232\n",
            "Step [86/275]:\tLoss: 1.235321044921875\n",
            "Step [87/275]:\tLoss: 1.3766878843307495\n",
            "Step [88/275]:\tLoss: 1.4258086681365967\n",
            "Step [89/275]:\tLoss: 1.123453140258789\n",
            "Step [90/275]:\tLoss: 0.8911413550376892\n",
            "Step [91/275]:\tLoss: 1.3274102210998535\n",
            "Step [92/275]:\tLoss: 1.4936445951461792\n",
            "Step [93/275]:\tLoss: 1.2852215766906738\n",
            "Step [94/275]:\tLoss: 1.8193092346191406\n",
            "Step [95/275]:\tLoss: 0.8179529905319214\n",
            "Step [96/275]:\tLoss: 1.6593677997589111\n",
            "Step [97/275]:\tLoss: 1.0614272356033325\n",
            "Step [98/275]:\tLoss: 1.1947925090789795\n",
            "Step [99/275]:\tLoss: 1.0306771993637085\n",
            "Step [100/275]:\tLoss: 1.1723265647888184\n",
            "Step [101/275]:\tLoss: 1.7662842273712158\n",
            "Step [102/275]:\tLoss: 1.702557921409607\n",
            "Step [103/275]:\tLoss: 0.8872655630111694\n",
            "Step [104/275]:\tLoss: 0.9033292531967163\n",
            "Step [105/275]:\tLoss: 1.0433040857315063\n",
            "Step [106/275]:\tLoss: 1.1047742366790771\n",
            "Step [107/275]:\tLoss: 1.2617416381835938\n",
            "Step [108/275]:\tLoss: 1.492781400680542\n",
            "Step [109/275]:\tLoss: 0.8632605075836182\n",
            "Step [110/275]:\tLoss: 3.142210006713867\n",
            "Step [111/275]:\tLoss: 2.7545166015625\n",
            "Step [112/275]:\tLoss: 2.878840923309326\n",
            "Step [113/275]:\tLoss: 2.112591505050659\n",
            "Step [114/275]:\tLoss: 1.8308043479919434\n",
            "Step [115/275]:\tLoss: 2.19179630279541\n",
            "Step [116/275]:\tLoss: 1.7119832038879395\n",
            "Step [117/275]:\tLoss: 1.7742646932601929\n",
            "Step [118/275]:\tLoss: 2.170452117919922\n",
            "Step [119/275]:\tLoss: 1.01810884475708\n",
            "Step [120/275]:\tLoss: 2.168478012084961\n",
            "Step [121/275]:\tLoss: 3.0161242485046387\n",
            "Step [122/275]:\tLoss: 1.7592426538467407\n",
            "Step [123/275]:\tLoss: 2.076746702194214\n",
            "Step [124/275]:\tLoss: 1.7792384624481201\n",
            "Step [125/275]:\tLoss: 2.3450121879577637\n",
            "Step [126/275]:\tLoss: 2.8373703956604004\n",
            "Step [127/275]:\tLoss: 2.566589593887329\n",
            "Step [128/275]:\tLoss: 1.8824794292449951\n",
            "Step [129/275]:\tLoss: 1.585533618927002\n",
            "Step [130/275]:\tLoss: 2.1858115196228027\n",
            "Step [131/275]:\tLoss: 1.485546588897705\n",
            "Step [132/275]:\tLoss: 1.8603829145431519\n",
            "Step [133/275]:\tLoss: 1.7384381294250488\n",
            "Step [134/275]:\tLoss: 1.8022593259811401\n",
            "Step [135/275]:\tLoss: 2.2598049640655518\n",
            "Step [136/275]:\tLoss: 2.0591371059417725\n",
            "Step [137/275]:\tLoss: 2.3073747158050537\n",
            "Step [138/275]:\tLoss: 1.3074606657028198\n",
            "Step [139/275]:\tLoss: 1.6681547164916992\n",
            "Step [140/275]:\tLoss: 1.8197073936462402\n",
            "Step [141/275]:\tLoss: 1.1139026880264282\n",
            "Step [142/275]:\tLoss: 2.3982014656066895\n",
            "Step [143/275]:\tLoss: 2.5769591331481934\n",
            "Step [144/275]:\tLoss: 1.4188376665115356\n",
            "Step [145/275]:\tLoss: 1.3337403535842896\n",
            "Step [146/275]:\tLoss: 1.039977788925171\n",
            "Step [147/275]:\tLoss: 1.6521930694580078\n",
            "Step [148/275]:\tLoss: 0.6542835831642151\n",
            "Step [149/275]:\tLoss: 1.560765027999878\n",
            "Step [150/275]:\tLoss: 1.4806628227233887\n",
            "Step [151/275]:\tLoss: 1.4998220205307007\n",
            "Step [152/275]:\tLoss: 1.2668652534484863\n",
            "Step [153/275]:\tLoss: 0.9748257398605347\n",
            "Step [154/275]:\tLoss: 1.2863143682479858\n",
            "Step [155/275]:\tLoss: 0.9246692657470703\n",
            "Step [156/275]:\tLoss: 0.8407485485076904\n",
            "Step [157/275]:\tLoss: 2.3568711280822754\n",
            "Step [158/275]:\tLoss: 1.6018586158752441\n",
            "Step [159/275]:\tLoss: 0.7276980876922607\n",
            "Step [160/275]:\tLoss: 0.7732778787612915\n",
            "Step [161/275]:\tLoss: 1.2992398738861084\n",
            "Step [162/275]:\tLoss: 0.9145779609680176\n",
            "Step [163/275]:\tLoss: 0.6769073009490967\n",
            "Step [164/275]:\tLoss: 1.923844337463379\n",
            "Step [165/275]:\tLoss: 0.8348588943481445\n",
            "Step [166/275]:\tLoss: 2.105598211288452\n",
            "Step [167/275]:\tLoss: 1.8699324131011963\n",
            "Step [168/275]:\tLoss: 1.6067078113555908\n",
            "Step [169/275]:\tLoss: 1.0203819274902344\n",
            "Step [170/275]:\tLoss: 1.3451671600341797\n",
            "Step [171/275]:\tLoss: 0.7742180228233337\n",
            "Step [172/275]:\tLoss: 1.0297274589538574\n",
            "Step [173/275]:\tLoss: 1.3706766366958618\n",
            "Step [174/275]:\tLoss: 1.1925530433654785\n",
            "Step [175/275]:\tLoss: 2.198995590209961\n",
            "Step [176/275]:\tLoss: 1.2424607276916504\n",
            "Step [177/275]:\tLoss: 0.9283337593078613\n",
            "Step [178/275]:\tLoss: 1.7419304847717285\n",
            "Step [179/275]:\tLoss: 1.58048415184021\n",
            "Step [180/275]:\tLoss: 0.9733929634094238\n",
            "Step [181/275]:\tLoss: 1.0129082202911377\n",
            "Step [182/275]:\tLoss: 0.9670239686965942\n",
            "Step [183/275]:\tLoss: 1.3273706436157227\n",
            "Step [184/275]:\tLoss: 1.4505255222320557\n",
            "Step [185/275]:\tLoss: 1.8344035148620605\n",
            "Step [186/275]:\tLoss: 1.4551682472229004\n",
            "Step [187/275]:\tLoss: 1.7420357465744019\n",
            "Step [188/275]:\tLoss: 1.6285820007324219\n",
            "Step [189/275]:\tLoss: 1.9259549379348755\n",
            "Step [190/275]:\tLoss: 1.1895723342895508\n",
            "Step [191/275]:\tLoss: 1.6422600746154785\n",
            "Step [192/275]:\tLoss: 2.4579410552978516\n",
            "Step [193/275]:\tLoss: 2.0212559700012207\n",
            "Step [194/275]:\tLoss: 1.5550564527511597\n",
            "Step [195/275]:\tLoss: 2.4968316555023193\n",
            "Step [196/275]:\tLoss: 2.4856107234954834\n",
            "Step [197/275]:\tLoss: 2.967632293701172\n",
            "Step [198/275]:\tLoss: 2.2504959106445312\n",
            "Step [199/275]:\tLoss: 2.7594428062438965\n",
            "Step [200/275]:\tLoss: 2.1091434955596924\n",
            "Step [201/275]:\tLoss: 1.3268616199493408\n",
            "Step [202/275]:\tLoss: 2.0437588691711426\n",
            "Step [203/275]:\tLoss: 2.3661322593688965\n",
            "Step [204/275]:\tLoss: 2.121969699859619\n",
            "Step [205/275]:\tLoss: 2.454730987548828\n",
            "Step [206/275]:\tLoss: 1.7529444694519043\n",
            "Step [207/275]:\tLoss: 2.0438499450683594\n",
            "Step [208/275]:\tLoss: 2.0725793838500977\n",
            "Step [209/275]:\tLoss: 2.188023567199707\n",
            "Step [210/275]:\tLoss: 2.261385917663574\n",
            "Step [211/275]:\tLoss: 1.5174874067306519\n",
            "Step [212/275]:\tLoss: 1.97938871383667\n",
            "Step [213/275]:\tLoss: 1.612931489944458\n",
            "Step [214/275]:\tLoss: 1.673694133758545\n",
            "Step [215/275]:\tLoss: 1.8027236461639404\n",
            "Step [216/275]:\tLoss: 1.56633460521698\n",
            "Step [217/275]:\tLoss: 1.9035398960113525\n",
            "Step [218/275]:\tLoss: 2.478557586669922\n",
            "Step [219/275]:\tLoss: 1.6644136905670166\n",
            "Step [220/275]:\tLoss: 1.692736029624939\n",
            "Step [221/275]:\tLoss: 1.7389247417449951\n",
            "Step [222/275]:\tLoss: 2.045435905456543\n",
            "Step [223/275]:\tLoss: 1.5523681640625\n",
            "Step [224/275]:\tLoss: 1.8149914741516113\n",
            "Step [225/275]:\tLoss: 2.0541858673095703\n",
            "Step [226/275]:\tLoss: 1.1311566829681396\n",
            "Step [227/275]:\tLoss: 1.8211482763290405\n",
            "Step [228/275]:\tLoss: 1.2960543632507324\n",
            "Step [229/275]:\tLoss: 1.807646632194519\n",
            "Step [230/275]:\tLoss: 1.998841404914856\n",
            "Step [231/275]:\tLoss: 1.8846179246902466\n",
            "Step [232/275]:\tLoss: 1.987627387046814\n",
            "Step [233/275]:\tLoss: 1.2833930253982544\n",
            "Step [234/275]:\tLoss: 1.31961989402771\n",
            "Step [235/275]:\tLoss: 1.6910336017608643\n",
            "Step [236/275]:\tLoss: 1.1906275749206543\n",
            "Step [237/275]:\tLoss: 2.0315160751342773\n",
            "Step [238/275]:\tLoss: 1.689016580581665\n",
            "Step [239/275]:\tLoss: 2.044431686401367\n",
            "Step [240/275]:\tLoss: 1.312500238418579\n",
            "Step [241/275]:\tLoss: 1.275636076927185\n",
            "Step [242/275]:\tLoss: 2.006168842315674\n",
            "Step [243/275]:\tLoss: 2.455353260040283\n",
            "Step [244/275]:\tLoss: 1.5479152202606201\n",
            "Step [245/275]:\tLoss: 2.1672329902648926\n",
            "Step [246/275]:\tLoss: 1.3480467796325684\n",
            "Step [247/275]:\tLoss: 1.5965254306793213\n",
            "Step [248/275]:\tLoss: 2.314666509628296\n",
            "Step [249/275]:\tLoss: 1.9222407341003418\n",
            "Step [250/275]:\tLoss: 1.6921579837799072\n",
            "Step [251/275]:\tLoss: 1.207733392715454\n",
            "Step [252/275]:\tLoss: 2.420114755630493\n",
            "Step [253/275]:\tLoss: 1.7643380165100098\n",
            "Step [254/275]:\tLoss: 2.5754194259643555\n",
            "Step [255/275]:\tLoss: 1.7410006523132324\n",
            "Step [256/275]:\tLoss: 1.9919353723526\n",
            "Step [257/275]:\tLoss: 1.3487025499343872\n",
            "Step [258/275]:\tLoss: 1.826568365097046\n",
            "Step [259/275]:\tLoss: 2.1222922801971436\n",
            "Step [260/275]:\tLoss: 1.9444622993469238\n",
            "Step [261/275]:\tLoss: 1.4446065425872803\n",
            "Step [262/275]:\tLoss: 1.6070502996444702\n",
            "Step [263/275]:\tLoss: 1.7570817470550537\n",
            "Step [264/275]:\tLoss: 1.6225559711456299\n",
            "Step [265/275]:\tLoss: 1.6562750339508057\n",
            "Step [266/275]:\tLoss: 1.5545223951339722\n",
            "Step [267/275]:\tLoss: 1.8438003063201904\n",
            "Step [268/275]:\tLoss: 1.6818432807922363\n",
            "Step [269/275]:\tLoss: 1.1695592403411865\n",
            "Step [270/275]:\tLoss: 1.8145461082458496\n",
            "Step [271/275]:\tLoss: 1.8562970161437988\n",
            "Step [272/275]:\tLoss: 1.4782192707061768\n",
            "Step [273/275]:\tLoss: 1.2297487258911133\n",
            "Step [274/275]:\tLoss: 2.220033645629883\n",
            "epoch number 97\n",
            "Step [0/275]:\tLoss: 1.6968629360198975\n",
            "Step [1/275]:\tLoss: 1.8908863067626953\n",
            "Step [2/275]:\tLoss: 2.136676788330078\n",
            "Step [3/275]:\tLoss: 1.2038811445236206\n",
            "Step [4/275]:\tLoss: 1.980134129524231\n",
            "Step [5/275]:\tLoss: 1.4087284803390503\n",
            "Step [6/275]:\tLoss: 1.5629918575286865\n",
            "Step [7/275]:\tLoss: 2.3797831535339355\n",
            "Step [8/275]:\tLoss: 1.7097256183624268\n",
            "Step [9/275]:\tLoss: 1.637683629989624\n",
            "Step [10/275]:\tLoss: 2.0365078449249268\n",
            "Step [11/275]:\tLoss: 1.5484167337417603\n",
            "Step [12/275]:\tLoss: 1.3272700309753418\n",
            "Step [13/275]:\tLoss: 1.5803768634796143\n",
            "Step [14/275]:\tLoss: 1.7679860591888428\n",
            "Step [15/275]:\tLoss: 1.8457467555999756\n",
            "Step [16/275]:\tLoss: 1.918178915977478\n",
            "Step [17/275]:\tLoss: 2.2289576530456543\n",
            "Step [18/275]:\tLoss: 1.444411039352417\n",
            "Step [19/275]:\tLoss: 2.03833270072937\n",
            "Step [20/275]:\tLoss: 1.6854034662246704\n",
            "Step [21/275]:\tLoss: 1.6763567924499512\n",
            "Step [22/275]:\tLoss: 1.743011474609375\n",
            "Step [23/275]:\tLoss: 2.1039366722106934\n",
            "Step [24/275]:\tLoss: 1.6591089963912964\n",
            "Step [25/275]:\tLoss: 1.1918774843215942\n",
            "Step [26/275]:\tLoss: 1.8050193786621094\n",
            "Step [27/275]:\tLoss: 2.0006566047668457\n",
            "Step [28/275]:\tLoss: 1.9289751052856445\n",
            "Step [29/275]:\tLoss: 1.4116125106811523\n",
            "Step [30/275]:\tLoss: 1.4171223640441895\n",
            "Step [31/275]:\tLoss: 1.2010115385055542\n",
            "Step [32/275]:\tLoss: 1.7033886909484863\n",
            "Step [33/275]:\tLoss: 1.821657657623291\n",
            "Step [34/275]:\tLoss: 1.1516505479812622\n",
            "Step [35/275]:\tLoss: 1.4507932662963867\n",
            "Step [36/275]:\tLoss: 0.6580312252044678\n",
            "Step [37/275]:\tLoss: 0.7745556831359863\n",
            "Step [38/275]:\tLoss: 1.7756255865097046\n",
            "Step [39/275]:\tLoss: 1.8069512844085693\n",
            "Step [40/275]:\tLoss: 1.5671110153198242\n",
            "Step [41/275]:\tLoss: 1.9781473875045776\n",
            "Step [42/275]:\tLoss: 1.6171188354492188\n",
            "Step [43/275]:\tLoss: 1.8907296657562256\n",
            "Step [44/275]:\tLoss: 1.1535489559173584\n",
            "Step [45/275]:\tLoss: 1.106462836265564\n",
            "Step [46/275]:\tLoss: 1.5627809762954712\n",
            "Step [47/275]:\tLoss: 1.4893289804458618\n",
            "Step [48/275]:\tLoss: 1.8569170236587524\n",
            "Step [49/275]:\tLoss: 2.0422468185424805\n",
            "Step [50/275]:\tLoss: 1.4694218635559082\n",
            "Step [51/275]:\tLoss: 0.8969292640686035\n",
            "Step [52/275]:\tLoss: 1.3043237924575806\n",
            "Step [53/275]:\tLoss: 1.1840457916259766\n",
            "Step [54/275]:\tLoss: 1.3140543699264526\n",
            "Step [55/275]:\tLoss: 1.2574055194854736\n",
            "Step [56/275]:\tLoss: 1.081865668296814\n",
            "Step [57/275]:\tLoss: 1.8533433675765991\n",
            "Step [58/275]:\tLoss: 1.0277849435806274\n",
            "Step [59/275]:\tLoss: 1.5427157878875732\n",
            "Step [60/275]:\tLoss: 1.5000569820404053\n",
            "Step [61/275]:\tLoss: 1.7262132167816162\n",
            "Step [62/275]:\tLoss: 2.3214941024780273\n",
            "Step [63/275]:\tLoss: 1.1463563442230225\n",
            "Step [64/275]:\tLoss: 2.072917938232422\n",
            "Step [65/275]:\tLoss: 1.168445110321045\n",
            "Step [66/275]:\tLoss: 1.0366889238357544\n",
            "Step [67/275]:\tLoss: 1.3835444450378418\n",
            "Step [68/275]:\tLoss: 1.6110397577285767\n",
            "Step [69/275]:\tLoss: 0.9674733877182007\n",
            "Step [70/275]:\tLoss: 1.5489732027053833\n",
            "Step [71/275]:\tLoss: 1.0017379522323608\n",
            "Step [72/275]:\tLoss: 1.1951713562011719\n",
            "Step [73/275]:\tLoss: 1.3951382637023926\n",
            "Step [74/275]:\tLoss: 1.106751561164856\n",
            "Step [75/275]:\tLoss: 1.5035622119903564\n",
            "Step [76/275]:\tLoss: 1.7242958545684814\n",
            "Step [77/275]:\tLoss: 2.1675915718078613\n",
            "Step [78/275]:\tLoss: 1.6295397281646729\n",
            "Step [79/275]:\tLoss: 2.094951868057251\n",
            "Step [80/275]:\tLoss: 1.3927738666534424\n",
            "Step [81/275]:\tLoss: 1.4532945156097412\n",
            "Step [82/275]:\tLoss: 1.6020839214324951\n",
            "Step [83/275]:\tLoss: 0.9205335974693298\n",
            "Step [84/275]:\tLoss: 1.4541938304901123\n",
            "Step [85/275]:\tLoss: 0.8167181015014648\n",
            "Step [86/275]:\tLoss: 2.3360209465026855\n",
            "Step [87/275]:\tLoss: 0.48371630907058716\n",
            "Step [88/275]:\tLoss: 1.849637508392334\n",
            "Step [89/275]:\tLoss: 1.3661766052246094\n",
            "Step [90/275]:\tLoss: 1.3223035335540771\n",
            "Step [91/275]:\tLoss: 1.685053825378418\n",
            "Step [92/275]:\tLoss: 2.1145639419555664\n",
            "Step [93/275]:\tLoss: 1.0219022035598755\n",
            "Step [94/275]:\tLoss: 1.9377272129058838\n",
            "Step [95/275]:\tLoss: 0.8619310855865479\n",
            "Step [96/275]:\tLoss: 1.9076292514801025\n",
            "Step [97/275]:\tLoss: 0.9379726648330688\n",
            "Step [98/275]:\tLoss: 1.319158911705017\n",
            "Step [99/275]:\tLoss: 1.1287586688995361\n",
            "Step [100/275]:\tLoss: 1.6110689640045166\n",
            "Step [101/275]:\tLoss: 1.1152513027191162\n",
            "Step [102/275]:\tLoss: 1.2423837184906006\n",
            "Step [103/275]:\tLoss: 1.9569733142852783\n",
            "Step [104/275]:\tLoss: 1.1595569849014282\n",
            "Step [105/275]:\tLoss: 1.3118209838867188\n",
            "Step [106/275]:\tLoss: 1.23301362991333\n",
            "Step [107/275]:\tLoss: 1.510117769241333\n",
            "Step [108/275]:\tLoss: 0.8920679092407227\n",
            "Step [109/275]:\tLoss: 2.453127861022949\n",
            "Step [110/275]:\tLoss: 1.7299987077713013\n",
            "Step [111/275]:\tLoss: 1.5199625492095947\n",
            "Step [112/275]:\tLoss: 1.4140307903289795\n",
            "Step [113/275]:\tLoss: 2.1826672554016113\n",
            "Step [114/275]:\tLoss: 2.8874008655548096\n",
            "Step [115/275]:\tLoss: 2.659440040588379\n",
            "Step [116/275]:\tLoss: 2.6966891288757324\n",
            "Step [117/275]:\tLoss: 2.0916647911071777\n",
            "Step [118/275]:\tLoss: 2.3938326835632324\n",
            "Step [119/275]:\tLoss: 1.8401730060577393\n",
            "Step [120/275]:\tLoss: 2.012157440185547\n",
            "Step [121/275]:\tLoss: 1.6457599401474\n",
            "Step [122/275]:\tLoss: 1.6697778701782227\n",
            "Step [123/275]:\tLoss: 2.0320894718170166\n",
            "Step [124/275]:\tLoss: 1.5057170391082764\n",
            "Step [125/275]:\tLoss: 2.1837761402130127\n",
            "Step [126/275]:\tLoss: 1.4413293600082397\n",
            "Step [127/275]:\tLoss: 2.162722110748291\n",
            "Step [128/275]:\tLoss: 1.9665842056274414\n",
            "Step [129/275]:\tLoss: 0.892490029335022\n",
            "Step [130/275]:\tLoss: 2.0024781227111816\n",
            "Step [131/275]:\tLoss: 1.6032047271728516\n",
            "Step [132/275]:\tLoss: 1.4759759902954102\n",
            "Step [133/275]:\tLoss: 3.135610580444336\n",
            "Step [134/275]:\tLoss: 1.560713291168213\n",
            "Step [135/275]:\tLoss: 2.149482250213623\n",
            "Step [136/275]:\tLoss: 1.111931324005127\n",
            "Step [137/275]:\tLoss: 1.9920434951782227\n",
            "Step [138/275]:\tLoss: 1.4613885879516602\n",
            "Step [139/275]:\tLoss: 2.419142246246338\n",
            "Step [140/275]:\tLoss: 1.598961591720581\n",
            "Step [141/275]:\tLoss: 1.498905897140503\n",
            "Step [142/275]:\tLoss: 1.7602958679199219\n",
            "Step [143/275]:\tLoss: 2.2683327198028564\n",
            "Step [144/275]:\tLoss: 1.001176118850708\n",
            "Step [145/275]:\tLoss: 1.7040598392486572\n",
            "Step [146/275]:\tLoss: 1.3210073709487915\n",
            "Step [147/275]:\tLoss: 1.5393891334533691\n",
            "Step [148/275]:\tLoss: 1.2300646305084229\n",
            "Step [149/275]:\tLoss: 1.3116295337677002\n",
            "Step [150/275]:\tLoss: 1.2295653820037842\n",
            "Step [151/275]:\tLoss: 0.9986557364463806\n",
            "Step [152/275]:\tLoss: 1.8220266103744507\n",
            "Step [153/275]:\tLoss: 1.5916311740875244\n",
            "Step [154/275]:\tLoss: 1.7213846445083618\n",
            "Step [155/275]:\tLoss: 0.9442194700241089\n",
            "Step [156/275]:\tLoss: 0.8715208172798157\n",
            "Step [157/275]:\tLoss: 2.52915358543396\n",
            "Step [158/275]:\tLoss: 0.7655438184738159\n",
            "Step [159/275]:\tLoss: 1.8959660530090332\n",
            "Step [160/275]:\tLoss: 1.2419805526733398\n",
            "Step [161/275]:\tLoss: 2.233354091644287\n",
            "Step [162/275]:\tLoss: 1.2442827224731445\n",
            "Step [163/275]:\tLoss: 0.6628929376602173\n",
            "Step [164/275]:\tLoss: 1.2153956890106201\n",
            "Step [165/275]:\tLoss: 2.139493942260742\n",
            "Step [166/275]:\tLoss: 1.1968414783477783\n",
            "Step [167/275]:\tLoss: 1.9716429710388184\n",
            "Step [168/275]:\tLoss: 1.093298077583313\n",
            "Step [169/275]:\tLoss: 2.169464111328125\n",
            "Step [170/275]:\tLoss: 2.13421368598938\n",
            "Step [171/275]:\tLoss: 1.498174786567688\n",
            "Step [172/275]:\tLoss: 1.178945541381836\n",
            "Step [173/275]:\tLoss: 1.5575125217437744\n",
            "Step [174/275]:\tLoss: 1.8432221412658691\n",
            "Step [175/275]:\tLoss: 2.732508420944214\n",
            "Step [176/275]:\tLoss: 1.9522900581359863\n",
            "Step [177/275]:\tLoss: 0.8423293828964233\n",
            "Step [178/275]:\tLoss: 1.5304491519927979\n",
            "Step [179/275]:\tLoss: 1.5145337581634521\n",
            "Step [180/275]:\tLoss: 1.1212393045425415\n",
            "Step [181/275]:\tLoss: 1.2398717403411865\n",
            "Step [182/275]:\tLoss: 1.219681978225708\n",
            "Step [183/275]:\tLoss: 0.9733514189720154\n",
            "Step [184/275]:\tLoss: 1.418252944946289\n",
            "Step [185/275]:\tLoss: 1.2112321853637695\n",
            "Step [186/275]:\tLoss: 1.2299681901931763\n",
            "Step [187/275]:\tLoss: 1.7049490213394165\n",
            "Step [188/275]:\tLoss: 1.6495774984359741\n",
            "Step [189/275]:\tLoss: 1.1304280757904053\n",
            "Step [190/275]:\tLoss: 1.8726993799209595\n",
            "Step [191/275]:\tLoss: 2.3252696990966797\n",
            "Step [192/275]:\tLoss: 2.11737322807312\n",
            "Step [193/275]:\tLoss: 1.645946741104126\n",
            "Step [194/275]:\tLoss: 2.304811477661133\n",
            "Step [195/275]:\tLoss: 2.0894083976745605\n",
            "Step [196/275]:\tLoss: 1.71441650390625\n",
            "Step [197/275]:\tLoss: 1.7368310689926147\n",
            "Step [198/275]:\tLoss: 1.4791955947875977\n",
            "Step [199/275]:\tLoss: 1.5866550207138062\n",
            "Step [200/275]:\tLoss: 1.5320792198181152\n",
            "Step [201/275]:\tLoss: 1.264567255973816\n",
            "Step [202/275]:\tLoss: 2.3646140098571777\n",
            "Step [203/275]:\tLoss: 2.4978976249694824\n",
            "Step [204/275]:\tLoss: 2.0222907066345215\n",
            "Step [205/275]:\tLoss: 1.402045488357544\n",
            "Step [206/275]:\tLoss: 1.6805838346481323\n",
            "Step [207/275]:\tLoss: 1.732170581817627\n",
            "Step [208/275]:\tLoss: 2.1259193420410156\n",
            "Step [209/275]:\tLoss: 1.6946550607681274\n",
            "Step [210/275]:\tLoss: 1.6483819484710693\n",
            "Step [211/275]:\tLoss: 1.9216749668121338\n",
            "Step [212/275]:\tLoss: 2.135145664215088\n",
            "Step [213/275]:\tLoss: 1.4641847610473633\n",
            "Step [214/275]:\tLoss: 1.9686694145202637\n",
            "Step [215/275]:\tLoss: 1.762587547302246\n",
            "Step [216/275]:\tLoss: 1.4787704944610596\n",
            "Step [217/275]:\tLoss: 1.0705955028533936\n",
            "Step [218/275]:\tLoss: 1.7197291851043701\n",
            "Step [219/275]:\tLoss: 1.5233656167984009\n",
            "Step [220/275]:\tLoss: 2.1310582160949707\n",
            "Step [221/275]:\tLoss: 1.5510039329528809\n",
            "Step [222/275]:\tLoss: 1.5931706428527832\n",
            "Step [223/275]:\tLoss: 2.0827126502990723\n",
            "Step [224/275]:\tLoss: 1.5165817737579346\n",
            "Step [225/275]:\tLoss: 2.1855111122131348\n",
            "Step [226/275]:\tLoss: 2.7143023014068604\n",
            "Step [227/275]:\tLoss: 1.5174692869186401\n",
            "Step [228/275]:\tLoss: 2.023909568786621\n",
            "Step [229/275]:\tLoss: 1.698596477508545\n",
            "Step [230/275]:\tLoss: 1.0353291034698486\n",
            "Step [231/275]:\tLoss: 1.7646230459213257\n",
            "Step [232/275]:\tLoss: 2.521617889404297\n",
            "Step [233/275]:\tLoss: 1.6613211631774902\n",
            "Step [234/275]:\tLoss: 1.3964728116989136\n",
            "Step [235/275]:\tLoss: 2.6830077171325684\n",
            "Step [236/275]:\tLoss: 1.4652434587478638\n",
            "Step [237/275]:\tLoss: 1.3058679103851318\n",
            "Step [238/275]:\tLoss: 1.287145733833313\n",
            "Step [239/275]:\tLoss: 2.1113944053649902\n",
            "Step [240/275]:\tLoss: 0.7898715734481812\n",
            "Step [241/275]:\tLoss: 1.9651429653167725\n",
            "Step [242/275]:\tLoss: 1.6746357679367065\n",
            "Step [243/275]:\tLoss: 1.9052252769470215\n",
            "Step [244/275]:\tLoss: 1.4277464151382446\n",
            "Step [245/275]:\tLoss: 2.2908248901367188\n",
            "Step [246/275]:\tLoss: 1.3052091598510742\n",
            "Step [247/275]:\tLoss: 1.3542592525482178\n",
            "Step [248/275]:\tLoss: 2.168435573577881\n",
            "Step [249/275]:\tLoss: 1.466320514678955\n",
            "Step [250/275]:\tLoss: 1.2136327028274536\n",
            "Step [251/275]:\tLoss: 1.727081298828125\n",
            "Step [252/275]:\tLoss: 1.4095332622528076\n",
            "Step [253/275]:\tLoss: 2.4617364406585693\n",
            "Step [254/275]:\tLoss: 1.6423851251602173\n",
            "Step [255/275]:\tLoss: 1.6651755571365356\n",
            "Step [256/275]:\tLoss: 1.6265013217926025\n",
            "Step [257/275]:\tLoss: 1.5432753562927246\n",
            "Step [258/275]:\tLoss: 2.2593419551849365\n",
            "Step [259/275]:\tLoss: 1.9352763891220093\n",
            "Step [260/275]:\tLoss: 1.683410406112671\n",
            "Step [261/275]:\tLoss: 1.7586925029754639\n",
            "Step [262/275]:\tLoss: 1.625631332397461\n",
            "Step [263/275]:\tLoss: 1.911863088607788\n",
            "Step [264/275]:\tLoss: 1.2606124877929688\n",
            "Step [265/275]:\tLoss: 1.1189277172088623\n",
            "Step [266/275]:\tLoss: 1.7145686149597168\n",
            "Step [267/275]:\tLoss: 1.6831724643707275\n",
            "Step [268/275]:\tLoss: 1.3639715909957886\n",
            "Step [269/275]:\tLoss: 1.7816781997680664\n",
            "Step [270/275]:\tLoss: 1.372328758239746\n",
            "Step [271/275]:\tLoss: 2.406848430633545\n",
            "Step [272/275]:\tLoss: 1.7651818990707397\n",
            "Step [273/275]:\tLoss: 1.8057005405426025\n",
            "Step [274/275]:\tLoss: 1.2450556755065918\n",
            "epoch number 98\n",
            "Step [0/275]:\tLoss: 1.6041368246078491\n",
            "Step [1/275]:\tLoss: 1.6451036930084229\n",
            "Step [2/275]:\tLoss: 1.8953369855880737\n",
            "Step [3/275]:\tLoss: 2.072744607925415\n",
            "Step [4/275]:\tLoss: 2.2160801887512207\n",
            "Step [5/275]:\tLoss: 1.5162380933761597\n",
            "Step [6/275]:\tLoss: 3.174577236175537\n",
            "Step [7/275]:\tLoss: 2.1913022994995117\n",
            "Step [8/275]:\tLoss: 1.600084900856018\n",
            "Step [9/275]:\tLoss: 1.5328783988952637\n",
            "Step [10/275]:\tLoss: 1.3485318422317505\n",
            "Step [11/275]:\tLoss: 2.381173610687256\n",
            "Step [12/275]:\tLoss: 1.129717230796814\n",
            "Step [13/275]:\tLoss: 1.6957378387451172\n",
            "Step [14/275]:\tLoss: 1.5092217922210693\n",
            "Step [15/275]:\tLoss: 2.5316357612609863\n",
            "Step [16/275]:\tLoss: 2.4854748249053955\n",
            "Step [17/275]:\tLoss: 1.843720555305481\n",
            "Step [18/275]:\tLoss: 2.57487154006958\n",
            "Step [19/275]:\tLoss: 1.4857577085494995\n",
            "Step [20/275]:\tLoss: 1.4716973304748535\n",
            "Step [21/275]:\tLoss: 1.7477989196777344\n",
            "Step [22/275]:\tLoss: 0.9526053667068481\n",
            "Step [23/275]:\tLoss: 1.5334265232086182\n",
            "Step [24/275]:\tLoss: 1.8120537996292114\n",
            "Step [25/275]:\tLoss: 1.7689628601074219\n",
            "Step [26/275]:\tLoss: 1.7004187107086182\n",
            "Step [27/275]:\tLoss: 1.3019778728485107\n",
            "Step [28/275]:\tLoss: 2.207731246948242\n",
            "Step [29/275]:\tLoss: 1.3997349739074707\n",
            "Step [30/275]:\tLoss: 1.4947290420532227\n",
            "Step [31/275]:\tLoss: 1.6355866193771362\n",
            "Step [32/275]:\tLoss: 1.1098345518112183\n",
            "Step [33/275]:\tLoss: 1.3882505893707275\n",
            "Step [34/275]:\tLoss: 1.1593188047409058\n",
            "Step [35/275]:\tLoss: 2.4257869720458984\n",
            "Step [36/275]:\tLoss: 1.0422691106796265\n",
            "Step [37/275]:\tLoss: 0.7708338499069214\n",
            "Step [38/275]:\tLoss: 1.0814849138259888\n",
            "Step [39/275]:\tLoss: 2.4355199337005615\n",
            "Step [40/275]:\tLoss: 1.703524112701416\n",
            "Step [41/275]:\tLoss: 1.3618931770324707\n",
            "Step [42/275]:\tLoss: 0.9800944924354553\n",
            "Step [43/275]:\tLoss: 1.4933974742889404\n",
            "Step [44/275]:\tLoss: 1.236422061920166\n",
            "Step [45/275]:\tLoss: 1.1438674926757812\n",
            "Step [46/275]:\tLoss: 2.0938005447387695\n",
            "Step [47/275]:\tLoss: 0.5943700075149536\n",
            "Step [48/275]:\tLoss: 0.9922899007797241\n",
            "Step [49/275]:\tLoss: 1.6808128356933594\n",
            "Step [50/275]:\tLoss: 0.936705470085144\n",
            "Step [51/275]:\tLoss: 1.3149195909500122\n",
            "Step [52/275]:\tLoss: 0.8454540967941284\n",
            "Step [53/275]:\tLoss: 1.9605662822723389\n",
            "Step [54/275]:\tLoss: 2.6083998680114746\n",
            "Step [55/275]:\tLoss: 1.3038883209228516\n",
            "Step [56/275]:\tLoss: 0.7840502262115479\n",
            "Step [57/275]:\tLoss: 1.2325912714004517\n",
            "Step [58/275]:\tLoss: 0.39535075426101685\n",
            "Step [59/275]:\tLoss: 1.6324963569641113\n",
            "Step [60/275]:\tLoss: 1.7622238397598267\n",
            "Step [61/275]:\tLoss: 1.1147068738937378\n",
            "Step [62/275]:\tLoss: 1.1082398891448975\n",
            "Step [63/275]:\tLoss: 1.2239930629730225\n",
            "Step [64/275]:\tLoss: 1.1783090829849243\n",
            "Step [65/275]:\tLoss: 1.2863240242004395\n",
            "Step [66/275]:\tLoss: 1.1516270637512207\n",
            "Step [67/275]:\tLoss: 1.286651611328125\n",
            "Step [68/275]:\tLoss: 0.8025178909301758\n",
            "Step [69/275]:\tLoss: 1.5230191946029663\n",
            "Step [70/275]:\tLoss: 2.029290199279785\n",
            "Step [71/275]:\tLoss: 1.2388774156570435\n",
            "Step [72/275]:\tLoss: 0.9882016777992249\n",
            "Step [73/275]:\tLoss: 0.8835755586624146\n",
            "Step [74/275]:\tLoss: 1.240705966949463\n",
            "Step [75/275]:\tLoss: 1.0232574939727783\n",
            "Step [76/275]:\tLoss: 1.0406124591827393\n",
            "Step [77/275]:\tLoss: 0.7913079261779785\n",
            "Step [78/275]:\tLoss: 1.1301902532577515\n",
            "Step [79/275]:\tLoss: 1.547696590423584\n",
            "Step [80/275]:\tLoss: 1.8824924230575562\n",
            "Step [81/275]:\tLoss: 1.1669371128082275\n",
            "Step [82/275]:\tLoss: 1.245759129524231\n",
            "Step [83/275]:\tLoss: 0.8477467894554138\n",
            "Step [84/275]:\tLoss: 1.000861406326294\n",
            "Step [85/275]:\tLoss: 1.9584052562713623\n",
            "Step [86/275]:\tLoss: 1.6439365148544312\n",
            "Step [87/275]:\tLoss: 0.838319718837738\n",
            "Step [88/275]:\tLoss: 0.6824437379837036\n",
            "Step [89/275]:\tLoss: 2.0468931198120117\n",
            "Step [90/275]:\tLoss: 1.0563554763793945\n",
            "Step [91/275]:\tLoss: 1.206347942352295\n",
            "Step [92/275]:\tLoss: 1.5359773635864258\n",
            "Step [93/275]:\tLoss: 0.7707105278968811\n",
            "Step [94/275]:\tLoss: 1.1969560384750366\n",
            "Step [95/275]:\tLoss: 0.7629817128181458\n",
            "Step [96/275]:\tLoss: 1.0347692966461182\n",
            "Step [97/275]:\tLoss: 1.5539312362670898\n",
            "Step [98/275]:\tLoss: 2.2107162475585938\n",
            "Step [99/275]:\tLoss: 1.5035381317138672\n",
            "Step [100/275]:\tLoss: 1.3478329181671143\n",
            "Step [101/275]:\tLoss: 2.077927589416504\n",
            "Step [102/275]:\tLoss: 1.332897424697876\n",
            "Step [103/275]:\tLoss: 0.5484746098518372\n",
            "Step [104/275]:\tLoss: 1.1634894609451294\n",
            "Step [105/275]:\tLoss: 0.6892699003219604\n",
            "Step [106/275]:\tLoss: 1.1067460775375366\n",
            "Step [107/275]:\tLoss: 1.2601914405822754\n",
            "Step [108/275]:\tLoss: 1.5140838623046875\n",
            "Step [109/275]:\tLoss: 1.0418357849121094\n",
            "Step [110/275]:\tLoss: 3.218592643737793\n",
            "Step [111/275]:\tLoss: 2.4204161167144775\n",
            "Step [112/275]:\tLoss: 2.07374906539917\n",
            "Step [113/275]:\tLoss: 1.9050321578979492\n",
            "Step [114/275]:\tLoss: 2.8584961891174316\n",
            "Step [115/275]:\tLoss: 3.0959243774414062\n",
            "Step [116/275]:\tLoss: 2.3998758792877197\n",
            "Step [117/275]:\tLoss: 3.0194640159606934\n",
            "Step [118/275]:\tLoss: 2.488253593444824\n",
            "Step [119/275]:\tLoss: 0.7528286576271057\n",
            "Step [120/275]:\tLoss: 3.5546364784240723\n",
            "Step [121/275]:\tLoss: 1.1102213859558105\n",
            "Step [122/275]:\tLoss: 1.5762674808502197\n",
            "Step [123/275]:\tLoss: 2.0114383697509766\n",
            "Step [124/275]:\tLoss: 1.4570684432983398\n",
            "Step [125/275]:\tLoss: 2.2720816135406494\n",
            "Step [126/275]:\tLoss: 2.806366443634033\n",
            "Step [127/275]:\tLoss: 1.869230031967163\n",
            "Step [128/275]:\tLoss: 2.369628429412842\n",
            "Step [129/275]:\tLoss: 2.1246113777160645\n",
            "Step [130/275]:\tLoss: 1.717195749282837\n",
            "Step [131/275]:\tLoss: 1.549043893814087\n",
            "Step [132/275]:\tLoss: 1.765573263168335\n",
            "Step [133/275]:\tLoss: 2.1152877807617188\n",
            "Step [134/275]:\tLoss: 2.2140848636627197\n",
            "Step [135/275]:\tLoss: 1.6336250305175781\n",
            "Step [136/275]:\tLoss: 1.5105407238006592\n",
            "Step [137/275]:\tLoss: 1.5715527534484863\n",
            "Step [138/275]:\tLoss: 0.7950129508972168\n",
            "Step [139/275]:\tLoss: 0.9153518676757812\n",
            "Step [140/275]:\tLoss: 1.0841929912567139\n",
            "Step [141/275]:\tLoss: 1.3028290271759033\n",
            "Step [142/275]:\tLoss: 1.4176923036575317\n",
            "Step [143/275]:\tLoss: 0.8057246208190918\n",
            "Step [144/275]:\tLoss: 1.1699544191360474\n",
            "Step [145/275]:\tLoss: 1.185455322265625\n",
            "Step [146/275]:\tLoss: 0.45949500799179077\n",
            "Step [147/275]:\tLoss: 1.3494187593460083\n",
            "Step [148/275]:\tLoss: 0.9312390089035034\n",
            "Step [149/275]:\tLoss: 0.7131531238555908\n",
            "Step [150/275]:\tLoss: 0.6378843784332275\n",
            "Step [151/275]:\tLoss: 1.2411792278289795\n",
            "Step [152/275]:\tLoss: 1.3147865533828735\n",
            "Step [153/275]:\tLoss: 1.3661798238754272\n",
            "Step [154/275]:\tLoss: 1.2658205032348633\n",
            "Step [155/275]:\tLoss: 1.2749483585357666\n",
            "Step [156/275]:\tLoss: 1.628684163093567\n",
            "Step [157/275]:\tLoss: 1.858776569366455\n",
            "Step [158/275]:\tLoss: 0.9724452495574951\n",
            "Step [159/275]:\tLoss: 1.6838901042938232\n",
            "Step [160/275]:\tLoss: 0.6082140207290649\n",
            "Step [161/275]:\tLoss: 1.108679175376892\n",
            "Step [162/275]:\tLoss: 0.44860559701919556\n",
            "Step [163/275]:\tLoss: 0.9375479221343994\n",
            "Step [164/275]:\tLoss: 1.3244612216949463\n",
            "Step [165/275]:\tLoss: 1.0709248781204224\n",
            "Step [166/275]:\tLoss: 0.8497106432914734\n",
            "Step [167/275]:\tLoss: 0.8211522102355957\n",
            "Step [168/275]:\tLoss: 1.46662175655365\n",
            "Step [169/275]:\tLoss: 1.1196212768554688\n",
            "Step [170/275]:\tLoss: 1.6749563217163086\n",
            "Step [171/275]:\tLoss: 0.947009265422821\n",
            "Step [172/275]:\tLoss: 1.7781758308410645\n",
            "Step [173/275]:\tLoss: 0.8916041851043701\n",
            "Step [174/275]:\tLoss: 1.2389506101608276\n",
            "Step [175/275]:\tLoss: 1.8743209838867188\n",
            "Step [176/275]:\tLoss: 1.5434521436691284\n",
            "Step [177/275]:\tLoss: 0.9431046843528748\n",
            "Step [178/275]:\tLoss: 1.235567569732666\n",
            "Step [179/275]:\tLoss: 0.7549512982368469\n",
            "Step [180/275]:\tLoss: 1.2475836277008057\n",
            "Step [181/275]:\tLoss: 1.3214478492736816\n",
            "Step [182/275]:\tLoss: 1.2424982786178589\n",
            "Step [183/275]:\tLoss: 0.8375808596611023\n",
            "Step [184/275]:\tLoss: 1.0959231853485107\n",
            "Step [185/275]:\tLoss: 1.1095499992370605\n",
            "Step [186/275]:\tLoss: 0.9192376136779785\n",
            "Step [187/275]:\tLoss: 1.3206820487976074\n",
            "Step [188/275]:\tLoss: 1.7859132289886475\n",
            "Step [189/275]:\tLoss: 1.1844924688339233\n",
            "Step [190/275]:\tLoss: 1.589963436126709\n",
            "Step [191/275]:\tLoss: 1.5834547281265259\n",
            "Step [192/275]:\tLoss: 2.7686429023742676\n",
            "Step [193/275]:\tLoss: 1.9988430738449097\n",
            "Step [194/275]:\tLoss: 2.119382619857788\n",
            "Step [195/275]:\tLoss: 1.6199872493743896\n",
            "Step [196/275]:\tLoss: 2.0257444381713867\n",
            "Step [197/275]:\tLoss: 2.295243263244629\n",
            "Step [198/275]:\tLoss: 1.712072491645813\n",
            "Step [199/275]:\tLoss: 2.33233642578125\n",
            "Step [200/275]:\tLoss: 2.5095648765563965\n",
            "Step [201/275]:\tLoss: 1.6573166847229004\n",
            "Step [202/275]:\tLoss: 1.6895415782928467\n",
            "Step [203/275]:\tLoss: 1.8574243783950806\n",
            "Step [204/275]:\tLoss: 2.6081771850585938\n",
            "Step [205/275]:\tLoss: 1.8791025876998901\n",
            "Step [206/275]:\tLoss: 1.5515624284744263\n",
            "Step [207/275]:\tLoss: 2.095451831817627\n",
            "Step [208/275]:\tLoss: 2.119446277618408\n",
            "Step [209/275]:\tLoss: 1.3559808731079102\n",
            "Step [210/275]:\tLoss: 1.5079307556152344\n",
            "Step [211/275]:\tLoss: 2.074309825897217\n",
            "Step [212/275]:\tLoss: 2.2504732608795166\n",
            "Step [213/275]:\tLoss: 1.6466819047927856\n",
            "Step [214/275]:\tLoss: 1.415034294128418\n",
            "Step [215/275]:\tLoss: 1.2840917110443115\n",
            "Step [216/275]:\tLoss: 2.039267063140869\n",
            "Step [217/275]:\tLoss: 1.9640288352966309\n",
            "Step [218/275]:\tLoss: 2.7439727783203125\n",
            "Step [219/275]:\tLoss: 1.0304720401763916\n",
            "Step [220/275]:\tLoss: 2.0323705673217773\n",
            "Step [221/275]:\tLoss: 1.4061957597732544\n",
            "Step [222/275]:\tLoss: 1.0252983570098877\n",
            "Step [223/275]:\tLoss: 1.9897806644439697\n",
            "Step [224/275]:\tLoss: 1.5497039556503296\n",
            "Step [225/275]:\tLoss: 1.8896101713180542\n",
            "Step [226/275]:\tLoss: 0.9227615594863892\n",
            "Step [227/275]:\tLoss: 1.1827037334442139\n",
            "Step [228/275]:\tLoss: 1.6623895168304443\n",
            "Step [229/275]:\tLoss: 2.449147939682007\n",
            "Step [230/275]:\tLoss: 1.3605271577835083\n",
            "Step [231/275]:\tLoss: 1.2922052145004272\n",
            "Step [232/275]:\tLoss: 1.465714454650879\n",
            "Step [233/275]:\tLoss: 1.8487141132354736\n",
            "Step [234/275]:\tLoss: 1.5129218101501465\n",
            "Step [235/275]:\tLoss: 1.1237831115722656\n",
            "Step [236/275]:\tLoss: 1.341003656387329\n",
            "Step [237/275]:\tLoss: 1.197843313217163\n",
            "Step [238/275]:\tLoss: 1.7579503059387207\n",
            "Step [239/275]:\tLoss: 1.8561816215515137\n",
            "Step [240/275]:\tLoss: 1.4339410066604614\n",
            "Step [241/275]:\tLoss: 2.031061887741089\n",
            "Step [242/275]:\tLoss: 1.66587495803833\n",
            "Step [243/275]:\tLoss: 2.4946436882019043\n",
            "Step [244/275]:\tLoss: 1.6767773628234863\n",
            "Step [245/275]:\tLoss: 2.040806770324707\n",
            "Step [246/275]:\tLoss: 2.443802833557129\n",
            "Step [247/275]:\tLoss: 2.094208002090454\n",
            "Step [248/275]:\tLoss: 1.378868818283081\n",
            "Step [249/275]:\tLoss: 1.7472448348999023\n",
            "Step [250/275]:\tLoss: 2.3752989768981934\n",
            "Step [251/275]:\tLoss: 0.8825454115867615\n",
            "Step [252/275]:\tLoss: 2.1090705394744873\n",
            "Step [253/275]:\tLoss: 1.663254976272583\n",
            "Step [254/275]:\tLoss: 2.100125312805176\n",
            "Step [255/275]:\tLoss: 1.4686387777328491\n",
            "Step [256/275]:\tLoss: 2.3899283409118652\n",
            "Step [257/275]:\tLoss: 1.7452797889709473\n",
            "Step [258/275]:\tLoss: 1.536952018737793\n",
            "Step [259/275]:\tLoss: 1.0412296056747437\n",
            "Step [260/275]:\tLoss: 1.4186062812805176\n",
            "Step [261/275]:\tLoss: 1.502441167831421\n",
            "Step [262/275]:\tLoss: 2.279331684112549\n",
            "Step [263/275]:\tLoss: 2.689950942993164\n",
            "Step [264/275]:\tLoss: 2.161066770553589\n",
            "Step [265/275]:\tLoss: 2.1768314838409424\n",
            "Step [266/275]:\tLoss: 2.0626394748687744\n",
            "Step [267/275]:\tLoss: 1.2722246646881104\n",
            "Step [268/275]:\tLoss: 1.399546504020691\n",
            "Step [269/275]:\tLoss: 1.6462581157684326\n",
            "Step [270/275]:\tLoss: 1.7573237419128418\n",
            "Step [271/275]:\tLoss: 1.8430136442184448\n",
            "Step [272/275]:\tLoss: 2.002333164215088\n",
            "Step [273/275]:\tLoss: 1.2128599882125854\n",
            "Step [274/275]:\tLoss: 1.7111369371414185\n",
            "epoch number 99\n",
            "Step [0/275]:\tLoss: 2.9741392135620117\n",
            "Step [1/275]:\tLoss: 1.9287723302841187\n",
            "Step [2/275]:\tLoss: 2.8064541816711426\n",
            "Step [3/275]:\tLoss: 2.157102108001709\n",
            "Step [4/275]:\tLoss: 3.02207088470459\n",
            "Step [5/275]:\tLoss: 1.4346020221710205\n",
            "Step [6/275]:\tLoss: 2.2516090869903564\n",
            "Step [7/275]:\tLoss: 1.285298466682434\n",
            "Step [8/275]:\tLoss: 1.4333827495574951\n",
            "Step [9/275]:\tLoss: 1.6125444173812866\n",
            "Step [10/275]:\tLoss: 1.5737855434417725\n",
            "Step [11/275]:\tLoss: 1.9622808694839478\n",
            "Step [12/275]:\tLoss: 2.0815093517303467\n",
            "Step [13/275]:\tLoss: 2.586503028869629\n",
            "Step [14/275]:\tLoss: 1.777972936630249\n",
            "Step [15/275]:\tLoss: 2.280803680419922\n",
            "Step [16/275]:\tLoss: 1.9417335987091064\n",
            "Step [17/275]:\tLoss: 0.9714517593383789\n",
            "Step [18/275]:\tLoss: 1.3140357732772827\n",
            "Step [19/275]:\tLoss: 2.229421615600586\n",
            "Step [20/275]:\tLoss: 1.1418585777282715\n",
            "Step [21/275]:\tLoss: 1.6008100509643555\n",
            "Step [22/275]:\tLoss: 1.8309879302978516\n",
            "Step [23/275]:\tLoss: 1.524118423461914\n",
            "Step [24/275]:\tLoss: 2.236086130142212\n",
            "Step [25/275]:\tLoss: 1.4884610176086426\n",
            "Step [26/275]:\tLoss: 1.6067907810211182\n",
            "Step [27/275]:\tLoss: 1.368910789489746\n",
            "Step [28/275]:\tLoss: 2.180614471435547\n",
            "Step [29/275]:\tLoss: 1.4830777645111084\n",
            "Step [30/275]:\tLoss: 1.4260808229446411\n",
            "Step [31/275]:\tLoss: 0.9292621612548828\n",
            "Step [32/275]:\tLoss: 1.3152143955230713\n",
            "Step [33/275]:\tLoss: 0.8025641441345215\n",
            "Step [34/275]:\tLoss: 0.31563884019851685\n",
            "Step [35/275]:\tLoss: 0.7803933024406433\n",
            "Step [36/275]:\tLoss: 1.2709848880767822\n",
            "Step [37/275]:\tLoss: 0.8122907876968384\n",
            "Step [38/275]:\tLoss: 0.6317719221115112\n",
            "Step [39/275]:\tLoss: 1.0475449562072754\n",
            "Step [40/275]:\tLoss: 0.785253643989563\n",
            "Step [41/275]:\tLoss: 1.3943979740142822\n",
            "Step [42/275]:\tLoss: 1.2861557006835938\n",
            "Step [43/275]:\tLoss: 0.8923430442810059\n",
            "Step [44/275]:\tLoss: 0.9290304780006409\n",
            "Step [45/275]:\tLoss: 1.6771502494812012\n",
            "Step [46/275]:\tLoss: 0.9072629809379578\n",
            "Step [47/275]:\tLoss: 1.3582947254180908\n",
            "Step [48/275]:\tLoss: 1.0072381496429443\n",
            "Step [49/275]:\tLoss: 0.8168430328369141\n",
            "Step [50/275]:\tLoss: 1.0515185594558716\n",
            "Step [51/275]:\tLoss: 1.4437568187713623\n",
            "Step [52/275]:\tLoss: 1.0950840711593628\n",
            "Step [53/275]:\tLoss: 1.0232715606689453\n",
            "Step [54/275]:\tLoss: 0.7606179714202881\n",
            "Step [55/275]:\tLoss: 0.693709135055542\n",
            "Step [56/275]:\tLoss: 1.2209117412567139\n",
            "Step [57/275]:\tLoss: 1.269052267074585\n",
            "Step [58/275]:\tLoss: 0.6924412250518799\n",
            "Step [59/275]:\tLoss: 1.055484414100647\n",
            "Step [60/275]:\tLoss: 1.585323691368103\n",
            "Step [61/275]:\tLoss: 0.7001341581344604\n",
            "Step [62/275]:\tLoss: 1.399404525756836\n",
            "Step [63/275]:\tLoss: 0.5242226123809814\n",
            "Step [64/275]:\tLoss: 0.6578661799430847\n",
            "Step [65/275]:\tLoss: 1.3583182096481323\n",
            "Step [66/275]:\tLoss: 0.6493740081787109\n",
            "Step [67/275]:\tLoss: 1.357499599456787\n",
            "Step [68/275]:\tLoss: 0.6774677634239197\n",
            "Step [69/275]:\tLoss: 1.2660056352615356\n",
            "Step [70/275]:\tLoss: 1.1191139221191406\n",
            "Step [71/275]:\tLoss: 0.8998275995254517\n",
            "Step [72/275]:\tLoss: 0.3052074909210205\n",
            "Step [73/275]:\tLoss: 0.7783081531524658\n",
            "Step [74/275]:\tLoss: 0.634444534778595\n",
            "Step [75/275]:\tLoss: 1.2985843420028687\n",
            "Step [76/275]:\tLoss: 0.9683607816696167\n",
            "Step [77/275]:\tLoss: 0.9296597838401794\n",
            "Step [78/275]:\tLoss: 0.9163671731948853\n",
            "Step [79/275]:\tLoss: 0.6729247570037842\n",
            "Step [80/275]:\tLoss: 0.7195412516593933\n",
            "Step [81/275]:\tLoss: 1.0978882312774658\n",
            "Step [82/275]:\tLoss: 2.0678744316101074\n",
            "Step [83/275]:\tLoss: 1.1351875066757202\n",
            "Step [84/275]:\tLoss: 0.42573195695877075\n",
            "Step [85/275]:\tLoss: 1.3587225675582886\n",
            "Step [86/275]:\tLoss: 1.2526206970214844\n",
            "Step [87/275]:\tLoss: 1.2987920045852661\n",
            "Step [88/275]:\tLoss: 0.8512537479400635\n",
            "Step [89/275]:\tLoss: 1.1711411476135254\n",
            "Step [90/275]:\tLoss: 1.0997014045715332\n",
            "Step [91/275]:\tLoss: 0.8188992738723755\n",
            "Step [92/275]:\tLoss: 0.9138143658638\n",
            "Step [93/275]:\tLoss: 1.1373004913330078\n",
            "Step [94/275]:\tLoss: 1.3400321006774902\n",
            "Step [95/275]:\tLoss: 0.32496336102485657\n",
            "Step [96/275]:\tLoss: 0.8609287142753601\n",
            "Step [97/275]:\tLoss: 0.5118213891983032\n",
            "Step [98/275]:\tLoss: 1.0917531251907349\n",
            "Step [99/275]:\tLoss: 0.6919411420822144\n",
            "Step [100/275]:\tLoss: 1.7773716449737549\n",
            "Step [101/275]:\tLoss: 1.223968505859375\n",
            "Step [102/275]:\tLoss: 1.2980101108551025\n",
            "Step [103/275]:\tLoss: 0.5202696323394775\n",
            "Step [104/275]:\tLoss: 0.5189470052719116\n",
            "Step [105/275]:\tLoss: 1.3131572008132935\n",
            "Step [106/275]:\tLoss: 1.0670636892318726\n",
            "Step [107/275]:\tLoss: 0.9183844327926636\n",
            "Step [108/275]:\tLoss: 1.1816192865371704\n",
            "Step [109/275]:\tLoss: 0.9412704110145569\n",
            "Step [110/275]:\tLoss: 2.729828357696533\n",
            "Step [111/275]:\tLoss: 2.0621654987335205\n",
            "Step [112/275]:\tLoss: 2.911843776702881\n",
            "Step [113/275]:\tLoss: 0.8487204313278198\n",
            "Step [114/275]:\tLoss: 2.1422438621520996\n",
            "Step [115/275]:\tLoss: 2.277064085006714\n",
            "Step [116/275]:\tLoss: 1.882117748260498\n",
            "Step [117/275]:\tLoss: 1.77982497215271\n",
            "Step [118/275]:\tLoss: 1.7219669818878174\n",
            "Step [119/275]:\tLoss: 0.570409893989563\n",
            "Step [120/275]:\tLoss: 1.1242804527282715\n",
            "Step [121/275]:\tLoss: 2.6319479942321777\n",
            "Step [122/275]:\tLoss: 1.7283684015274048\n",
            "Step [123/275]:\tLoss: 2.620030641555786\n",
            "Step [124/275]:\tLoss: 1.1009042263031006\n",
            "Step [125/275]:\tLoss: 2.314267635345459\n",
            "Step [126/275]:\tLoss: 2.0168673992156982\n",
            "Step [127/275]:\tLoss: 1.5120699405670166\n",
            "Step [128/275]:\tLoss: 1.564486026763916\n",
            "Step [129/275]:\tLoss: 1.5776491165161133\n",
            "Step [130/275]:\tLoss: 2.123699188232422\n",
            "Step [131/275]:\tLoss: 1.494584321975708\n",
            "Step [132/275]:\tLoss: 2.0181236267089844\n",
            "Step [133/275]:\tLoss: 1.6126221418380737\n",
            "Step [134/275]:\tLoss: 1.3691425323486328\n",
            "Step [135/275]:\tLoss: 2.534191370010376\n",
            "Step [136/275]:\tLoss: 2.1444616317749023\n",
            "Step [137/275]:\tLoss: 3.3415331840515137\n",
            "Step [138/275]:\tLoss: 0.8059240579605103\n",
            "Step [139/275]:\tLoss: 1.6860144138336182\n",
            "Step [140/275]:\tLoss: 1.1835758686065674\n",
            "Step [141/275]:\tLoss: 1.4407732486724854\n",
            "Step [142/275]:\tLoss: 0.9434258937835693\n",
            "Step [143/275]:\tLoss: 1.1636430025100708\n",
            "Step [144/275]:\tLoss: 1.3801344633102417\n",
            "Step [145/275]:\tLoss: 0.9975324273109436\n",
            "Step [146/275]:\tLoss: 0.9478656053543091\n",
            "Step [147/275]:\tLoss: 1.115837812423706\n",
            "Step [148/275]:\tLoss: 0.8812127113342285\n",
            "Step [149/275]:\tLoss: 1.2102513313293457\n",
            "Step [150/275]:\tLoss: 0.8547062277793884\n",
            "Step [151/275]:\tLoss: 0.939781665802002\n",
            "Step [152/275]:\tLoss: 1.379284143447876\n",
            "Step [153/275]:\tLoss: 0.8584998846054077\n",
            "Step [154/275]:\tLoss: 1.4902713298797607\n",
            "Step [155/275]:\tLoss: 0.5113482475280762\n",
            "Step [156/275]:\tLoss: 0.7167115807533264\n",
            "Step [157/275]:\tLoss: 0.7673731446266174\n",
            "Step [158/275]:\tLoss: 1.503363847732544\n",
            "Step [159/275]:\tLoss: 1.2802215814590454\n",
            "Step [160/275]:\tLoss: 1.3200912475585938\n",
            "Step [161/275]:\tLoss: 1.3952714204788208\n",
            "Step [162/275]:\tLoss: 1.3078229427337646\n",
            "Step [163/275]:\tLoss: 0.7031973600387573\n",
            "Step [164/275]:\tLoss: 1.3574689626693726\n",
            "Step [165/275]:\tLoss: 1.1828418970108032\n",
            "Step [166/275]:\tLoss: 1.7310130596160889\n",
            "Step [167/275]:\tLoss: 0.860260009765625\n",
            "Step [168/275]:\tLoss: 0.5301695466041565\n",
            "Step [169/275]:\tLoss: 1.4784969091415405\n",
            "Step [170/275]:\tLoss: 1.0808250904083252\n",
            "Step [171/275]:\tLoss: 1.6009737253189087\n",
            "Step [172/275]:\tLoss: 0.9683406352996826\n",
            "Step [173/275]:\tLoss: 1.5531888008117676\n",
            "Step [174/275]:\tLoss: 0.6168842315673828\n",
            "Step [175/275]:\tLoss: 1.9922051429748535\n",
            "Step [176/275]:\tLoss: 0.8398613333702087\n",
            "Step [177/275]:\tLoss: 1.3658080101013184\n",
            "Step [178/275]:\tLoss: 2.0754125118255615\n",
            "Step [179/275]:\tLoss: 1.2426823377609253\n",
            "Step [180/275]:\tLoss: 0.7378520965576172\n",
            "Step [181/275]:\tLoss: 1.1520297527313232\n",
            "Step [182/275]:\tLoss: 1.028763771057129\n",
            "Step [183/275]:\tLoss: 0.8656245470046997\n",
            "Step [184/275]:\tLoss: 1.001283884048462\n",
            "Step [185/275]:\tLoss: 1.534024715423584\n",
            "Step [186/275]:\tLoss: 1.3673830032348633\n",
            "Step [187/275]:\tLoss: 1.4722940921783447\n",
            "Step [188/275]:\tLoss: 1.5526466369628906\n",
            "Step [189/275]:\tLoss: 1.3695281744003296\n",
            "Step [190/275]:\tLoss: 1.461549997329712\n",
            "Step [191/275]:\tLoss: 1.3485186100006104\n",
            "Step [192/275]:\tLoss: 1.740146279335022\n",
            "Step [193/275]:\tLoss: 1.5481271743774414\n",
            "Step [194/275]:\tLoss: 1.4346323013305664\n",
            "Step [195/275]:\tLoss: 2.220466375350952\n",
            "Step [196/275]:\tLoss: 2.3637161254882812\n",
            "Step [197/275]:\tLoss: 2.1229162216186523\n",
            "Step [198/275]:\tLoss: 2.4937639236450195\n",
            "Step [199/275]:\tLoss: 2.193249225616455\n",
            "Step [200/275]:\tLoss: 1.0871553421020508\n",
            "Step [201/275]:\tLoss: 1.304693341255188\n",
            "Step [202/275]:\tLoss: 2.4331133365631104\n",
            "Step [203/275]:\tLoss: 2.04152250289917\n",
            "Step [204/275]:\tLoss: 1.579468011856079\n",
            "Step [205/275]:\tLoss: 1.7913060188293457\n",
            "Step [206/275]:\tLoss: 1.8801336288452148\n",
            "Step [207/275]:\tLoss: 1.4722120761871338\n",
            "Step [208/275]:\tLoss: 1.5119742155075073\n",
            "Step [209/275]:\tLoss: 1.0865315198898315\n",
            "Step [210/275]:\tLoss: 2.294750690460205\n",
            "Step [211/275]:\tLoss: 1.974785327911377\n",
            "Step [212/275]:\tLoss: 1.9782652854919434\n",
            "Step [213/275]:\tLoss: 1.2670387029647827\n",
            "Step [214/275]:\tLoss: 1.428473711013794\n",
            "Step [215/275]:\tLoss: 1.6571621894836426\n",
            "Step [216/275]:\tLoss: 1.6195480823516846\n",
            "Step [217/275]:\tLoss: 1.7219361066818237\n",
            "Step [218/275]:\tLoss: 1.4645919799804688\n",
            "Step [219/275]:\tLoss: 1.2129456996917725\n",
            "Step [220/275]:\tLoss: 1.5014830827713013\n",
            "Step [221/275]:\tLoss: 1.059849500656128\n",
            "Step [222/275]:\tLoss: 1.691933274269104\n",
            "Step [223/275]:\tLoss: 1.4417033195495605\n",
            "Step [224/275]:\tLoss: 0.8746892213821411\n",
            "Step [225/275]:\tLoss: 1.8016774654388428\n",
            "Step [226/275]:\tLoss: 1.951188564300537\n",
            "Step [227/275]:\tLoss: 0.7974281311035156\n",
            "Step [228/275]:\tLoss: 0.9558161497116089\n",
            "Step [229/275]:\tLoss: 1.1759140491485596\n",
            "Step [230/275]:\tLoss: 1.3322436809539795\n",
            "Step [231/275]:\tLoss: 1.4389595985412598\n",
            "Step [232/275]:\tLoss: 1.6698322296142578\n",
            "Step [233/275]:\tLoss: 0.9931461811065674\n",
            "Step [234/275]:\tLoss: 1.8131945133209229\n",
            "Step [235/275]:\tLoss: 1.9170440435409546\n",
            "Step [236/275]:\tLoss: 1.4194189310073853\n",
            "Step [237/275]:\tLoss: 1.7684926986694336\n",
            "Step [238/275]:\tLoss: 1.1779229640960693\n",
            "Step [239/275]:\tLoss: 1.627824068069458\n",
            "Step [240/275]:\tLoss: 2.0387625694274902\n",
            "Step [241/275]:\tLoss: 1.9400641918182373\n",
            "Step [242/275]:\tLoss: 0.8238461017608643\n",
            "Step [243/275]:\tLoss: 1.765321135520935\n",
            "Step [244/275]:\tLoss: 1.531703233718872\n",
            "Step [245/275]:\tLoss: 1.4420654773712158\n",
            "Step [246/275]:\tLoss: 0.9782777428627014\n",
            "Step [247/275]:\tLoss: 2.014106273651123\n",
            "Step [248/275]:\tLoss: 2.354222536087036\n",
            "Step [249/275]:\tLoss: 1.3620579242706299\n",
            "Step [250/275]:\tLoss: 1.6437098979949951\n",
            "Step [251/275]:\tLoss: 3.3923797607421875\n",
            "Step [252/275]:\tLoss: 1.1325346231460571\n",
            "Step [253/275]:\tLoss: 1.5916035175323486\n",
            "Step [254/275]:\tLoss: 2.0860395431518555\n",
            "Step [255/275]:\tLoss: 2.1685450077056885\n",
            "Step [256/275]:\tLoss: 2.168015718460083\n",
            "Step [257/275]:\tLoss: 2.394773006439209\n",
            "Step [258/275]:\tLoss: 2.0908117294311523\n",
            "Step [259/275]:\tLoss: 1.530243158340454\n",
            "Step [260/275]:\tLoss: 1.4981908798217773\n",
            "Step [261/275]:\tLoss: 1.9211645126342773\n",
            "Step [262/275]:\tLoss: 3.6820220947265625\n",
            "Step [263/275]:\tLoss: 3.1988139152526855\n",
            "Step [264/275]:\tLoss: 1.5959099531173706\n",
            "Step [265/275]:\tLoss: 2.407917022705078\n",
            "Step [266/275]:\tLoss: 2.1140902042388916\n",
            "Step [267/275]:\tLoss: 1.2854218482971191\n",
            "Step [268/275]:\tLoss: 1.4517464637756348\n",
            "Step [269/275]:\tLoss: 2.074968099594116\n",
            "Step [270/275]:\tLoss: 1.7392699718475342\n",
            "Step [271/275]:\tLoss: 2.028550624847412\n",
            "Step [272/275]:\tLoss: 1.9399418830871582\n",
            "Step [273/275]:\tLoss: 1.3363757133483887\n",
            "Step [274/275]:\tLoss: 1.6679093837738037\n",
            "epoch number 100\n",
            "Step [0/275]:\tLoss: 2.6377663612365723\n",
            "Step [1/275]:\tLoss: 2.0092380046844482\n",
            "Step [2/275]:\tLoss: 1.8023691177368164\n",
            "Step [3/275]:\tLoss: 2.4092371463775635\n",
            "Step [4/275]:\tLoss: 1.8644914627075195\n",
            "Step [5/275]:\tLoss: 2.4504125118255615\n",
            "Step [6/275]:\tLoss: 2.1315817832946777\n",
            "Step [7/275]:\tLoss: 1.8496612310409546\n",
            "Step [8/275]:\tLoss: 1.5681629180908203\n",
            "Step [9/275]:\tLoss: 1.7314459085464478\n",
            "Step [10/275]:\tLoss: 1.4812290668487549\n",
            "Step [11/275]:\tLoss: 1.425138235092163\n",
            "Step [12/275]:\tLoss: 1.7311193943023682\n",
            "Step [13/275]:\tLoss: 2.0821704864501953\n",
            "Step [14/275]:\tLoss: 1.2598316669464111\n",
            "Step [15/275]:\tLoss: 1.4512274265289307\n",
            "Step [16/275]:\tLoss: 0.7798652648925781\n",
            "Step [17/275]:\tLoss: 2.1641159057617188\n",
            "Step [18/275]:\tLoss: 2.586688995361328\n",
            "Step [19/275]:\tLoss: 2.119126319885254\n",
            "Step [20/275]:\tLoss: 0.5249806642532349\n",
            "Step [21/275]:\tLoss: 1.8969464302062988\n",
            "Step [22/275]:\tLoss: 0.7719614505767822\n",
            "Step [23/275]:\tLoss: 2.41756534576416\n",
            "Step [24/275]:\tLoss: 2.779176712036133\n",
            "Step [25/275]:\tLoss: 1.2573943138122559\n",
            "Step [26/275]:\tLoss: 1.445597767829895\n",
            "Step [27/275]:\tLoss: 0.982236921787262\n",
            "Step [28/275]:\tLoss: 1.6530311107635498\n",
            "Step [29/275]:\tLoss: 1.4258815050125122\n",
            "Step [30/275]:\tLoss: 1.4740068912506104\n",
            "Step [31/275]:\tLoss: 1.0952271223068237\n",
            "Step [32/275]:\tLoss: 1.0019340515136719\n",
            "Step [33/275]:\tLoss: 0.9214823246002197\n",
            "Step [34/275]:\tLoss: 0.7710004448890686\n",
            "Step [35/275]:\tLoss: 1.0473086833953857\n",
            "Step [36/275]:\tLoss: 0.50935298204422\n",
            "Step [37/275]:\tLoss: 0.9488407969474792\n",
            "Step [38/275]:\tLoss: 1.783519983291626\n",
            "Step [39/275]:\tLoss: 1.2975530624389648\n",
            "Step [40/275]:\tLoss: 0.9100073575973511\n",
            "Step [41/275]:\tLoss: 0.95155268907547\n",
            "Step [42/275]:\tLoss: 1.0150724649429321\n",
            "Step [43/275]:\tLoss: 1.3022685050964355\n",
            "Step [44/275]:\tLoss: 1.096403956413269\n",
            "Step [45/275]:\tLoss: 1.3333072662353516\n",
            "Step [46/275]:\tLoss: 1.1875985860824585\n",
            "Step [47/275]:\tLoss: 1.7526366710662842\n",
            "Step [48/275]:\tLoss: 0.7639787197113037\n",
            "Step [49/275]:\tLoss: 1.2211544513702393\n",
            "Step [50/275]:\tLoss: 1.2536449432373047\n",
            "Step [51/275]:\tLoss: 1.4377830028533936\n",
            "Step [52/275]:\tLoss: 2.100820541381836\n",
            "Step [53/275]:\tLoss: 1.3473113775253296\n",
            "Step [54/275]:\tLoss: 1.6240308284759521\n",
            "Step [55/275]:\tLoss: 1.5085384845733643\n",
            "Step [56/275]:\tLoss: 1.1865030527114868\n",
            "Step [57/275]:\tLoss: 1.190041184425354\n",
            "Step [58/275]:\tLoss: 0.9291342496871948\n",
            "Step [59/275]:\tLoss: 1.346927285194397\n",
            "Step [60/275]:\tLoss: 1.1425254344940186\n",
            "Step [61/275]:\tLoss: 0.7346540689468384\n",
            "Step [62/275]:\tLoss: 0.7388842105865479\n",
            "Step [63/275]:\tLoss: 1.1154775619506836\n",
            "Step [64/275]:\tLoss: 0.7452151775360107\n",
            "Step [65/275]:\tLoss: 1.4394980669021606\n",
            "Step [66/275]:\tLoss: 0.4960997402667999\n",
            "Step [67/275]:\tLoss: 0.49365490674972534\n",
            "Step [68/275]:\tLoss: 0.950961172580719\n",
            "Step [69/275]:\tLoss: 1.1574134826660156\n",
            "Step [70/275]:\tLoss: 1.1435153484344482\n",
            "Step [71/275]:\tLoss: 0.9269238710403442\n",
            "Step [72/275]:\tLoss: 1.0733232498168945\n",
            "Step [73/275]:\tLoss: 0.8781346082687378\n",
            "Step [74/275]:\tLoss: 0.714216411113739\n",
            "Step [75/275]:\tLoss: 0.800032377243042\n",
            "Step [76/275]:\tLoss: 0.8573822379112244\n",
            "Step [77/275]:\tLoss: 0.7605476975440979\n",
            "Step [78/275]:\tLoss: 0.635971188545227\n",
            "Step [79/275]:\tLoss: 0.591452419757843\n",
            "Step [80/275]:\tLoss: 1.4361910820007324\n",
            "Step [81/275]:\tLoss: 0.585277259349823\n",
            "Step [82/275]:\tLoss: 0.8664884567260742\n",
            "Step [83/275]:\tLoss: 1.335118055343628\n",
            "Step [84/275]:\tLoss: 0.6967843770980835\n",
            "Step [85/275]:\tLoss: 0.9218133687973022\n",
            "Step [86/275]:\tLoss: 1.3380811214447021\n",
            "Step [87/275]:\tLoss: 1.0525965690612793\n",
            "Step [88/275]:\tLoss: 0.7731071710586548\n",
            "Step [89/275]:\tLoss: 0.9870508909225464\n",
            "Step [90/275]:\tLoss: 0.6969835758209229\n",
            "Step [91/275]:\tLoss: 0.962199866771698\n",
            "Step [92/275]:\tLoss: 0.8075412511825562\n",
            "Step [93/275]:\tLoss: 0.3722134530544281\n",
            "Step [94/275]:\tLoss: 1.0805048942565918\n",
            "Step [95/275]:\tLoss: 0.5687226057052612\n",
            "Step [96/275]:\tLoss: 1.1442962884902954\n",
            "Step [97/275]:\tLoss: 0.9977357983589172\n",
            "Step [98/275]:\tLoss: 1.0134717226028442\n",
            "Step [99/275]:\tLoss: 0.962942898273468\n",
            "Step [100/275]:\tLoss: 0.7587394714355469\n",
            "Step [101/275]:\tLoss: 0.4438285827636719\n",
            "Step [102/275]:\tLoss: 1.0360054969787598\n",
            "Step [103/275]:\tLoss: 1.5827982425689697\n",
            "Step [104/275]:\tLoss: 0.7585567235946655\n",
            "Step [105/275]:\tLoss: 0.9064782857894897\n",
            "Step [106/275]:\tLoss: 1.6883677244186401\n",
            "Step [107/275]:\tLoss: 1.2820717096328735\n",
            "Step [108/275]:\tLoss: 1.0572326183319092\n",
            "Step [109/275]:\tLoss: 0.982219934463501\n",
            "Step [110/275]:\tLoss: 2.5173699855804443\n",
            "Step [111/275]:\tLoss: 3.225090503692627\n",
            "Step [112/275]:\tLoss: 2.408154249191284\n",
            "Step [113/275]:\tLoss: 1.981343388557434\n",
            "Step [114/275]:\tLoss: 2.34771728515625\n",
            "Step [115/275]:\tLoss: 2.5196356773376465\n",
            "Step [116/275]:\tLoss: 1.9609858989715576\n",
            "Step [117/275]:\tLoss: 2.062251567840576\n",
            "Step [118/275]:\tLoss: 1.4241282939910889\n",
            "Step [119/275]:\tLoss: 1.0340665578842163\n",
            "Step [120/275]:\tLoss: 0.7535362243652344\n",
            "Step [121/275]:\tLoss: 0.8544752597808838\n",
            "Step [122/275]:\tLoss: 2.878885269165039\n",
            "Step [123/275]:\tLoss: 1.25711989402771\n",
            "Step [124/275]:\tLoss: 3.241408109664917\n",
            "Step [125/275]:\tLoss: 2.5179824829101562\n",
            "Step [126/275]:\tLoss: 1.1318259239196777\n",
            "Step [127/275]:\tLoss: 0.9436124563217163\n",
            "Step [128/275]:\tLoss: 2.146293878555298\n",
            "Step [129/275]:\tLoss: 1.6564899682998657\n",
            "Step [130/275]:\tLoss: 2.3288893699645996\n",
            "Step [131/275]:\tLoss: 1.3518483638763428\n",
            "Step [132/275]:\tLoss: 1.6351237297058105\n",
            "Step [133/275]:\tLoss: 1.8640639781951904\n",
            "Step [134/275]:\tLoss: 1.5483139753341675\n",
            "Step [135/275]:\tLoss: 2.9524214267730713\n",
            "Step [136/275]:\tLoss: 2.391847848892212\n",
            "Step [137/275]:\tLoss: 0.9930022954940796\n",
            "Step [138/275]:\tLoss: 0.5701903104782104\n",
            "Step [139/275]:\tLoss: 1.2620022296905518\n",
            "Step [140/275]:\tLoss: 0.7765654921531677\n",
            "Step [141/275]:\tLoss: 0.9471490979194641\n",
            "Step [142/275]:\tLoss: 0.7513346672058105\n",
            "Step [143/275]:\tLoss: 0.6382762789726257\n",
            "Step [144/275]:\tLoss: 1.315920352935791\n",
            "Step [145/275]:\tLoss: 0.48183542490005493\n",
            "Step [146/275]:\tLoss: 0.9729799032211304\n",
            "Step [147/275]:\tLoss: 0.7725638151168823\n",
            "Step [148/275]:\tLoss: 0.9991128444671631\n",
            "Step [149/275]:\tLoss: 1.0639127492904663\n",
            "Step [150/275]:\tLoss: 0.804419219493866\n",
            "Step [151/275]:\tLoss: 1.3639644384384155\n",
            "Step [152/275]:\tLoss: 1.0752097368240356\n",
            "Step [153/275]:\tLoss: 0.6860877275466919\n",
            "Step [154/275]:\tLoss: 0.7646818161010742\n",
            "Step [155/275]:\tLoss: 1.211747169494629\n",
            "Step [156/275]:\tLoss: 0.8991237282752991\n",
            "Step [157/275]:\tLoss: 0.34740352630615234\n",
            "Step [158/275]:\tLoss: 0.6878154277801514\n",
            "Step [159/275]:\tLoss: 0.6140390634536743\n",
            "Step [160/275]:\tLoss: 1.57763671875\n",
            "Step [161/275]:\tLoss: 1.289568543434143\n",
            "Step [162/275]:\tLoss: 0.9655516743659973\n",
            "Step [163/275]:\tLoss: 1.1271724700927734\n",
            "Step [164/275]:\tLoss: 0.7711499333381653\n",
            "Step [165/275]:\tLoss: 0.7612115144729614\n",
            "Step [166/275]:\tLoss: 1.5000981092453003\n",
            "Step [167/275]:\tLoss: 1.4026795625686646\n",
            "Step [168/275]:\tLoss: 0.9173332452774048\n",
            "Step [169/275]:\tLoss: 1.2660534381866455\n",
            "Step [170/275]:\tLoss: 1.05292546749115\n",
            "Step [171/275]:\tLoss: 1.1959881782531738\n",
            "Step [172/275]:\tLoss: 0.82027667760849\n",
            "Step [173/275]:\tLoss: 1.0813076496124268\n",
            "Step [174/275]:\tLoss: 1.5696439743041992\n",
            "Step [175/275]:\tLoss: 2.627011775970459\n",
            "Step [176/275]:\tLoss: 0.881201982498169\n",
            "Step [177/275]:\tLoss: 0.6514631509780884\n",
            "Step [178/275]:\tLoss: 1.1285303831100464\n",
            "Step [179/275]:\tLoss: 1.2952868938446045\n",
            "Step [180/275]:\tLoss: 1.0142754316329956\n",
            "Step [181/275]:\tLoss: 1.086525321006775\n",
            "Step [182/275]:\tLoss: 0.972976803779602\n",
            "Step [183/275]:\tLoss: 1.1608794927597046\n",
            "Step [184/275]:\tLoss: 1.2467354536056519\n",
            "Step [185/275]:\tLoss: 1.199357032775879\n",
            "Step [186/275]:\tLoss: 1.6017272472381592\n",
            "Step [187/275]:\tLoss: 0.9171800017356873\n",
            "Step [188/275]:\tLoss: 0.7782540321350098\n",
            "Step [189/275]:\tLoss: 0.8539345860481262\n",
            "Step [190/275]:\tLoss: 1.4304888248443604\n",
            "Step [191/275]:\tLoss: 1.642835259437561\n",
            "Step [192/275]:\tLoss: 1.8283157348632812\n",
            "Step [193/275]:\tLoss: 2.8597912788391113\n",
            "Step [194/275]:\tLoss: 2.057325839996338\n",
            "Step [195/275]:\tLoss: 2.248185157775879\n",
            "Step [196/275]:\tLoss: 3.162623405456543\n",
            "Step [197/275]:\tLoss: 2.516129493713379\n",
            "Step [198/275]:\tLoss: 1.9885978698730469\n",
            "Step [199/275]:\tLoss: 1.5758075714111328\n",
            "Step [200/275]:\tLoss: 3.1836938858032227\n",
            "Step [201/275]:\tLoss: 1.638559103012085\n",
            "Step [202/275]:\tLoss: 2.1262567043304443\n",
            "Step [203/275]:\tLoss: 1.5416676998138428\n",
            "Step [204/275]:\tLoss: 2.042701482772827\n",
            "Step [205/275]:\tLoss: 1.0764986276626587\n",
            "Step [206/275]:\tLoss: 2.0425796508789062\n",
            "Step [207/275]:\tLoss: 2.29103422164917\n",
            "Step [208/275]:\tLoss: 2.04465913772583\n",
            "Step [209/275]:\tLoss: 1.930762767791748\n",
            "Step [210/275]:\tLoss: 1.8298790454864502\n",
            "Step [211/275]:\tLoss: 2.039849281311035\n",
            "Step [212/275]:\tLoss: 1.944041132926941\n",
            "Step [213/275]:\tLoss: 2.003875494003296\n",
            "Step [214/275]:\tLoss: 1.5010828971862793\n",
            "Step [215/275]:\tLoss: 1.5883450508117676\n",
            "Step [216/275]:\tLoss: 2.115689516067505\n",
            "Step [217/275]:\tLoss: 2.0529773235321045\n",
            "Step [218/275]:\tLoss: 2.2288570404052734\n",
            "Step [219/275]:\tLoss: 1.5099903345108032\n",
            "Step [220/275]:\tLoss: 1.8185372352600098\n",
            "Step [221/275]:\tLoss: 1.4017791748046875\n",
            "Step [222/275]:\tLoss: 2.7887980937957764\n",
            "Step [223/275]:\tLoss: 1.144529104232788\n",
            "Step [224/275]:\tLoss: 1.5768933296203613\n",
            "Step [225/275]:\tLoss: 1.5805673599243164\n",
            "Step [226/275]:\tLoss: 1.8888829946517944\n",
            "Step [227/275]:\tLoss: 1.6240692138671875\n",
            "Step [228/275]:\tLoss: 0.9024084806442261\n",
            "Step [229/275]:\tLoss: 1.432612657546997\n",
            "Step [230/275]:\tLoss: 1.8720176219940186\n",
            "Step [231/275]:\tLoss: 1.8035800457000732\n",
            "Step [232/275]:\tLoss: 2.249666213989258\n",
            "Step [233/275]:\tLoss: 1.4961684942245483\n",
            "Step [234/275]:\tLoss: 2.428337574005127\n",
            "Step [235/275]:\tLoss: 2.1535048484802246\n",
            "Step [236/275]:\tLoss: 1.3475770950317383\n",
            "Step [237/275]:\tLoss: 2.1177444458007812\n",
            "Step [238/275]:\tLoss: 1.9966320991516113\n",
            "Step [239/275]:\tLoss: 1.5790319442749023\n",
            "Step [240/275]:\tLoss: 1.5614519119262695\n",
            "Step [241/275]:\tLoss: 1.5708410739898682\n",
            "Step [242/275]:\tLoss: 1.7744319438934326\n",
            "Step [243/275]:\tLoss: 2.3298850059509277\n",
            "Step [244/275]:\tLoss: 1.9959361553192139\n",
            "Step [245/275]:\tLoss: 1.4195046424865723\n",
            "Step [246/275]:\tLoss: 1.5033881664276123\n",
            "Step [247/275]:\tLoss: 1.3658784627914429\n",
            "Step [248/275]:\tLoss: 1.2508138418197632\n",
            "Step [249/275]:\tLoss: 2.1103272438049316\n",
            "Step [250/275]:\tLoss: 1.356445550918579\n",
            "Step [251/275]:\tLoss: 2.574917793273926\n",
            "Step [252/275]:\tLoss: 1.8478524684906006\n",
            "Step [253/275]:\tLoss: 1.5473284721374512\n",
            "Step [254/275]:\tLoss: 3.0213756561279297\n",
            "Step [255/275]:\tLoss: 3.46022891998291\n",
            "Step [256/275]:\tLoss: 1.4728703498840332\n",
            "Step [257/275]:\tLoss: 1.6151074171066284\n",
            "Step [258/275]:\tLoss: 1.606130838394165\n",
            "Step [259/275]:\tLoss: 0.9807234406471252\n",
            "Step [260/275]:\tLoss: 2.1473917961120605\n",
            "Step [261/275]:\tLoss: 2.3011913299560547\n",
            "Step [262/275]:\tLoss: 2.0625860691070557\n",
            "Step [263/275]:\tLoss: 2.4932382106781006\n",
            "Step [264/275]:\tLoss: 1.9629905223846436\n",
            "Step [265/275]:\tLoss: 1.694258451461792\n",
            "Step [266/275]:\tLoss: 2.0744009017944336\n",
            "Step [267/275]:\tLoss: 1.5916119813919067\n",
            "Step [268/275]:\tLoss: 1.9085438251495361\n",
            "Step [269/275]:\tLoss: 2.2984509468078613\n",
            "Step [270/275]:\tLoss: 1.4577109813690186\n",
            "Step [271/275]:\tLoss: 2.610238552093506\n",
            "Step [272/275]:\tLoss: 1.224909782409668\n",
            "Step [273/275]:\tLoss: 1.9201213121414185\n",
            "Step [274/275]:\tLoss: 1.6450092792510986\n",
            "Saving model at epoch 100\n",
            "epoch number 101\n",
            "Step [0/275]:\tLoss: 1.7800054550170898\n",
            "Step [1/275]:\tLoss: 2.153353452682495\n",
            "Step [2/275]:\tLoss: 0.5438070893287659\n",
            "Step [3/275]:\tLoss: 0.9485695362091064\n",
            "Step [4/275]:\tLoss: 0.819677472114563\n",
            "Step [5/275]:\tLoss: 3.649461269378662\n",
            "Step [6/275]:\tLoss: 1.4602875709533691\n",
            "Step [7/275]:\tLoss: 1.1785564422607422\n",
            "Step [8/275]:\tLoss: 2.4048399925231934\n",
            "Step [9/275]:\tLoss: 3.652986526489258\n",
            "Step [10/275]:\tLoss: 1.341895341873169\n",
            "Step [11/275]:\tLoss: 0.5663096904754639\n",
            "Step [12/275]:\tLoss: 0.3149632513523102\n",
            "Step [13/275]:\tLoss: 0.9010540843009949\n",
            "Step [14/275]:\tLoss: 0.8522377014160156\n",
            "Step [15/275]:\tLoss: 1.0161913633346558\n",
            "Step [16/275]:\tLoss: 0.8851104974746704\n",
            "Step [17/275]:\tLoss: 0.3144024610519409\n",
            "Step [18/275]:\tLoss: 1.5954904556274414\n",
            "Step [19/275]:\tLoss: 1.740158200263977\n",
            "Step [20/275]:\tLoss: 0.7613599300384521\n",
            "Step [21/275]:\tLoss: 1.414257526397705\n",
            "Step [22/275]:\tLoss: 0.8494758605957031\n",
            "Step [23/275]:\tLoss: 2.263688802719116\n",
            "Step [24/275]:\tLoss: 1.4830759763717651\n",
            "Step [25/275]:\tLoss: 2.7983174324035645\n",
            "Step [26/275]:\tLoss: 0.956638514995575\n",
            "Step [27/275]:\tLoss: 1.5272400379180908\n",
            "Step [28/275]:\tLoss: 3.134614944458008\n",
            "Step [29/275]:\tLoss: 1.136624813079834\n",
            "Step [30/275]:\tLoss: 0.9911993741989136\n",
            "Step [31/275]:\tLoss: 0.5823911428451538\n",
            "Step [32/275]:\tLoss: 0.7937551140785217\n",
            "Step [33/275]:\tLoss: 0.7714167237281799\n",
            "Step [34/275]:\tLoss: 1.0574284791946411\n",
            "Step [35/275]:\tLoss: 0.5887259244918823\n",
            "Step [36/275]:\tLoss: 0.4527142643928528\n",
            "Step [37/275]:\tLoss: 1.4004652500152588\n",
            "Step [38/275]:\tLoss: 1.221942663192749\n",
            "Step [39/275]:\tLoss: 0.980077862739563\n",
            "Step [40/275]:\tLoss: 1.0173845291137695\n",
            "Step [41/275]:\tLoss: 0.5915910005569458\n",
            "Step [42/275]:\tLoss: 0.6216499209403992\n",
            "Step [43/275]:\tLoss: 1.0683014392852783\n",
            "Step [44/275]:\tLoss: 0.5776540040969849\n",
            "Step [45/275]:\tLoss: 0.44070446491241455\n",
            "Step [46/275]:\tLoss: 0.5092393159866333\n",
            "Step [47/275]:\tLoss: 0.8031197190284729\n",
            "Step [48/275]:\tLoss: 0.8951647877693176\n",
            "Step [49/275]:\tLoss: 0.47278332710266113\n",
            "Step [50/275]:\tLoss: 1.0582780838012695\n",
            "Step [51/275]:\tLoss: 1.1962034702301025\n",
            "Step [52/275]:\tLoss: 0.592089831829071\n",
            "Step [53/275]:\tLoss: 1.4151942729949951\n",
            "Step [54/275]:\tLoss: 1.9731619358062744\n",
            "Step [55/275]:\tLoss: 0.3485791087150574\n",
            "Step [56/275]:\tLoss: 0.4126361906528473\n",
            "Step [57/275]:\tLoss: 0.35066401958465576\n",
            "Step [58/275]:\tLoss: 1.2330639362335205\n",
            "Step [59/275]:\tLoss: 0.7318063974380493\n",
            "Step [60/275]:\tLoss: 1.1563916206359863\n",
            "Step [61/275]:\tLoss: 0.6371595859527588\n",
            "Step [62/275]:\tLoss: 1.0846909284591675\n",
            "Step [63/275]:\tLoss: 0.8988681435585022\n",
            "Step [64/275]:\tLoss: 1.148376226425171\n",
            "Step [65/275]:\tLoss: 0.9240065813064575\n",
            "Step [66/275]:\tLoss: 0.7042844891548157\n",
            "Step [67/275]:\tLoss: 0.43978798389434814\n",
            "Step [68/275]:\tLoss: 0.7069152593612671\n",
            "Step [69/275]:\tLoss: 0.7051310539245605\n",
            "Step [70/275]:\tLoss: 0.6023428440093994\n",
            "Step [71/275]:\tLoss: 0.37837696075439453\n",
            "Step [72/275]:\tLoss: 1.1527495384216309\n",
            "Step [73/275]:\tLoss: 1.2358214855194092\n",
            "Step [74/275]:\tLoss: 1.3643826246261597\n",
            "Step [75/275]:\tLoss: 1.3165161609649658\n",
            "Step [76/275]:\tLoss: 0.5328778624534607\n",
            "Step [77/275]:\tLoss: 1.1601420640945435\n",
            "Step [78/275]:\tLoss: 1.346346378326416\n",
            "Step [79/275]:\tLoss: 0.9222605228424072\n",
            "Step [80/275]:\tLoss: 0.9020739793777466\n",
            "Step [81/275]:\tLoss: 0.8271284103393555\n",
            "Step [82/275]:\tLoss: 1.2888545989990234\n",
            "Step [83/275]:\tLoss: 0.4286381006240845\n",
            "Step [84/275]:\tLoss: 0.832375705242157\n",
            "Step [85/275]:\tLoss: 0.711941659450531\n",
            "Step [86/275]:\tLoss: 1.3819013833999634\n",
            "Step [87/275]:\tLoss: 0.6727356910705566\n",
            "Step [88/275]:\tLoss: 0.6605965495109558\n",
            "Step [89/275]:\tLoss: 0.8755156993865967\n",
            "Step [90/275]:\tLoss: 0.96876060962677\n",
            "Step [91/275]:\tLoss: 1.2965033054351807\n",
            "Step [92/275]:\tLoss: 1.0189571380615234\n",
            "Step [93/275]:\tLoss: 1.2939482927322388\n",
            "Step [94/275]:\tLoss: 0.6490087509155273\n",
            "Step [95/275]:\tLoss: 0.5300579071044922\n",
            "Step [96/275]:\tLoss: 0.4874602258205414\n",
            "Step [97/275]:\tLoss: 0.9554954171180725\n",
            "Step [98/275]:\tLoss: 0.9953668117523193\n",
            "Step [99/275]:\tLoss: 0.8940690755844116\n",
            "Step [100/275]:\tLoss: 0.6401181221008301\n",
            "Step [101/275]:\tLoss: 1.3463811874389648\n",
            "Step [102/275]:\tLoss: 0.5576183199882507\n",
            "Step [103/275]:\tLoss: 0.8148825168609619\n",
            "Step [104/275]:\tLoss: 0.9233399033546448\n",
            "Step [105/275]:\tLoss: 1.2228426933288574\n",
            "Step [106/275]:\tLoss: 1.4119634628295898\n",
            "Step [107/275]:\tLoss: 1.9164904356002808\n",
            "Step [108/275]:\tLoss: 0.6567549705505371\n",
            "Step [109/275]:\tLoss: 0.47600048780441284\n",
            "Step [110/275]:\tLoss: 2.8301339149475098\n",
            "Step [111/275]:\tLoss: 2.693596601486206\n",
            "Step [112/275]:\tLoss: 1.7817533016204834\n",
            "Step [113/275]:\tLoss: 1.7940423488616943\n",
            "Step [114/275]:\tLoss: 2.83836030960083\n",
            "Step [115/275]:\tLoss: 2.9283199310302734\n",
            "Step [116/275]:\tLoss: 1.7058571577072144\n",
            "Step [117/275]:\tLoss: 1.571192741394043\n",
            "Step [118/275]:\tLoss: 1.4432933330535889\n",
            "Step [119/275]:\tLoss: 0.9684065580368042\n",
            "Step [120/275]:\tLoss: 0.9389451742172241\n",
            "Step [121/275]:\tLoss: 0.4444425702095032\n",
            "Step [122/275]:\tLoss: 0.8910210132598877\n",
            "Step [123/275]:\tLoss: 1.0232281684875488\n",
            "Step [124/275]:\tLoss: 1.7701817750930786\n",
            "Step [125/275]:\tLoss: 2.6245739459991455\n",
            "Step [126/275]:\tLoss: 1.774569034576416\n",
            "Step [127/275]:\tLoss: 2.133366584777832\n",
            "Step [128/275]:\tLoss: 1.5814145803451538\n",
            "Step [129/275]:\tLoss: 1.3529261350631714\n",
            "Step [130/275]:\tLoss: 1.6802260875701904\n",
            "Step [131/275]:\tLoss: 2.2704100608825684\n",
            "Step [132/275]:\tLoss: 2.3378422260284424\n",
            "Step [133/275]:\tLoss: 0.966988205909729\n",
            "Step [134/275]:\tLoss: 2.4834325313568115\n",
            "Step [135/275]:\tLoss: 1.9880855083465576\n",
            "Step [136/275]:\tLoss: 1.4007264375686646\n",
            "Step [137/275]:\tLoss: 1.8461899757385254\n",
            "Step [138/275]:\tLoss: 0.4963482916355133\n",
            "Step [139/275]:\tLoss: 1.3276331424713135\n",
            "Step [140/275]:\tLoss: 1.2232701778411865\n",
            "Step [141/275]:\tLoss: 0.8591316938400269\n",
            "Step [142/275]:\tLoss: 0.4111419916152954\n",
            "Step [143/275]:\tLoss: 0.5590173602104187\n",
            "Step [144/275]:\tLoss: 0.718504786491394\n",
            "Step [145/275]:\tLoss: 0.608984649181366\n",
            "Step [146/275]:\tLoss: 1.526648998260498\n",
            "Step [147/275]:\tLoss: 0.947986364364624\n",
            "Step [148/275]:\tLoss: 0.5474417209625244\n",
            "Step [149/275]:\tLoss: 0.8268494606018066\n",
            "Step [150/275]:\tLoss: 0.989912211894989\n",
            "Step [151/275]:\tLoss: 1.0305356979370117\n",
            "Step [152/275]:\tLoss: 1.3343157768249512\n",
            "Step [153/275]:\tLoss: 1.3561820983886719\n",
            "Step [154/275]:\tLoss: 0.7179413437843323\n",
            "Step [155/275]:\tLoss: 0.7216118574142456\n",
            "Step [156/275]:\tLoss: 0.47944170236587524\n",
            "Step [157/275]:\tLoss: 0.9397676587104797\n",
            "Step [158/275]:\tLoss: 1.2306792736053467\n",
            "Step [159/275]:\tLoss: 0.54996657371521\n",
            "Step [160/275]:\tLoss: 0.6151860952377319\n",
            "Step [161/275]:\tLoss: 0.7358711361885071\n",
            "Step [162/275]:\tLoss: 0.5963866114616394\n",
            "Step [163/275]:\tLoss: 0.6545060873031616\n",
            "Step [164/275]:\tLoss: 0.8776358366012573\n",
            "Step [165/275]:\tLoss: 1.1739091873168945\n",
            "Step [166/275]:\tLoss: 1.4358501434326172\n",
            "Step [167/275]:\tLoss: 1.0089778900146484\n",
            "Step [168/275]:\tLoss: 0.6345175504684448\n",
            "Step [169/275]:\tLoss: 1.3449060916900635\n",
            "Step [170/275]:\tLoss: 0.6195736527442932\n",
            "Step [171/275]:\tLoss: 1.3928024768829346\n",
            "Step [172/275]:\tLoss: 1.2982399463653564\n",
            "Step [173/275]:\tLoss: 0.8832284212112427\n",
            "Step [174/275]:\tLoss: 1.0401707887649536\n",
            "Step [175/275]:\tLoss: 1.2054704427719116\n",
            "Step [176/275]:\tLoss: 0.9759808778762817\n",
            "Step [177/275]:\tLoss: 0.8806812763214111\n",
            "Step [178/275]:\tLoss: 1.1656943559646606\n",
            "Step [179/275]:\tLoss: 1.0464107990264893\n",
            "Step [180/275]:\tLoss: 1.0160496234893799\n",
            "Step [181/275]:\tLoss: 0.6522204875946045\n",
            "Step [182/275]:\tLoss: 0.8085362911224365\n",
            "Step [183/275]:\tLoss: 0.4212755262851715\n",
            "Step [184/275]:\tLoss: 0.6401751041412354\n",
            "Step [185/275]:\tLoss: 0.4368426203727722\n",
            "Step [186/275]:\tLoss: 0.8364243507385254\n",
            "Step [187/275]:\tLoss: 1.120762586593628\n",
            "Step [188/275]:\tLoss: 1.142624855041504\n",
            "Step [189/275]:\tLoss: 0.7775560617446899\n",
            "Step [190/275]:\tLoss: 1.0365796089172363\n",
            "Step [191/275]:\tLoss: 1.4907002449035645\n",
            "Step [192/275]:\tLoss: 1.18349027633667\n",
            "Step [193/275]:\tLoss: 2.032956123352051\n",
            "Step [194/275]:\tLoss: 2.8505444526672363\n",
            "Step [195/275]:\tLoss: 2.9789016246795654\n",
            "Step [196/275]:\tLoss: 3.0517520904541016\n",
            "Step [197/275]:\tLoss: 1.7376868724822998\n",
            "Step [198/275]:\tLoss: 1.7942408323287964\n",
            "Step [199/275]:\tLoss: 2.467751979827881\n",
            "Step [200/275]:\tLoss: 1.8185358047485352\n",
            "Step [201/275]:\tLoss: 1.6833539009094238\n",
            "Step [202/275]:\tLoss: 1.7862130403518677\n",
            "Step [203/275]:\tLoss: 2.3057079315185547\n",
            "Step [204/275]:\tLoss: 2.06478214263916\n",
            "Step [205/275]:\tLoss: 2.2540204524993896\n",
            "Step [206/275]:\tLoss: 1.8979450464248657\n",
            "Step [207/275]:\tLoss: 0.9960575699806213\n",
            "Step [208/275]:\tLoss: 1.9868158102035522\n",
            "Step [209/275]:\tLoss: 2.026358127593994\n",
            "Step [210/275]:\tLoss: 2.0635106563568115\n",
            "Step [211/275]:\tLoss: 1.3949792385101318\n",
            "Step [212/275]:\tLoss: 1.898430585861206\n",
            "Step [213/275]:\tLoss: 1.6565907001495361\n",
            "Step [214/275]:\tLoss: 1.6912169456481934\n",
            "Step [215/275]:\tLoss: 1.3771734237670898\n",
            "Step [216/275]:\tLoss: 1.7136311531066895\n",
            "Step [217/275]:\tLoss: 2.461134433746338\n",
            "Step [218/275]:\tLoss: 1.8192472457885742\n",
            "Step [219/275]:\tLoss: 1.7954590320587158\n",
            "Step [220/275]:\tLoss: 2.918745517730713\n",
            "Step [221/275]:\tLoss: 1.508026361465454\n",
            "Step [222/275]:\tLoss: 1.7485384941101074\n",
            "Step [223/275]:\tLoss: 1.1350841522216797\n",
            "Step [224/275]:\tLoss: 1.4874122142791748\n",
            "Step [225/275]:\tLoss: 1.5885239839553833\n",
            "Step [226/275]:\tLoss: 1.4751694202423096\n",
            "Step [227/275]:\tLoss: 1.1826155185699463\n",
            "Step [228/275]:\tLoss: 1.862668752670288\n",
            "Step [229/275]:\tLoss: 1.0693254470825195\n",
            "Step [230/275]:\tLoss: 1.2548911571502686\n",
            "Step [231/275]:\tLoss: 1.749734878540039\n",
            "Step [232/275]:\tLoss: 1.363402247428894\n",
            "Step [233/275]:\tLoss: 2.548112392425537\n",
            "Step [234/275]:\tLoss: 1.5896849632263184\n",
            "Step [235/275]:\tLoss: 1.2826976776123047\n",
            "Step [236/275]:\tLoss: 1.2742829322814941\n",
            "Step [237/275]:\tLoss: 1.1443243026733398\n",
            "Step [238/275]:\tLoss: 1.4137248992919922\n",
            "Step [239/275]:\tLoss: 1.1743199825286865\n",
            "Step [240/275]:\tLoss: 2.0446901321411133\n",
            "Step [241/275]:\tLoss: 2.126673698425293\n",
            "Step [242/275]:\tLoss: 1.1889536380767822\n",
            "Step [243/275]:\tLoss: 1.4089759588241577\n",
            "Step [244/275]:\tLoss: 2.0351972579956055\n",
            "Step [245/275]:\tLoss: 1.458736777305603\n",
            "Step [246/275]:\tLoss: 2.42889404296875\n",
            "Step [247/275]:\tLoss: 1.7232937812805176\n",
            "Step [248/275]:\tLoss: 1.0720423460006714\n",
            "Step [249/275]:\tLoss: 1.3755555152893066\n",
            "Step [250/275]:\tLoss: 1.3656636476516724\n",
            "Step [251/275]:\tLoss: 2.283944606781006\n",
            "Step [252/275]:\tLoss: 2.5676279067993164\n",
            "Step [253/275]:\tLoss: 1.7158819437026978\n",
            "Step [254/275]:\tLoss: 1.8402884006500244\n",
            "Step [255/275]:\tLoss: 2.5025718212127686\n",
            "Step [256/275]:\tLoss: 2.8398749828338623\n",
            "Step [257/275]:\tLoss: 1.8987476825714111\n",
            "Step [258/275]:\tLoss: 1.0434144735336304\n",
            "Step [259/275]:\tLoss: 2.009615182876587\n",
            "Step [260/275]:\tLoss: 1.5975925922393799\n",
            "Step [261/275]:\tLoss: 2.2113828659057617\n",
            "Step [262/275]:\tLoss: 2.140150547027588\n",
            "Step [263/275]:\tLoss: 1.7999731302261353\n",
            "Step [264/275]:\tLoss: 2.16607666015625\n",
            "Step [265/275]:\tLoss: 1.8599343299865723\n",
            "Step [266/275]:\tLoss: 1.3742609024047852\n",
            "Step [267/275]:\tLoss: 1.9753098487854004\n",
            "Step [268/275]:\tLoss: 1.8657987117767334\n",
            "Step [269/275]:\tLoss: 1.4510152339935303\n",
            "Step [270/275]:\tLoss: 1.150699257850647\n",
            "Step [271/275]:\tLoss: 1.2443667650222778\n",
            "Step [272/275]:\tLoss: 1.604759693145752\n",
            "Step [273/275]:\tLoss: 1.7045807838439941\n",
            "Step [274/275]:\tLoss: 3.2723371982574463\n",
            "epoch number 102\n",
            "Step [0/275]:\tLoss: 0.43334394693374634\n",
            "Step [1/275]:\tLoss: 1.755131721496582\n",
            "Step [2/275]:\tLoss: 1.9532763957977295\n",
            "Step [3/275]:\tLoss: 2.877758741378784\n",
            "Step [4/275]:\tLoss: 1.067021131515503\n",
            "Step [5/275]:\tLoss: 3.3906660079956055\n",
            "Step [6/275]:\tLoss: 2.5416665077209473\n",
            "Step [7/275]:\tLoss: 1.106170654296875\n",
            "Step [8/275]:\tLoss: 2.632269859313965\n",
            "Step [9/275]:\tLoss: 1.5781028270721436\n",
            "Step [10/275]:\tLoss: 1.17876136302948\n",
            "Step [11/275]:\tLoss: 2.428445816040039\n",
            "Step [12/275]:\tLoss: 1.011791706085205\n",
            "Step [13/275]:\tLoss: 2.6049208641052246\n",
            "Step [14/275]:\tLoss: 1.532224178314209\n",
            "Step [15/275]:\tLoss: 1.5956535339355469\n",
            "Step [16/275]:\tLoss: 1.4983305931091309\n",
            "Step [17/275]:\tLoss: 2.643554210662842\n",
            "Step [18/275]:\tLoss: 0.9411401748657227\n",
            "Step [19/275]:\tLoss: 2.037083625793457\n",
            "Step [20/275]:\tLoss: 0.8278966546058655\n",
            "Step [21/275]:\tLoss: 3.467092990875244\n",
            "Step [22/275]:\tLoss: 1.567406415939331\n",
            "Step [23/275]:\tLoss: 1.5624983310699463\n",
            "Step [24/275]:\tLoss: 2.009882926940918\n",
            "Step [25/275]:\tLoss: 1.740141749382019\n",
            "Step [26/275]:\tLoss: 2.4303035736083984\n",
            "Step [27/275]:\tLoss: 0.8239650130271912\n",
            "Step [28/275]:\tLoss: 2.327834129333496\n",
            "Step [29/275]:\tLoss: 0.9791125059127808\n",
            "Step [30/275]:\tLoss: 1.0744225978851318\n",
            "Step [31/275]:\tLoss: 0.705653190612793\n",
            "Step [32/275]:\tLoss: 0.7317180633544922\n",
            "Step [33/275]:\tLoss: 1.8572473526000977\n",
            "Step [34/275]:\tLoss: 0.9359266757965088\n",
            "Step [35/275]:\tLoss: 0.7446103096008301\n",
            "Step [36/275]:\tLoss: 0.42789795994758606\n",
            "Step [37/275]:\tLoss: 1.4858661890029907\n",
            "Step [38/275]:\tLoss: 1.1476110219955444\n",
            "Step [39/275]:\tLoss: 0.9697450399398804\n",
            "Step [40/275]:\tLoss: 0.6779075860977173\n",
            "Step [41/275]:\tLoss: 0.4675874710083008\n",
            "Step [42/275]:\tLoss: 0.8283048868179321\n",
            "Step [43/275]:\tLoss: 0.6645364165306091\n",
            "Step [44/275]:\tLoss: 1.2584179639816284\n",
            "Step [45/275]:\tLoss: 0.6783102750778198\n",
            "Step [46/275]:\tLoss: 0.8490498065948486\n",
            "Step [47/275]:\tLoss: 0.7985800504684448\n",
            "Step [48/275]:\tLoss: 1.3426127433776855\n",
            "Step [49/275]:\tLoss: 0.4301106035709381\n",
            "Step [50/275]:\tLoss: 1.2735998630523682\n",
            "Step [51/275]:\tLoss: 1.4291610717773438\n",
            "Step [52/275]:\tLoss: 0.99211585521698\n",
            "Step [53/275]:\tLoss: 1.1661416292190552\n",
            "Step [54/275]:\tLoss: 1.2142682075500488\n",
            "Step [55/275]:\tLoss: 1.2581095695495605\n",
            "Step [56/275]:\tLoss: 1.073343276977539\n",
            "Step [57/275]:\tLoss: 1.0538161993026733\n",
            "Step [58/275]:\tLoss: 0.9828541278839111\n",
            "Step [59/275]:\tLoss: 1.1657732725143433\n",
            "Step [60/275]:\tLoss: 0.6263442635536194\n",
            "Step [61/275]:\tLoss: 0.8596866130828857\n",
            "Step [62/275]:\tLoss: 1.5878793001174927\n",
            "Step [63/275]:\tLoss: 0.41929924488067627\n",
            "Step [64/275]:\tLoss: 0.48188164830207825\n",
            "Step [65/275]:\tLoss: 0.9226794242858887\n",
            "Step [66/275]:\tLoss: 0.7093616127967834\n",
            "Step [67/275]:\tLoss: 0.9134988784790039\n",
            "Step [68/275]:\tLoss: 0.7895444631576538\n",
            "Step [69/275]:\tLoss: 1.1778193712234497\n",
            "Step [70/275]:\tLoss: 0.9782086610794067\n",
            "Step [71/275]:\tLoss: 1.067367434501648\n",
            "Step [72/275]:\tLoss: 1.208292007446289\n",
            "Step [73/275]:\tLoss: 0.6457152962684631\n",
            "Step [74/275]:\tLoss: 1.4150025844573975\n",
            "Step [75/275]:\tLoss: 0.3764374256134033\n",
            "Step [76/275]:\tLoss: 0.7327954769134521\n",
            "Step [77/275]:\tLoss: 0.8931986093521118\n",
            "Step [78/275]:\tLoss: 1.1233291625976562\n",
            "Step [79/275]:\tLoss: 0.8309681415557861\n",
            "Step [80/275]:\tLoss: 0.3209596872329712\n",
            "Step [81/275]:\tLoss: 0.5055151581764221\n",
            "Step [82/275]:\tLoss: 1.1660566329956055\n",
            "Step [83/275]:\tLoss: 0.6896292567253113\n",
            "Step [84/275]:\tLoss: 0.8626678586006165\n",
            "Step [85/275]:\tLoss: 0.5039634704589844\n",
            "Step [86/275]:\tLoss: 1.480993390083313\n",
            "Step [87/275]:\tLoss: 1.3026468753814697\n",
            "Step [88/275]:\tLoss: 0.319622278213501\n",
            "Step [89/275]:\tLoss: 0.5644992589950562\n",
            "Step [90/275]:\tLoss: 1.434608817100525\n",
            "Step [91/275]:\tLoss: 1.3677175045013428\n",
            "Step [92/275]:\tLoss: 0.41991305351257324\n",
            "Step [93/275]:\tLoss: 0.6406168937683105\n",
            "Step [94/275]:\tLoss: 0.8308539390563965\n",
            "Step [95/275]:\tLoss: 0.49054598808288574\n",
            "Step [96/275]:\tLoss: 1.4029638767242432\n",
            "Step [97/275]:\tLoss: 0.44792377948760986\n",
            "Step [98/275]:\tLoss: 0.8232420682907104\n",
            "Step [99/275]:\tLoss: 1.2599964141845703\n",
            "Step [100/275]:\tLoss: 0.5904303193092346\n",
            "Step [101/275]:\tLoss: 0.721454381942749\n",
            "Step [102/275]:\tLoss: 0.5516909956932068\n",
            "Step [103/275]:\tLoss: 0.5369428396224976\n",
            "Step [104/275]:\tLoss: 1.0647962093353271\n",
            "Step [105/275]:\tLoss: 0.8145394921302795\n",
            "Step [106/275]:\tLoss: 1.6147749423980713\n",
            "Step [107/275]:\tLoss: 0.9701659679412842\n",
            "Step [108/275]:\tLoss: 0.7338480949401855\n",
            "Step [109/275]:\tLoss: 1.065308690071106\n",
            "Step [110/275]:\tLoss: 2.3096320629119873\n",
            "Step [111/275]:\tLoss: 2.5255305767059326\n",
            "Step [112/275]:\tLoss: 2.3387527465820312\n",
            "Step [113/275]:\tLoss: 0.8827207088470459\n",
            "Step [114/275]:\tLoss: 2.9013867378234863\n",
            "Step [115/275]:\tLoss: 2.0156407356262207\n",
            "Step [116/275]:\tLoss: 2.2673287391662598\n",
            "Step [117/275]:\tLoss: 2.7183191776275635\n",
            "Step [118/275]:\tLoss: 1.8398964405059814\n",
            "Step [119/275]:\tLoss: 1.8913642168045044\n",
            "Step [120/275]:\tLoss: 2.22725772857666\n",
            "Step [121/275]:\tLoss: 0.9494652152061462\n",
            "Step [122/275]:\tLoss: 2.2505507469177246\n",
            "Step [123/275]:\tLoss: 1.7954137325286865\n",
            "Step [124/275]:\tLoss: 1.3883894681930542\n",
            "Step [125/275]:\tLoss: 2.0926413536071777\n",
            "Step [126/275]:\tLoss: 1.696528673171997\n",
            "Step [127/275]:\tLoss: 1.279051423072815\n",
            "Step [128/275]:\tLoss: 2.710244655609131\n",
            "Step [129/275]:\tLoss: 2.2965269088745117\n",
            "Step [130/275]:\tLoss: 1.6970696449279785\n",
            "Step [131/275]:\tLoss: 1.7329847812652588\n",
            "Step [132/275]:\tLoss: 1.898917317390442\n",
            "Step [133/275]:\tLoss: 1.4604766368865967\n",
            "Step [134/275]:\tLoss: 1.3886032104492188\n",
            "Step [135/275]:\tLoss: 1.6009867191314697\n",
            "Step [136/275]:\tLoss: 1.5406708717346191\n",
            "Step [137/275]:\tLoss: 1.4725000858306885\n",
            "Step [138/275]:\tLoss: 1.10640287399292\n",
            "Step [139/275]:\tLoss: 0.6693446636199951\n",
            "Step [140/275]:\tLoss: 1.2073516845703125\n",
            "Step [141/275]:\tLoss: 0.7479000091552734\n",
            "Step [142/275]:\tLoss: 1.6361476182937622\n",
            "Step [143/275]:\tLoss: 1.072744607925415\n",
            "Step [144/275]:\tLoss: 1.2070529460906982\n",
            "Step [145/275]:\tLoss: 0.738774299621582\n",
            "Step [146/275]:\tLoss: 1.5040429830551147\n",
            "Step [147/275]:\tLoss: 1.197662353515625\n",
            "Step [148/275]:\tLoss: 0.7043471336364746\n",
            "Step [149/275]:\tLoss: 0.7216606140136719\n",
            "Step [150/275]:\tLoss: 1.5895295143127441\n",
            "Step [151/275]:\tLoss: 1.0073466300964355\n",
            "Step [152/275]:\tLoss: 1.042083978652954\n",
            "Step [153/275]:\tLoss: 0.986264705657959\n",
            "Step [154/275]:\tLoss: 1.0218868255615234\n",
            "Step [155/275]:\tLoss: 1.3959791660308838\n",
            "Step [156/275]:\tLoss: 0.5546262264251709\n",
            "Step [157/275]:\tLoss: 0.7228063344955444\n",
            "Step [158/275]:\tLoss: 0.9921296834945679\n",
            "Step [159/275]:\tLoss: 1.1240944862365723\n",
            "Step [160/275]:\tLoss: 0.3430171012878418\n",
            "Step [161/275]:\tLoss: 1.222585678100586\n",
            "Step [162/275]:\tLoss: 0.633237361907959\n",
            "Step [163/275]:\tLoss: 0.776587188243866\n",
            "Step [164/275]:\tLoss: 0.7397891879081726\n",
            "Step [165/275]:\tLoss: 1.573228120803833\n",
            "Step [166/275]:\tLoss: 1.21268892288208\n",
            "Step [167/275]:\tLoss: 1.94813871383667\n",
            "Step [168/275]:\tLoss: 1.329458475112915\n",
            "Step [169/275]:\tLoss: 0.6800981163978577\n",
            "Step [170/275]:\tLoss: 1.0704498291015625\n",
            "Step [171/275]:\tLoss: 0.7744805216789246\n",
            "Step [172/275]:\tLoss: 1.4501073360443115\n",
            "Step [173/275]:\tLoss: 0.9533591270446777\n",
            "Step [174/275]:\tLoss: 0.9039053916931152\n",
            "Step [175/275]:\tLoss: 2.6759486198425293\n",
            "Step [176/275]:\tLoss: 0.9668771028518677\n",
            "Step [177/275]:\tLoss: 1.198685646057129\n",
            "Step [178/275]:\tLoss: 1.164699673652649\n",
            "Step [179/275]:\tLoss: 0.9389229416847229\n",
            "Step [180/275]:\tLoss: 0.48732811212539673\n",
            "Step [181/275]:\tLoss: 1.2604024410247803\n",
            "Step [182/275]:\tLoss: 1.282616138458252\n",
            "Step [183/275]:\tLoss: 1.3273041248321533\n",
            "Step [184/275]:\tLoss: 0.7377504110336304\n",
            "Step [185/275]:\tLoss: 0.5244317650794983\n",
            "Step [186/275]:\tLoss: 1.1226673126220703\n",
            "Step [187/275]:\tLoss: 1.2953044176101685\n",
            "Step [188/275]:\tLoss: 0.9647794961929321\n",
            "Step [189/275]:\tLoss: 0.8836690187454224\n",
            "Step [190/275]:\tLoss: 0.8379393815994263\n",
            "Step [191/275]:\tLoss: 1.5328476428985596\n",
            "Step [192/275]:\tLoss: 0.9447857737541199\n",
            "Step [193/275]:\tLoss: 1.9507981538772583\n",
            "Step [194/275]:\tLoss: 2.6790146827697754\n",
            "Step [195/275]:\tLoss: 2.0856778621673584\n",
            "Step [196/275]:\tLoss: 1.4459495544433594\n",
            "Step [197/275]:\tLoss: 1.2949044704437256\n",
            "Step [198/275]:\tLoss: 1.6521965265274048\n",
            "Step [199/275]:\tLoss: 1.8273887634277344\n",
            "Step [200/275]:\tLoss: 3.168544054031372\n",
            "Step [201/275]:\tLoss: 2.1254992485046387\n",
            "Step [202/275]:\tLoss: 2.3915343284606934\n",
            "Step [203/275]:\tLoss: 2.123037576675415\n",
            "Step [204/275]:\tLoss: 1.4476344585418701\n",
            "Step [205/275]:\tLoss: 1.7901984453201294\n",
            "Step [206/275]:\tLoss: 1.7369589805603027\n",
            "Step [207/275]:\tLoss: 1.3342909812927246\n",
            "Step [208/275]:\tLoss: 2.1382510662078857\n",
            "Step [209/275]:\tLoss: 1.5553526878356934\n",
            "Step [210/275]:\tLoss: 0.9399545192718506\n",
            "Step [211/275]:\tLoss: 1.1723092794418335\n",
            "Step [212/275]:\tLoss: 1.965410590171814\n",
            "Step [213/275]:\tLoss: 2.073744297027588\n",
            "Step [214/275]:\tLoss: 1.9009085893630981\n",
            "Step [215/275]:\tLoss: 1.5618956089019775\n",
            "Step [216/275]:\tLoss: 1.468483805656433\n",
            "Step [217/275]:\tLoss: 1.4946634769439697\n",
            "Step [218/275]:\tLoss: 1.3109279870986938\n",
            "Step [219/275]:\tLoss: 2.2694919109344482\n",
            "Step [220/275]:\tLoss: 1.630637764930725\n",
            "Step [221/275]:\tLoss: 2.0759928226470947\n",
            "Step [222/275]:\tLoss: 2.155993938446045\n",
            "Step [223/275]:\tLoss: 1.6316163539886475\n",
            "Step [224/275]:\tLoss: 1.3149752616882324\n",
            "Step [225/275]:\tLoss: 1.1127358675003052\n",
            "Step [226/275]:\tLoss: 1.7150745391845703\n",
            "Step [227/275]:\tLoss: 1.3589253425598145\n",
            "Step [228/275]:\tLoss: 1.4654388427734375\n",
            "Step [229/275]:\tLoss: 1.135571002960205\n",
            "Step [230/275]:\tLoss: 1.510617733001709\n",
            "Step [231/275]:\tLoss: 1.636163353919983\n",
            "Step [232/275]:\tLoss: 1.2487742900848389\n",
            "Step [233/275]:\tLoss: 1.581357479095459\n",
            "Step [234/275]:\tLoss: 1.1427849531173706\n",
            "Step [235/275]:\tLoss: 1.7033960819244385\n",
            "Step [236/275]:\tLoss: 1.542432427406311\n",
            "Step [237/275]:\tLoss: 1.5757824182510376\n",
            "Step [238/275]:\tLoss: 1.2673096656799316\n",
            "Step [239/275]:\tLoss: 1.7531201839447021\n",
            "Step [240/275]:\tLoss: 1.6399916410446167\n",
            "Step [241/275]:\tLoss: 2.030210494995117\n",
            "Step [242/275]:\tLoss: 1.8196743726730347\n",
            "Step [243/275]:\tLoss: 1.298858404159546\n",
            "Step [244/275]:\tLoss: 1.377993106842041\n",
            "Step [245/275]:\tLoss: 1.5991449356079102\n",
            "Step [246/275]:\tLoss: 0.8734487295150757\n",
            "Step [247/275]:\tLoss: 2.258314609527588\n",
            "Step [248/275]:\tLoss: 2.06067156791687\n",
            "Step [249/275]:\tLoss: 1.2503554821014404\n",
            "Step [250/275]:\tLoss: 1.135433554649353\n",
            "Step [251/275]:\tLoss: 3.4325475692749023\n",
            "Step [252/275]:\tLoss: 1.6757920980453491\n",
            "Step [253/275]:\tLoss: 2.3483705520629883\n",
            "Step [254/275]:\tLoss: 2.9044559001922607\n",
            "Step [255/275]:\tLoss: 3.8323540687561035\n",
            "Step [256/275]:\tLoss: 0.8540300130844116\n",
            "Step [257/275]:\tLoss: 1.4787099361419678\n",
            "Step [258/275]:\tLoss: 1.478506326675415\n",
            "Step [259/275]:\tLoss: 1.1035248041152954\n",
            "Step [260/275]:\tLoss: 1.8042006492614746\n",
            "Step [261/275]:\tLoss: 2.6554527282714844\n",
            "Step [262/275]:\tLoss: 1.4520422220230103\n",
            "Step [263/275]:\tLoss: 1.292662501335144\n",
            "Step [264/275]:\tLoss: 1.3321465253829956\n",
            "Step [265/275]:\tLoss: 1.7399667501449585\n",
            "Step [266/275]:\tLoss: 1.756388783454895\n",
            "Step [267/275]:\tLoss: 1.6518921852111816\n",
            "Step [268/275]:\tLoss: 1.1366634368896484\n",
            "Step [269/275]:\tLoss: 2.9953858852386475\n",
            "Step [270/275]:\tLoss: 1.6051909923553467\n",
            "Step [271/275]:\tLoss: 1.8792531490325928\n",
            "Step [272/275]:\tLoss: 2.920234203338623\n",
            "Step [273/275]:\tLoss: 1.1992416381835938\n",
            "Step [274/275]:\tLoss: 1.7026453018188477\n",
            "epoch number 103\n",
            "Step [0/275]:\tLoss: 2.5338573455810547\n",
            "Step [1/275]:\tLoss: 1.7825400829315186\n",
            "Step [2/275]:\tLoss: 1.7620694637298584\n",
            "Step [3/275]:\tLoss: 3.864201068878174\n",
            "Step [4/275]:\tLoss: 2.116006851196289\n",
            "Step [5/275]:\tLoss: 1.1656584739685059\n",
            "Step [6/275]:\tLoss: 1.7433273792266846\n",
            "Step [7/275]:\tLoss: 1.2992504835128784\n",
            "Step [8/275]:\tLoss: 1.4485543966293335\n",
            "Step [9/275]:\tLoss: 1.0147590637207031\n",
            "Step [10/275]:\tLoss: 1.0856177806854248\n",
            "Step [11/275]:\tLoss: 3.2238481044769287\n",
            "Step [12/275]:\tLoss: 2.282235622406006\n",
            "Step [13/275]:\tLoss: 2.242851734161377\n",
            "Step [14/275]:\tLoss: 1.0161375999450684\n",
            "Step [15/275]:\tLoss: 1.4386014938354492\n",
            "Step [16/275]:\tLoss: 2.411862850189209\n",
            "Step [17/275]:\tLoss: 1.0839903354644775\n",
            "Step [18/275]:\tLoss: 0.9979130029678345\n",
            "Step [19/275]:\tLoss: 1.1693305969238281\n",
            "Step [20/275]:\tLoss: 0.48669254779815674\n",
            "Step [21/275]:\tLoss: 3.2991690635681152\n",
            "Step [22/275]:\tLoss: 0.6418390274047852\n",
            "Step [23/275]:\tLoss: 1.6762505769729614\n",
            "Step [24/275]:\tLoss: 1.048459529876709\n",
            "Step [25/275]:\tLoss: 1.7180397510528564\n",
            "Step [26/275]:\tLoss: 1.456727147102356\n",
            "Step [27/275]:\tLoss: 1.0068514347076416\n",
            "Step [28/275]:\tLoss: 1.1839561462402344\n",
            "Step [29/275]:\tLoss: 0.4900878369808197\n",
            "Step [30/275]:\tLoss: 0.6485112905502319\n",
            "Step [31/275]:\tLoss: 2.0443530082702637\n",
            "Step [32/275]:\tLoss: 1.108481526374817\n",
            "Step [33/275]:\tLoss: 1.2297087907791138\n",
            "Step [34/275]:\tLoss: 0.2522238790988922\n",
            "Step [35/275]:\tLoss: 1.153423547744751\n",
            "Step [36/275]:\tLoss: 0.6097574234008789\n",
            "Step [37/275]:\tLoss: 1.011724829673767\n",
            "Step [38/275]:\tLoss: 0.8478211760520935\n",
            "Step [39/275]:\tLoss: 0.26664143800735474\n",
            "Step [40/275]:\tLoss: 1.4898364543914795\n",
            "Step [41/275]:\tLoss: 0.38859185576438904\n",
            "Step [42/275]:\tLoss: 1.1660664081573486\n",
            "Step [43/275]:\tLoss: 1.237969160079956\n",
            "Step [44/275]:\tLoss: 0.8206510543823242\n",
            "Step [45/275]:\tLoss: 1.1156678199768066\n",
            "Step [46/275]:\tLoss: 0.9585825800895691\n",
            "Step [47/275]:\tLoss: 1.350792407989502\n",
            "Step [48/275]:\tLoss: 0.7433722019195557\n",
            "Step [49/275]:\tLoss: 0.49653160572052\n",
            "Step [50/275]:\tLoss: 0.8238718509674072\n",
            "Step [51/275]:\tLoss: 0.7746772766113281\n",
            "Step [52/275]:\tLoss: 1.7914469242095947\n",
            "Step [53/275]:\tLoss: 0.38921332359313965\n",
            "Step [54/275]:\tLoss: 0.6013870239257812\n",
            "Step [55/275]:\tLoss: 0.7515109777450562\n",
            "Step [56/275]:\tLoss: 1.0412057638168335\n",
            "Step [57/275]:\tLoss: 1.3308359384536743\n",
            "Step [58/275]:\tLoss: 0.9821290969848633\n",
            "Step [59/275]:\tLoss: 1.032015323638916\n",
            "Step [60/275]:\tLoss: 1.1497255563735962\n",
            "Step [61/275]:\tLoss: 0.9483141303062439\n",
            "Step [62/275]:\tLoss: 1.2608797550201416\n",
            "Step [63/275]:\tLoss: 0.9955881237983704\n",
            "Step [64/275]:\tLoss: 0.2407274842262268\n",
            "Step [65/275]:\tLoss: 1.1126735210418701\n",
            "Step [66/275]:\tLoss: 0.4915376603603363\n",
            "Step [67/275]:\tLoss: 0.8566014170646667\n",
            "Step [68/275]:\tLoss: 0.760483980178833\n",
            "Step [69/275]:\tLoss: 0.6950179934501648\n",
            "Step [70/275]:\tLoss: 0.9436103105545044\n",
            "Step [71/275]:\tLoss: 0.793418288230896\n",
            "Step [72/275]:\tLoss: 0.7403236031532288\n",
            "Step [73/275]:\tLoss: 0.36036407947540283\n",
            "Step [74/275]:\tLoss: 1.0707285404205322\n",
            "Step [75/275]:\tLoss: 1.102410912513733\n",
            "Step [76/275]:\tLoss: 0.35853320360183716\n",
            "Step [77/275]:\tLoss: 1.5442609786987305\n",
            "Step [78/275]:\tLoss: 0.6864463090896606\n",
            "Step [79/275]:\tLoss: 0.8452023267745972\n",
            "Step [80/275]:\tLoss: 0.8340668678283691\n",
            "Step [81/275]:\tLoss: 0.6625490188598633\n",
            "Step [82/275]:\tLoss: 0.6350556015968323\n",
            "Step [83/275]:\tLoss: 0.6842760443687439\n",
            "Step [84/275]:\tLoss: 0.40349140763282776\n",
            "Step [85/275]:\tLoss: 1.5573256015777588\n",
            "Step [86/275]:\tLoss: 0.5347904562950134\n",
            "Step [87/275]:\tLoss: 1.0849435329437256\n",
            "Step [88/275]:\tLoss: 0.78045654296875\n",
            "Step [89/275]:\tLoss: 0.961955189704895\n",
            "Step [90/275]:\tLoss: 0.8995816707611084\n",
            "Step [91/275]:\tLoss: 0.8963429927825928\n",
            "Step [92/275]:\tLoss: 0.9192597270011902\n",
            "Step [93/275]:\tLoss: 1.1182200908660889\n",
            "Step [94/275]:\tLoss: 0.43214982748031616\n",
            "Step [95/275]:\tLoss: 0.9277743101119995\n",
            "Step [96/275]:\tLoss: 1.128012776374817\n",
            "Step [97/275]:\tLoss: 1.1888105869293213\n",
            "Step [98/275]:\tLoss: 1.1036052703857422\n",
            "Step [99/275]:\tLoss: 0.7374088764190674\n",
            "Step [100/275]:\tLoss: 0.4055972695350647\n",
            "Step [101/275]:\tLoss: 1.2780992984771729\n",
            "Step [102/275]:\tLoss: 1.0606541633605957\n",
            "Step [103/275]:\tLoss: 0.5816450715065002\n",
            "Step [104/275]:\tLoss: 0.9015505909919739\n",
            "Step [105/275]:\tLoss: 0.42403778433799744\n",
            "Step [106/275]:\tLoss: 1.1589672565460205\n",
            "Step [107/275]:\tLoss: 1.0361722707748413\n",
            "Step [108/275]:\tLoss: 0.37710604071617126\n",
            "Step [109/275]:\tLoss: 0.8834712505340576\n",
            "Step [110/275]:\tLoss: 2.668762683868408\n",
            "Step [111/275]:\tLoss: 2.5238399505615234\n",
            "Step [112/275]:\tLoss: 2.757838249206543\n",
            "Step [113/275]:\tLoss: 1.1957532167434692\n",
            "Step [114/275]:\tLoss: 1.8601382970809937\n",
            "Step [115/275]:\tLoss: 2.3082704544067383\n",
            "Step [116/275]:\tLoss: 1.7310950756072998\n",
            "Step [117/275]:\tLoss: 1.6021637916564941\n",
            "Step [118/275]:\tLoss: 1.4163694381713867\n",
            "Step [119/275]:\tLoss: 0.7688727974891663\n",
            "Step [120/275]:\tLoss: 0.9066516160964966\n",
            "Step [121/275]:\tLoss: 0.16293580830097198\n",
            "Step [122/275]:\tLoss: 2.0776214599609375\n",
            "Step [123/275]:\tLoss: 0.9833323955535889\n",
            "Step [124/275]:\tLoss: 1.9788836240768433\n",
            "Step [125/275]:\tLoss: 1.8037452697753906\n",
            "Step [126/275]:\tLoss: 1.7815330028533936\n",
            "Step [127/275]:\tLoss: 0.9382264614105225\n",
            "Step [128/275]:\tLoss: 0.492915540933609\n",
            "Step [129/275]:\tLoss: 1.658825397491455\n",
            "Step [130/275]:\tLoss: 1.7500766515731812\n",
            "Step [131/275]:\tLoss: 1.8622832298278809\n",
            "Step [132/275]:\tLoss: 1.5293958187103271\n",
            "Step [133/275]:\tLoss: 1.0904521942138672\n",
            "Step [134/275]:\tLoss: 1.8035552501678467\n",
            "Step [135/275]:\tLoss: 3.0192673206329346\n",
            "Step [136/275]:\tLoss: 1.5009446144104004\n",
            "Step [137/275]:\tLoss: 1.1414623260498047\n",
            "Step [138/275]:\tLoss: 0.706579327583313\n",
            "Step [139/275]:\tLoss: 1.0370314121246338\n",
            "Step [140/275]:\tLoss: 1.3981053829193115\n",
            "Step [141/275]:\tLoss: 1.2257957458496094\n",
            "Step [142/275]:\tLoss: 0.6963537931442261\n",
            "Step [143/275]:\tLoss: 1.2790374755859375\n",
            "Step [144/275]:\tLoss: 1.0149238109588623\n",
            "Step [145/275]:\tLoss: 0.4843709170818329\n",
            "Step [146/275]:\tLoss: 0.9433624744415283\n",
            "Step [147/275]:\tLoss: 0.42759257555007935\n",
            "Step [148/275]:\tLoss: 0.7035892605781555\n",
            "Step [149/275]:\tLoss: 0.9885554313659668\n",
            "Step [150/275]:\tLoss: 1.417599081993103\n",
            "Step [151/275]:\tLoss: 0.458186537027359\n",
            "Step [152/275]:\tLoss: 0.6366926431655884\n",
            "Step [153/275]:\tLoss: 0.7066388130187988\n",
            "Step [154/275]:\tLoss: 1.215625524520874\n",
            "Step [155/275]:\tLoss: 1.1952252388000488\n",
            "Step [156/275]:\tLoss: 1.099016785621643\n",
            "Step [157/275]:\tLoss: 0.18594050407409668\n",
            "Step [158/275]:\tLoss: 1.1104700565338135\n",
            "Step [159/275]:\tLoss: 1.080507516860962\n",
            "Step [160/275]:\tLoss: 0.38212519884109497\n",
            "Step [161/275]:\tLoss: 0.7785769104957581\n",
            "Step [162/275]:\tLoss: 0.8486710786819458\n",
            "Step [163/275]:\tLoss: 0.30348384380340576\n",
            "Step [164/275]:\tLoss: 1.1285409927368164\n",
            "Step [165/275]:\tLoss: 1.3709453344345093\n",
            "Step [166/275]:\tLoss: 0.9994443655014038\n",
            "Step [167/275]:\tLoss: 0.7524703145027161\n",
            "Step [168/275]:\tLoss: 0.7506961822509766\n",
            "Step [169/275]:\tLoss: 1.204569697380066\n",
            "Step [170/275]:\tLoss: 0.9147181510925293\n",
            "Step [171/275]:\tLoss: 0.8837212920188904\n",
            "Step [172/275]:\tLoss: 1.1880239248275757\n",
            "Step [173/275]:\tLoss: 1.0923480987548828\n",
            "Step [174/275]:\tLoss: 1.0610365867614746\n",
            "Step [175/275]:\tLoss: 1.9246894121170044\n",
            "Step [176/275]:\tLoss: 0.8890795707702637\n",
            "Step [177/275]:\tLoss: 0.63112473487854\n",
            "Step [178/275]:\tLoss: 0.9337223768234253\n",
            "Step [179/275]:\tLoss: 1.0018630027770996\n",
            "Step [180/275]:\tLoss: 0.8739851713180542\n",
            "Step [181/275]:\tLoss: 0.8377928733825684\n",
            "Step [182/275]:\tLoss: 0.7123643159866333\n",
            "Step [183/275]:\tLoss: 0.7456929087638855\n",
            "Step [184/275]:\tLoss: 1.0469592809677124\n",
            "Step [185/275]:\tLoss: 1.1569890975952148\n",
            "Step [186/275]:\tLoss: 0.6078363060951233\n",
            "Step [187/275]:\tLoss: 1.2280755043029785\n",
            "Step [188/275]:\tLoss: 1.1738978624343872\n",
            "Step [189/275]:\tLoss: 1.0550999641418457\n",
            "Step [190/275]:\tLoss: 1.2502508163452148\n",
            "Step [191/275]:\tLoss: 1.1899216175079346\n",
            "Step [192/275]:\tLoss: 0.8976924419403076\n",
            "Step [193/275]:\tLoss: 1.6663073301315308\n",
            "Step [194/275]:\tLoss: 2.216075897216797\n",
            "Step [195/275]:\tLoss: 2.7177894115448\n",
            "Step [196/275]:\tLoss: 2.576773166656494\n",
            "Step [197/275]:\tLoss: 2.6354432106018066\n",
            "Step [198/275]:\tLoss: 1.463212013244629\n",
            "Step [199/275]:\tLoss: 1.946765661239624\n",
            "Step [200/275]:\tLoss: 1.814295768737793\n",
            "Step [201/275]:\tLoss: 1.6515166759490967\n",
            "Step [202/275]:\tLoss: 2.2133054733276367\n",
            "Step [203/275]:\tLoss: 2.691648483276367\n",
            "Step [204/275]:\tLoss: 1.5019484758377075\n",
            "Step [205/275]:\tLoss: 2.036353826522827\n",
            "Step [206/275]:\tLoss: 2.363335132598877\n",
            "Step [207/275]:\tLoss: 1.4215333461761475\n",
            "Step [208/275]:\tLoss: 1.5038013458251953\n",
            "Step [209/275]:\tLoss: 1.669396162033081\n",
            "Step [210/275]:\tLoss: 2.032541275024414\n",
            "Step [211/275]:\tLoss: 1.513275146484375\n",
            "Step [212/275]:\tLoss: 1.8818999528884888\n",
            "Step [213/275]:\tLoss: 1.3812077045440674\n",
            "Step [214/275]:\tLoss: 1.3762481212615967\n",
            "Step [215/275]:\tLoss: 2.08487606048584\n",
            "Step [216/275]:\tLoss: 1.93308687210083\n",
            "Step [217/275]:\tLoss: 1.6230781078338623\n",
            "Step [218/275]:\tLoss: 1.5836668014526367\n",
            "Step [219/275]:\tLoss: 3.254220962524414\n",
            "Step [220/275]:\tLoss: 1.7670007944107056\n",
            "Step [221/275]:\tLoss: 1.7827441692352295\n",
            "Step [222/275]:\tLoss: 1.5940203666687012\n",
            "Step [223/275]:\tLoss: 1.7510547637939453\n",
            "Step [224/275]:\tLoss: 1.6939244270324707\n",
            "Step [225/275]:\tLoss: 2.1684327125549316\n",
            "Step [226/275]:\tLoss: 1.6535027027130127\n",
            "Step [227/275]:\tLoss: 2.6370534896850586\n",
            "Step [228/275]:\tLoss: 1.255887746810913\n",
            "Step [229/275]:\tLoss: 1.1668189764022827\n",
            "Step [230/275]:\tLoss: 1.0855700969696045\n",
            "Step [231/275]:\tLoss: 2.02526593208313\n",
            "Step [232/275]:\tLoss: 1.2866857051849365\n",
            "Step [233/275]:\tLoss: 1.764760971069336\n",
            "Step [234/275]:\tLoss: 1.7616037130355835\n",
            "Step [235/275]:\tLoss: 1.4474098682403564\n",
            "Step [236/275]:\tLoss: 1.270847201347351\n",
            "Step [237/275]:\tLoss: 1.4552137851715088\n",
            "Step [238/275]:\tLoss: 1.5882948637008667\n",
            "Step [239/275]:\tLoss: 1.6678649187088013\n",
            "Step [240/275]:\tLoss: 1.6789743900299072\n",
            "Step [241/275]:\tLoss: 2.2554268836975098\n",
            "Step [242/275]:\tLoss: 1.29083251953125\n",
            "Step [243/275]:\tLoss: 0.8732118606567383\n",
            "Step [244/275]:\tLoss: 1.4706971645355225\n",
            "Step [245/275]:\tLoss: 1.8399872779846191\n",
            "Step [246/275]:\tLoss: 0.8727980852127075\n",
            "Step [247/275]:\tLoss: 1.3965059518814087\n",
            "Step [248/275]:\tLoss: 1.498265027999878\n",
            "Step [249/275]:\tLoss: 1.8412508964538574\n",
            "Step [250/275]:\tLoss: 0.787822425365448\n",
            "Step [251/275]:\tLoss: 1.6958328485488892\n",
            "Step [252/275]:\tLoss: 1.256592869758606\n",
            "Step [253/275]:\tLoss: 2.811768054962158\n",
            "Step [254/275]:\tLoss: 2.265634536743164\n",
            "Step [255/275]:\tLoss: 3.5728511810302734\n",
            "Step [256/275]:\tLoss: 1.08455228805542\n",
            "Step [257/275]:\tLoss: 2.5227513313293457\n",
            "Step [258/275]:\tLoss: 1.639925241470337\n",
            "Step [259/275]:\tLoss: 1.9098832607269287\n",
            "Step [260/275]:\tLoss: 1.7321882247924805\n",
            "Step [261/275]:\tLoss: 1.3922092914581299\n",
            "Step [262/275]:\tLoss: 0.7220129370689392\n",
            "Step [263/275]:\tLoss: 1.938623309135437\n",
            "Step [264/275]:\tLoss: 1.5451358556747437\n",
            "Step [265/275]:\tLoss: 2.0125179290771484\n",
            "Step [266/275]:\tLoss: 1.4061777591705322\n",
            "Step [267/275]:\tLoss: 1.4402979612350464\n",
            "Step [268/275]:\tLoss: 1.8989580869674683\n",
            "Step [269/275]:\tLoss: 1.898055911064148\n",
            "Step [270/275]:\tLoss: 3.234847068786621\n",
            "Step [271/275]:\tLoss: 1.883195400238037\n",
            "Step [272/275]:\tLoss: 2.783764362335205\n",
            "Step [273/275]:\tLoss: 1.492185115814209\n",
            "Step [274/275]:\tLoss: 1.2209488153457642\n",
            "epoch number 104\n",
            "Step [0/275]:\tLoss: 1.0925874710083008\n",
            "Step [1/275]:\tLoss: 2.4730019569396973\n",
            "Step [2/275]:\tLoss: 1.5054361820220947\n",
            "Step [3/275]:\tLoss: 1.780806303024292\n",
            "Step [4/275]:\tLoss: 1.507014274597168\n",
            "Step [5/275]:\tLoss: 2.8569064140319824\n",
            "Step [6/275]:\tLoss: 1.2501752376556396\n",
            "Step [7/275]:\tLoss: 0.9899924993515015\n",
            "Step [8/275]:\tLoss: 1.616309404373169\n",
            "Step [9/275]:\tLoss: 2.227539539337158\n",
            "Step [10/275]:\tLoss: 1.3674672842025757\n",
            "Step [11/275]:\tLoss: 0.8873391151428223\n",
            "Step [12/275]:\tLoss: 1.825648307800293\n",
            "Step [13/275]:\tLoss: 0.7353240251541138\n",
            "Step [14/275]:\tLoss: 1.4237937927246094\n",
            "Step [15/275]:\tLoss: 1.6714048385620117\n",
            "Step [16/275]:\tLoss: 1.2808048725128174\n",
            "Step [17/275]:\tLoss: 1.1149216890335083\n",
            "Step [18/275]:\tLoss: 3.1272683143615723\n",
            "Step [19/275]:\tLoss: 0.9706267714500427\n",
            "Step [20/275]:\tLoss: 0.5960471630096436\n",
            "Step [21/275]:\tLoss: 2.2502260208129883\n",
            "Step [22/275]:\tLoss: 0.604479193687439\n",
            "Step [23/275]:\tLoss: 1.640403151512146\n",
            "Step [24/275]:\tLoss: 1.0365875959396362\n",
            "Step [25/275]:\tLoss: 1.64817476272583\n",
            "Step [26/275]:\tLoss: 1.4433705806732178\n",
            "Step [27/275]:\tLoss: 1.4272809028625488\n",
            "Step [28/275]:\tLoss: 0.6897647976875305\n",
            "Step [29/275]:\tLoss: 0.6033692359924316\n",
            "Step [30/275]:\tLoss: 1.1966770887374878\n",
            "Step [31/275]:\tLoss: 0.8133339881896973\n",
            "Step [32/275]:\tLoss: 1.0362855195999146\n",
            "Step [33/275]:\tLoss: 0.3937341868877411\n",
            "Step [34/275]:\tLoss: 0.4398127794265747\n",
            "Step [35/275]:\tLoss: 0.3440355062484741\n",
            "Step [36/275]:\tLoss: 0.6012304425239563\n",
            "Step [37/275]:\tLoss: 0.7896511554718018\n",
            "Step [38/275]:\tLoss: 0.7945791482925415\n",
            "Step [39/275]:\tLoss: 0.38690850138664246\n",
            "Step [40/275]:\tLoss: 0.8840509653091431\n",
            "Step [41/275]:\tLoss: 0.8298100829124451\n",
            "Step [42/275]:\tLoss: 1.1265065670013428\n",
            "Step [43/275]:\tLoss: 1.3452054262161255\n",
            "Step [44/275]:\tLoss: 0.3427129089832306\n",
            "Step [45/275]:\tLoss: 0.8661482334136963\n",
            "Step [46/275]:\tLoss: 0.8186311721801758\n",
            "Step [47/275]:\tLoss: 0.2512408494949341\n",
            "Step [48/275]:\tLoss: 1.0463941097259521\n",
            "Step [49/275]:\tLoss: 1.1963186264038086\n",
            "Step [50/275]:\tLoss: 0.7356220483779907\n",
            "Step [51/275]:\tLoss: 1.1728742122650146\n",
            "Step [52/275]:\tLoss: 1.409501075744629\n",
            "Step [53/275]:\tLoss: 0.7950667142868042\n",
            "Step [54/275]:\tLoss: 1.2523667812347412\n",
            "Step [55/275]:\tLoss: 1.2107188701629639\n",
            "Step [56/275]:\tLoss: 0.8171536922454834\n",
            "Step [57/275]:\tLoss: 0.13589906692504883\n",
            "Step [58/275]:\tLoss: 0.7703191637992859\n",
            "Step [59/275]:\tLoss: 0.6262468099594116\n",
            "Step [60/275]:\tLoss: 1.0409750938415527\n",
            "Step [61/275]:\tLoss: 0.3387351930141449\n",
            "Step [62/275]:\tLoss: 0.6966664791107178\n",
            "Step [63/275]:\tLoss: 0.8937289714813232\n",
            "Step [64/275]:\tLoss: 0.6740291118621826\n",
            "Step [65/275]:\tLoss: 1.3350056409835815\n",
            "Step [66/275]:\tLoss: 0.949302077293396\n",
            "Step [67/275]:\tLoss: 0.9179495573043823\n",
            "Step [68/275]:\tLoss: 0.36081361770629883\n",
            "Step [69/275]:\tLoss: 1.551311731338501\n",
            "Step [70/275]:\tLoss: 1.3179253339767456\n",
            "Step [71/275]:\tLoss: 0.6150574684143066\n",
            "Step [72/275]:\tLoss: 0.5788218379020691\n",
            "Step [73/275]:\tLoss: 0.7087957859039307\n",
            "Step [74/275]:\tLoss: 0.32944437861442566\n",
            "Step [75/275]:\tLoss: 0.7626259922981262\n",
            "Step [76/275]:\tLoss: 1.1777772903442383\n",
            "Step [77/275]:\tLoss: 0.946675717830658\n",
            "Step [78/275]:\tLoss: 0.4257652461528778\n",
            "Step [79/275]:\tLoss: 0.8780875205993652\n",
            "Step [80/275]:\tLoss: 1.0453526973724365\n",
            "Step [81/275]:\tLoss: 0.9119513034820557\n",
            "Step [82/275]:\tLoss: 0.7723481059074402\n",
            "Step [83/275]:\tLoss: 0.589781641960144\n",
            "Step [84/275]:\tLoss: 0.24780048429965973\n",
            "Step [85/275]:\tLoss: 0.7594586610794067\n",
            "Step [86/275]:\tLoss: 0.15245452523231506\n",
            "Step [87/275]:\tLoss: 0.8866411447525024\n",
            "Step [88/275]:\tLoss: 0.6755021214485168\n",
            "Step [89/275]:\tLoss: 0.8553622961044312\n",
            "Step [90/275]:\tLoss: 0.33742496371269226\n",
            "Step [91/275]:\tLoss: 0.737547755241394\n",
            "Step [92/275]:\tLoss: 1.1482025384902954\n",
            "Step [93/275]:\tLoss: 1.2364065647125244\n",
            "Step [94/275]:\tLoss: 0.6640342473983765\n",
            "Step [95/275]:\tLoss: 0.6768581867218018\n",
            "Step [96/275]:\tLoss: 1.164362907409668\n",
            "Step [97/275]:\tLoss: 0.9409652948379517\n",
            "Step [98/275]:\tLoss: 1.1008120775222778\n",
            "Step [99/275]:\tLoss: 0.9928697943687439\n",
            "Step [100/275]:\tLoss: 0.6503053903579712\n",
            "Step [101/275]:\tLoss: 0.5191934108734131\n",
            "Step [102/275]:\tLoss: 0.5029987692832947\n",
            "Step [103/275]:\tLoss: 1.0008560419082642\n",
            "Step [104/275]:\tLoss: 0.6710864901542664\n",
            "Step [105/275]:\tLoss: 0.34038862586021423\n",
            "Step [106/275]:\tLoss: 0.40158534049987793\n",
            "Step [107/275]:\tLoss: 0.9539952278137207\n",
            "Step [108/275]:\tLoss: 1.7261971235275269\n",
            "Step [109/275]:\tLoss: 0.5313946008682251\n",
            "Step [110/275]:\tLoss: 2.112607717514038\n",
            "Step [111/275]:\tLoss: 1.8770740032196045\n",
            "Step [112/275]:\tLoss: 2.848005771636963\n",
            "Step [113/275]:\tLoss: 1.9985370635986328\n",
            "Step [114/275]:\tLoss: 3.1330180168151855\n",
            "Step [115/275]:\tLoss: 2.9564476013183594\n",
            "Step [116/275]:\tLoss: 2.358109951019287\n",
            "Step [117/275]:\tLoss: 1.8188209533691406\n",
            "Step [118/275]:\tLoss: 1.3702080249786377\n",
            "Step [119/275]:\tLoss: 1.474086046218872\n",
            "Step [120/275]:\tLoss: 1.5806118249893188\n",
            "Step [121/275]:\tLoss: 1.4379684925079346\n",
            "Step [122/275]:\tLoss: 2.3992083072662354\n",
            "Step [123/275]:\tLoss: 0.3658474087715149\n",
            "Step [124/275]:\tLoss: 0.8372817039489746\n",
            "Step [125/275]:\tLoss: 3.1181178092956543\n",
            "Step [126/275]:\tLoss: 2.2712087631225586\n",
            "Step [127/275]:\tLoss: 1.9161956310272217\n",
            "Step [128/275]:\tLoss: 0.890308141708374\n",
            "Step [129/275]:\tLoss: 2.36557674407959\n",
            "Step [130/275]:\tLoss: 1.780991554260254\n",
            "Step [131/275]:\tLoss: 1.8577336072921753\n",
            "Step [132/275]:\tLoss: 2.371535301208496\n",
            "Step [133/275]:\tLoss: 1.3101303577423096\n",
            "Step [134/275]:\tLoss: 2.298616647720337\n",
            "Step [135/275]:\tLoss: 2.0488154888153076\n",
            "Step [136/275]:\tLoss: 1.6393475532531738\n",
            "Step [137/275]:\tLoss: 0.7816159725189209\n",
            "Step [138/275]:\tLoss: 0.4021120071411133\n",
            "Step [139/275]:\tLoss: 0.9537768363952637\n",
            "Step [140/275]:\tLoss: 0.6018953323364258\n",
            "Step [141/275]:\tLoss: 1.0814244747161865\n",
            "Step [142/275]:\tLoss: 0.6943241953849792\n",
            "Step [143/275]:\tLoss: 0.7243589162826538\n",
            "Step [144/275]:\tLoss: 0.8089020848274231\n",
            "Step [145/275]:\tLoss: 0.3375222086906433\n",
            "Step [146/275]:\tLoss: 0.7121767401695251\n",
            "Step [147/275]:\tLoss: 0.8495321273803711\n",
            "Step [148/275]:\tLoss: 0.6774536371231079\n",
            "Step [149/275]:\tLoss: 0.5162644386291504\n",
            "Step [150/275]:\tLoss: 0.9323679804801941\n",
            "Step [151/275]:\tLoss: 1.003111720085144\n",
            "Step [152/275]:\tLoss: 1.1495765447616577\n",
            "Step [153/275]:\tLoss: 0.8229974508285522\n",
            "Step [154/275]:\tLoss: 0.7317910194396973\n",
            "Step [155/275]:\tLoss: 0.9187939763069153\n",
            "Step [156/275]:\tLoss: 0.9159499406814575\n",
            "Step [157/275]:\tLoss: 0.2291889637708664\n",
            "Step [158/275]:\tLoss: 0.9471185803413391\n",
            "Step [159/275]:\tLoss: 0.6741854548454285\n",
            "Step [160/275]:\tLoss: 0.8295828104019165\n",
            "Step [161/275]:\tLoss: 0.9892697930335999\n",
            "Step [162/275]:\tLoss: 0.193892240524292\n",
            "Step [163/275]:\tLoss: 0.7744784355163574\n",
            "Step [164/275]:\tLoss: 0.8985899090766907\n",
            "Step [165/275]:\tLoss: 0.520095944404602\n",
            "Step [166/275]:\tLoss: 1.2163245677947998\n",
            "Step [167/275]:\tLoss: 1.1070952415466309\n",
            "Step [168/275]:\tLoss: 0.984234631061554\n",
            "Step [169/275]:\tLoss: 1.1383111476898193\n",
            "Step [170/275]:\tLoss: 0.69513338804245\n",
            "Step [171/275]:\tLoss: 1.1130445003509521\n",
            "Step [172/275]:\tLoss: 0.5549827814102173\n",
            "Step [173/275]:\tLoss: 0.44763895869255066\n",
            "Step [174/275]:\tLoss: 1.0079931020736694\n",
            "Step [175/275]:\tLoss: 1.0482265949249268\n",
            "Step [176/275]:\tLoss: 0.876449704170227\n",
            "Step [177/275]:\tLoss: 0.9911620616912842\n",
            "Step [178/275]:\tLoss: 0.7436071038246155\n",
            "Step [179/275]:\tLoss: 0.6742883920669556\n",
            "Step [180/275]:\tLoss: 0.35049694776535034\n",
            "Step [181/275]:\tLoss: 0.6806087493896484\n",
            "Step [182/275]:\tLoss: 0.7579161524772644\n",
            "Step [183/275]:\tLoss: 0.35728007555007935\n",
            "Step [184/275]:\tLoss: 0.38070327043533325\n",
            "Step [185/275]:\tLoss: 1.1288223266601562\n",
            "Step [186/275]:\tLoss: 0.7936098575592041\n",
            "Step [187/275]:\tLoss: 1.2673293352127075\n",
            "Step [188/275]:\tLoss: 0.7528384923934937\n",
            "Step [189/275]:\tLoss: 0.7649279832839966\n",
            "Step [190/275]:\tLoss: 1.1362988948822021\n",
            "Step [191/275]:\tLoss: 0.645153284072876\n",
            "Step [192/275]:\tLoss: 1.8727091550827026\n",
            "Step [193/275]:\tLoss: 3.385880470275879\n",
            "Step [194/275]:\tLoss: 2.7370564937591553\n",
            "Step [195/275]:\tLoss: 2.1327929496765137\n",
            "Step [196/275]:\tLoss: 2.330747604370117\n",
            "Step [197/275]:\tLoss: 2.2534561157226562\n",
            "Step [198/275]:\tLoss: 1.8178322315216064\n",
            "Step [199/275]:\tLoss: 2.7030391693115234\n",
            "Step [200/275]:\tLoss: 1.831151008605957\n",
            "Step [201/275]:\tLoss: 1.3875811100006104\n",
            "Step [202/275]:\tLoss: 1.430034875869751\n",
            "Step [203/275]:\tLoss: 1.610682487487793\n",
            "Step [204/275]:\tLoss: 1.6925430297851562\n",
            "Step [205/275]:\tLoss: 1.0738813877105713\n",
            "Step [206/275]:\tLoss: 2.417872428894043\n",
            "Step [207/275]:\tLoss: 1.6402910947799683\n",
            "Step [208/275]:\tLoss: 1.4378410577774048\n",
            "Step [209/275]:\tLoss: 1.8300104141235352\n",
            "Step [210/275]:\tLoss: 1.8129758834838867\n",
            "Step [211/275]:\tLoss: 2.8304295539855957\n",
            "Step [212/275]:\tLoss: 1.211607575416565\n",
            "Step [213/275]:\tLoss: 1.764000415802002\n",
            "Step [214/275]:\tLoss: 1.3865694999694824\n",
            "Step [215/275]:\tLoss: 1.5345187187194824\n",
            "Step [216/275]:\tLoss: 2.0698258876800537\n",
            "Step [217/275]:\tLoss: 1.7762889862060547\n",
            "Step [218/275]:\tLoss: 1.8751959800720215\n",
            "Step [219/275]:\tLoss: 2.725221633911133\n",
            "Step [220/275]:\tLoss: 2.2944560050964355\n",
            "Step [221/275]:\tLoss: 1.569023609161377\n",
            "Step [222/275]:\tLoss: 1.7903809547424316\n",
            "Step [223/275]:\tLoss: 0.8727673888206482\n",
            "Step [224/275]:\tLoss: 2.4397764205932617\n",
            "Step [225/275]:\tLoss: 1.702161192893982\n",
            "Step [226/275]:\tLoss: 1.5433497428894043\n",
            "Step [227/275]:\tLoss: 1.4481309652328491\n",
            "Step [228/275]:\tLoss: 1.9771652221679688\n",
            "Step [229/275]:\tLoss: 1.3187370300292969\n",
            "Step [230/275]:\tLoss: 1.352525234222412\n",
            "Step [231/275]:\tLoss: 2.0085575580596924\n",
            "Step [232/275]:\tLoss: 2.0868301391601562\n",
            "Step [233/275]:\tLoss: 1.3881169557571411\n",
            "Step [234/275]:\tLoss: 2.010274887084961\n",
            "Step [235/275]:\tLoss: 1.7586355209350586\n",
            "Step [236/275]:\tLoss: 1.7992596626281738\n",
            "Step [237/275]:\tLoss: 1.8956923484802246\n",
            "Step [238/275]:\tLoss: 2.374497890472412\n",
            "Step [239/275]:\tLoss: 1.6629221439361572\n",
            "Step [240/275]:\tLoss: 1.0647356510162354\n",
            "Step [241/275]:\tLoss: 1.728156566619873\n",
            "Step [242/275]:\tLoss: 1.0813651084899902\n",
            "Step [243/275]:\tLoss: 1.5140981674194336\n",
            "Step [244/275]:\tLoss: 2.707225799560547\n",
            "Step [245/275]:\tLoss: 1.2170381546020508\n",
            "Step [246/275]:\tLoss: 1.7365946769714355\n",
            "Step [247/275]:\tLoss: 2.4356563091278076\n",
            "Step [248/275]:\tLoss: 1.5845599174499512\n",
            "Step [249/275]:\tLoss: 1.853568434715271\n",
            "Step [250/275]:\tLoss: 2.317413806915283\n",
            "Step [251/275]:\tLoss: 1.2494823932647705\n",
            "Step [252/275]:\tLoss: 3.1629106998443604\n",
            "Step [253/275]:\tLoss: 1.4237282276153564\n",
            "Step [254/275]:\tLoss: 1.1765363216400146\n",
            "Step [255/275]:\tLoss: 2.240018844604492\n",
            "Step [256/275]:\tLoss: 1.1472418308258057\n",
            "Step [257/275]:\tLoss: 1.7807881832122803\n",
            "Step [258/275]:\tLoss: 2.803036689758301\n",
            "Step [259/275]:\tLoss: 2.4469943046569824\n",
            "Step [260/275]:\tLoss: 2.7595396041870117\n",
            "Step [261/275]:\tLoss: 2.1158299446105957\n",
            "Step [262/275]:\tLoss: 3.2346560955047607\n",
            "Step [263/275]:\tLoss: 1.8615295886993408\n",
            "Step [264/275]:\tLoss: 2.3322091102600098\n",
            "Step [265/275]:\tLoss: 2.3216114044189453\n",
            "Step [266/275]:\tLoss: 2.4426941871643066\n",
            "Step [267/275]:\tLoss: 2.4131834506988525\n",
            "Step [268/275]:\tLoss: 0.7751330137252808\n",
            "Step [269/275]:\tLoss: 1.5374536514282227\n",
            "Step [270/275]:\tLoss: 1.8120603561401367\n",
            "Step [271/275]:\tLoss: 1.466588020324707\n",
            "Step [272/275]:\tLoss: 1.9867660999298096\n",
            "Step [273/275]:\tLoss: 1.5590064525604248\n",
            "Step [274/275]:\tLoss: 1.289611577987671\n",
            "epoch number 105\n",
            "Step [0/275]:\tLoss: 1.1450634002685547\n",
            "Step [1/275]:\tLoss: 1.3874746561050415\n",
            "Step [2/275]:\tLoss: 1.8226819038391113\n",
            "Step [3/275]:\tLoss: 1.3323307037353516\n",
            "Step [4/275]:\tLoss: 0.6672159433364868\n",
            "Step [5/275]:\tLoss: 0.7261962890625\n",
            "Step [6/275]:\tLoss: 0.9388443827629089\n",
            "Step [7/275]:\tLoss: 0.506355881690979\n",
            "Step [8/275]:\tLoss: 1.593735694885254\n",
            "Step [9/275]:\tLoss: 1.3985042572021484\n",
            "Step [10/275]:\tLoss: 0.6757372617721558\n",
            "Step [11/275]:\tLoss: 1.0035496950149536\n",
            "Step [12/275]:\tLoss: 0.960803747177124\n",
            "Step [13/275]:\tLoss: 0.9851680397987366\n",
            "Step [14/275]:\tLoss: 1.4035766124725342\n",
            "Step [15/275]:\tLoss: 0.9607810974121094\n",
            "Step [16/275]:\tLoss: 0.5481783747673035\n",
            "Step [17/275]:\tLoss: 0.7706949710845947\n",
            "Step [18/275]:\tLoss: 0.8423916697502136\n",
            "Step [19/275]:\tLoss: 1.2802355289459229\n",
            "Step [20/275]:\tLoss: 0.9427153468132019\n",
            "Step [21/275]:\tLoss: 1.9002983570098877\n",
            "Step [22/275]:\tLoss: 1.0804998874664307\n",
            "Step [23/275]:\tLoss: 1.9110233783721924\n",
            "Step [24/275]:\tLoss: 1.007354497909546\n",
            "Step [25/275]:\tLoss: 0.6406726837158203\n",
            "Step [26/275]:\tLoss: 0.868492603302002\n",
            "Step [27/275]:\tLoss: 0.782651424407959\n",
            "Step [28/275]:\tLoss: 1.0094335079193115\n",
            "Step [29/275]:\tLoss: 0.7220346927642822\n",
            "Step [30/275]:\tLoss: 0.6052881479263306\n",
            "Step [31/275]:\tLoss: 1.043860673904419\n",
            "Step [32/275]:\tLoss: 0.6757521033287048\n",
            "Step [33/275]:\tLoss: 0.8145831823348999\n",
            "Step [34/275]:\tLoss: 0.6753298044204712\n",
            "Step [35/275]:\tLoss: 0.999935507774353\n",
            "Step [36/275]:\tLoss: 0.30238524079322815\n",
            "Step [37/275]:\tLoss: 0.32616516947746277\n",
            "Step [38/275]:\tLoss: 0.8811712861061096\n",
            "Step [39/275]:\tLoss: 1.3247804641723633\n",
            "Step [40/275]:\tLoss: 0.8934083580970764\n",
            "Step [41/275]:\tLoss: 0.4886840581893921\n",
            "Step [42/275]:\tLoss: 0.9211521148681641\n",
            "Step [43/275]:\tLoss: 1.1196503639221191\n",
            "Step [44/275]:\tLoss: 0.6117316484451294\n",
            "Step [45/275]:\tLoss: 0.2011110633611679\n",
            "Step [46/275]:\tLoss: 0.865756630897522\n",
            "Step [47/275]:\tLoss: 0.35213422775268555\n",
            "Step [48/275]:\tLoss: 0.9731648564338684\n",
            "Step [49/275]:\tLoss: 0.5261811017990112\n",
            "Step [50/275]:\tLoss: 0.7020678520202637\n",
            "Step [51/275]:\tLoss: 0.7909964919090271\n",
            "Step [52/275]:\tLoss: 0.415844202041626\n",
            "Step [53/275]:\tLoss: 0.40421897172927856\n",
            "Step [54/275]:\tLoss: 0.8283875584602356\n",
            "Step [55/275]:\tLoss: 0.7133952975273132\n",
            "Step [56/275]:\tLoss: 0.5043063163757324\n",
            "Step [57/275]:\tLoss: 0.759536862373352\n",
            "Step [58/275]:\tLoss: 1.0177350044250488\n",
            "Step [59/275]:\tLoss: 1.0590864419937134\n",
            "Step [60/275]:\tLoss: 0.911967933177948\n",
            "Step [61/275]:\tLoss: 0.6305122971534729\n",
            "Step [62/275]:\tLoss: 0.8015275001525879\n",
            "Step [63/275]:\tLoss: 1.016018271446228\n",
            "Step [64/275]:\tLoss: 1.0956237316131592\n",
            "Step [65/275]:\tLoss: 0.344465047121048\n",
            "Step [66/275]:\tLoss: 0.908547043800354\n",
            "Step [67/275]:\tLoss: 0.3372616171836853\n",
            "Step [68/275]:\tLoss: 0.8944483995437622\n",
            "Step [69/275]:\tLoss: 0.2628159821033478\n",
            "Step [70/275]:\tLoss: 1.1252820491790771\n",
            "Step [71/275]:\tLoss: 0.8512290716171265\n",
            "Step [72/275]:\tLoss: 0.8506267070770264\n",
            "Step [73/275]:\tLoss: 0.35656189918518066\n",
            "Step [74/275]:\tLoss: 0.8205219507217407\n",
            "Step [75/275]:\tLoss: 0.8140155076980591\n",
            "Step [76/275]:\tLoss: 1.0176259279251099\n",
            "Step [77/275]:\tLoss: 0.4386124610900879\n",
            "Step [78/275]:\tLoss: 0.17373204231262207\n",
            "Step [79/275]:\tLoss: 0.8998622298240662\n",
            "Step [80/275]:\tLoss: 0.6073078513145447\n",
            "Step [81/275]:\tLoss: 1.046829342842102\n",
            "Step [82/275]:\tLoss: 0.7436849474906921\n",
            "Step [83/275]:\tLoss: 0.6508747339248657\n",
            "Step [84/275]:\tLoss: 0.654097855091095\n",
            "Step [85/275]:\tLoss: 0.8280991315841675\n",
            "Step [86/275]:\tLoss: 0.23924121260643005\n",
            "Step [87/275]:\tLoss: 0.9940194487571716\n",
            "Step [88/275]:\tLoss: 1.0235860347747803\n",
            "Step [89/275]:\tLoss: 0.7332690954208374\n",
            "Step [90/275]:\tLoss: 0.43432632088661194\n",
            "Step [91/275]:\tLoss: 0.9098314046859741\n",
            "Step [92/275]:\tLoss: 0.5518734455108643\n",
            "Step [93/275]:\tLoss: 1.2319469451904297\n",
            "Step [94/275]:\tLoss: 0.8493317365646362\n",
            "Step [95/275]:\tLoss: 0.6296660900115967\n",
            "Step [96/275]:\tLoss: 0.8879976272583008\n",
            "Step [97/275]:\tLoss: 0.7808800935745239\n",
            "Step [98/275]:\tLoss: 0.6611863374710083\n",
            "Step [99/275]:\tLoss: 1.2604796886444092\n",
            "Step [100/275]:\tLoss: 0.7125252485275269\n",
            "Step [101/275]:\tLoss: 0.9964307546615601\n",
            "Step [102/275]:\tLoss: 0.4400233328342438\n",
            "Step [103/275]:\tLoss: 0.7601020932197571\n",
            "Step [104/275]:\tLoss: 0.38480496406555176\n",
            "Step [105/275]:\tLoss: 0.80063796043396\n",
            "Step [106/275]:\tLoss: 0.9534707069396973\n",
            "Step [107/275]:\tLoss: 0.6012977361679077\n",
            "Step [108/275]:\tLoss: 1.0193992853164673\n",
            "Step [109/275]:\tLoss: 0.3907638192176819\n",
            "Step [110/275]:\tLoss: 1.8803133964538574\n",
            "Step [111/275]:\tLoss: 1.9725791215896606\n",
            "Step [112/275]:\tLoss: 2.2876248359680176\n",
            "Step [113/275]:\tLoss: 1.7891271114349365\n",
            "Step [114/275]:\tLoss: 1.3665199279785156\n",
            "Step [115/275]:\tLoss: 2.127554416656494\n",
            "Step [116/275]:\tLoss: 1.9877070188522339\n",
            "Step [117/275]:\tLoss: 1.3736398220062256\n",
            "Step [118/275]:\tLoss: 2.0249152183532715\n",
            "Step [119/275]:\tLoss: 2.623936176300049\n",
            "Step [120/275]:\tLoss: 0.8298209309577942\n",
            "Step [121/275]:\tLoss: 0.8294851779937744\n",
            "Step [122/275]:\tLoss: 1.4113678932189941\n",
            "Step [123/275]:\tLoss: 1.532111406326294\n",
            "Step [124/275]:\tLoss: 1.6914598941802979\n",
            "Step [125/275]:\tLoss: 1.3144745826721191\n",
            "Step [126/275]:\tLoss: 1.7682946920394897\n",
            "Step [127/275]:\tLoss: 0.4517371356487274\n",
            "Step [128/275]:\tLoss: 1.631492018699646\n",
            "Step [129/275]:\tLoss: 1.1964566707611084\n",
            "Step [130/275]:\tLoss: 1.121842861175537\n",
            "Step [131/275]:\tLoss: 1.633736252784729\n",
            "Step [132/275]:\tLoss: 1.291029930114746\n",
            "Step [133/275]:\tLoss: 2.0455191135406494\n",
            "Step [134/275]:\tLoss: 1.7259060144424438\n",
            "Step [135/275]:\tLoss: 1.9721709489822388\n",
            "Step [136/275]:\tLoss: 1.479714274406433\n",
            "Step [137/275]:\tLoss: 1.765963077545166\n",
            "Step [138/275]:\tLoss: 0.9276324510574341\n",
            "Step [139/275]:\tLoss: 0.7654226422309875\n",
            "Step [140/275]:\tLoss: 0.9456678628921509\n",
            "Step [141/275]:\tLoss: 0.9237825870513916\n",
            "Step [142/275]:\tLoss: 0.5959696769714355\n",
            "Step [143/275]:\tLoss: 0.961290717124939\n",
            "Step [144/275]:\tLoss: 0.8527918457984924\n",
            "Step [145/275]:\tLoss: 0.9525368213653564\n",
            "Step [146/275]:\tLoss: 0.7776497602462769\n",
            "Step [147/275]:\tLoss: 1.1295490264892578\n",
            "Step [148/275]:\tLoss: 1.0041403770446777\n",
            "Step [149/275]:\tLoss: 0.9621747732162476\n",
            "Step [150/275]:\tLoss: 0.8657488822937012\n",
            "Step [151/275]:\tLoss: 0.8434365391731262\n",
            "Step [152/275]:\tLoss: 0.7256870269775391\n",
            "Step [153/275]:\tLoss: 0.8767476081848145\n",
            "Step [154/275]:\tLoss: 1.2141607999801636\n",
            "Step [155/275]:\tLoss: 1.099618911743164\n",
            "Step [156/275]:\tLoss: 0.5103632211685181\n",
            "Step [157/275]:\tLoss: 0.6032019853591919\n",
            "Step [158/275]:\tLoss: 0.7459713816642761\n",
            "Step [159/275]:\tLoss: 0.9996545314788818\n",
            "Step [160/275]:\tLoss: 1.4462769031524658\n",
            "Step [161/275]:\tLoss: 0.4318847060203552\n",
            "Step [162/275]:\tLoss: 0.9737058877944946\n",
            "Step [163/275]:\tLoss: 0.8602386116981506\n",
            "Step [164/275]:\tLoss: 0.7574025988578796\n",
            "Step [165/275]:\tLoss: 0.37438130378723145\n",
            "Step [166/275]:\tLoss: 1.0751017332077026\n",
            "Step [167/275]:\tLoss: 0.7314171195030212\n",
            "Step [168/275]:\tLoss: 0.9159097075462341\n",
            "Step [169/275]:\tLoss: 0.9411149024963379\n",
            "Step [170/275]:\tLoss: 0.6677998304367065\n",
            "Step [171/275]:\tLoss: 1.428208351135254\n",
            "Step [172/275]:\tLoss: 1.0859270095825195\n",
            "Step [173/275]:\tLoss: 1.100001335144043\n",
            "Step [174/275]:\tLoss: 0.9290125370025635\n",
            "Step [175/275]:\tLoss: 1.3332501649856567\n",
            "Step [176/275]:\tLoss: 0.8286466598510742\n",
            "Step [177/275]:\tLoss: 0.6197829842567444\n",
            "Step [178/275]:\tLoss: 1.0211554765701294\n",
            "Step [179/275]:\tLoss: 0.7601209878921509\n",
            "Step [180/275]:\tLoss: 0.9533005952835083\n",
            "Step [181/275]:\tLoss: 0.931877851486206\n",
            "Step [182/275]:\tLoss: 0.8894273042678833\n",
            "Step [183/275]:\tLoss: 0.4667155146598816\n",
            "Step [184/275]:\tLoss: 0.8101626634597778\n",
            "Step [185/275]:\tLoss: 0.5666565895080566\n",
            "Step [186/275]:\tLoss: 0.3964388072490692\n",
            "Step [187/275]:\tLoss: 1.1874494552612305\n",
            "Step [188/275]:\tLoss: 0.6535716652870178\n",
            "Step [189/275]:\tLoss: 0.9843202233314514\n",
            "Step [190/275]:\tLoss: 0.9017540216445923\n",
            "Step [191/275]:\tLoss: 0.9914254546165466\n",
            "Step [192/275]:\tLoss: 1.9364416599273682\n",
            "Step [193/275]:\tLoss: 1.9926252365112305\n",
            "Step [194/275]:\tLoss: 1.7079782485961914\n",
            "Step [195/275]:\tLoss: 1.8970344066619873\n",
            "Step [196/275]:\tLoss: 2.34269642829895\n",
            "Step [197/275]:\tLoss: 2.5084304809570312\n",
            "Step [198/275]:\tLoss: 1.6775587797164917\n",
            "Step [199/275]:\tLoss: 1.5379912853240967\n",
            "Step [200/275]:\tLoss: 2.1465673446655273\n",
            "Step [201/275]:\tLoss: 1.48655104637146\n",
            "Step [202/275]:\tLoss: 1.3754500150680542\n",
            "Step [203/275]:\tLoss: 1.8153375387191772\n",
            "Step [204/275]:\tLoss: 1.8516331911087036\n",
            "Step [205/275]:\tLoss: 1.3646345138549805\n",
            "Step [206/275]:\tLoss: 2.0498299598693848\n",
            "Step [207/275]:\tLoss: 1.450770378112793\n",
            "Step [208/275]:\tLoss: 1.3000261783599854\n",
            "Step [209/275]:\tLoss: 2.058619976043701\n",
            "Step [210/275]:\tLoss: 1.3792028427124023\n",
            "Step [211/275]:\tLoss: 2.0780632495880127\n",
            "Step [212/275]:\tLoss: 1.7207810878753662\n",
            "Step [213/275]:\tLoss: 1.6434739828109741\n",
            "Step [214/275]:\tLoss: 2.0614187717437744\n",
            "Step [215/275]:\tLoss: 1.6115825176239014\n",
            "Step [216/275]:\tLoss: 1.3379420042037964\n",
            "Step [217/275]:\tLoss: 2.1502208709716797\n",
            "Step [218/275]:\tLoss: 1.7739765644073486\n",
            "Step [219/275]:\tLoss: 1.74526846408844\n",
            "Step [220/275]:\tLoss: 2.0944414138793945\n",
            "Step [221/275]:\tLoss: 1.523718237876892\n",
            "Step [222/275]:\tLoss: 1.143380045890808\n",
            "Step [223/275]:\tLoss: 1.3715534210205078\n",
            "Step [224/275]:\tLoss: 2.8083834648132324\n",
            "Step [225/275]:\tLoss: 1.4864990711212158\n",
            "Step [226/275]:\tLoss: 1.586575984954834\n",
            "Step [227/275]:\tLoss: 1.452193021774292\n",
            "Step [228/275]:\tLoss: 1.7393946647644043\n",
            "Step [229/275]:\tLoss: 2.742194652557373\n",
            "Step [230/275]:\tLoss: 1.3261841535568237\n",
            "Step [231/275]:\tLoss: 2.045330047607422\n",
            "Step [232/275]:\tLoss: 1.1968828439712524\n",
            "Step [233/275]:\tLoss: 1.5105434656143188\n",
            "Step [234/275]:\tLoss: 2.27097225189209\n",
            "Step [235/275]:\tLoss: 2.0318102836608887\n",
            "Step [236/275]:\tLoss: 1.2392182350158691\n",
            "Step [237/275]:\tLoss: 2.0348641872406006\n",
            "Step [238/275]:\tLoss: 1.271284580230713\n",
            "Step [239/275]:\tLoss: 1.6504795551300049\n",
            "Step [240/275]:\tLoss: 1.19437575340271\n",
            "Step [241/275]:\tLoss: 1.866818904876709\n",
            "Step [242/275]:\tLoss: 1.3208061456680298\n",
            "Step [243/275]:\tLoss: 1.1785955429077148\n",
            "Step [244/275]:\tLoss: 1.485846757888794\n",
            "Step [245/275]:\tLoss: 2.0203795433044434\n",
            "Step [246/275]:\tLoss: 1.8144643306732178\n",
            "Step [247/275]:\tLoss: 2.0624732971191406\n",
            "Step [248/275]:\tLoss: 1.811270833015442\n",
            "Step [249/275]:\tLoss: 1.8990434408187866\n",
            "Step [250/275]:\tLoss: 1.4968304634094238\n",
            "Step [251/275]:\tLoss: 1.901026725769043\n",
            "Step [252/275]:\tLoss: 1.352717399597168\n",
            "Step [253/275]:\tLoss: 2.38063907623291\n",
            "Step [254/275]:\tLoss: 2.021198272705078\n",
            "Step [255/275]:\tLoss: 2.1097826957702637\n",
            "Step [256/275]:\tLoss: 2.080681800842285\n",
            "Step [257/275]:\tLoss: 2.2285959720611572\n",
            "Step [258/275]:\tLoss: 2.0504581928253174\n",
            "Step [259/275]:\tLoss: 1.1052680015563965\n",
            "Step [260/275]:\tLoss: 1.3098793029785156\n",
            "Step [261/275]:\tLoss: 1.872049331665039\n",
            "Step [262/275]:\tLoss: 2.3819985389709473\n",
            "Step [263/275]:\tLoss: 1.1094223260879517\n",
            "Step [264/275]:\tLoss: 1.2934423685073853\n",
            "Step [265/275]:\tLoss: 1.4672679901123047\n",
            "Step [266/275]:\tLoss: 2.486581325531006\n",
            "Step [267/275]:\tLoss: 2.265012502670288\n",
            "Step [268/275]:\tLoss: 0.9845083951950073\n",
            "Step [269/275]:\tLoss: 1.094782829284668\n",
            "Step [270/275]:\tLoss: 0.9827678203582764\n",
            "Step [271/275]:\tLoss: 1.4980087280273438\n",
            "Step [272/275]:\tLoss: 2.074591875076294\n",
            "Step [273/275]:\tLoss: 1.4404058456420898\n",
            "Step [274/275]:\tLoss: 1.0094765424728394\n",
            "epoch number 106\n",
            "Step [0/275]:\tLoss: 1.717124581336975\n",
            "Step [1/275]:\tLoss: 2.572422981262207\n",
            "Step [2/275]:\tLoss: 2.029797077178955\n",
            "Step [3/275]:\tLoss: 1.8111259937286377\n",
            "Step [4/275]:\tLoss: 1.5538206100463867\n",
            "Step [5/275]:\tLoss: 2.4852194786071777\n",
            "Step [6/275]:\tLoss: 1.0232875347137451\n",
            "Step [7/275]:\tLoss: 0.9744347929954529\n",
            "Step [8/275]:\tLoss: 1.818455696105957\n",
            "Step [9/275]:\tLoss: 2.016019344329834\n",
            "Step [10/275]:\tLoss: 1.2791049480438232\n",
            "Step [11/275]:\tLoss: 1.96855628490448\n",
            "Step [12/275]:\tLoss: 0.928592324256897\n",
            "Step [13/275]:\tLoss: 0.7312774658203125\n",
            "Step [14/275]:\tLoss: 0.6366475224494934\n",
            "Step [15/275]:\tLoss: 0.9835144281387329\n",
            "Step [16/275]:\tLoss: 1.874111533164978\n",
            "Step [17/275]:\tLoss: 2.0959250926971436\n",
            "Step [18/275]:\tLoss: 1.3595163822174072\n",
            "Step [19/275]:\tLoss: 2.2328877449035645\n",
            "Step [20/275]:\tLoss: 1.1485908031463623\n",
            "Step [21/275]:\tLoss: 1.1519944667816162\n",
            "Step [22/275]:\tLoss: 2.1173007488250732\n",
            "Step [23/275]:\tLoss: 1.6637237071990967\n",
            "Step [24/275]:\tLoss: 1.3538503646850586\n",
            "Step [25/275]:\tLoss: 0.8015224933624268\n",
            "Step [26/275]:\tLoss: 0.9958720803260803\n",
            "Step [27/275]:\tLoss: 0.7195072770118713\n",
            "Step [28/275]:\tLoss: 1.4045991897583008\n",
            "Step [29/275]:\tLoss: 0.5946874618530273\n",
            "Step [30/275]:\tLoss: 0.9919995069503784\n",
            "Step [31/275]:\tLoss: 0.7055561542510986\n",
            "Step [32/275]:\tLoss: 1.0945158004760742\n",
            "Step [33/275]:\tLoss: 0.5758970379829407\n",
            "Step [34/275]:\tLoss: 0.920792818069458\n",
            "Step [35/275]:\tLoss: 0.34069159626960754\n",
            "Step [36/275]:\tLoss: 0.9830897450447083\n",
            "Step [37/275]:\tLoss: 1.1040759086608887\n",
            "Step [38/275]:\tLoss: 0.9001772403717041\n",
            "Step [39/275]:\tLoss: 0.8999674916267395\n",
            "Step [40/275]:\tLoss: 0.47528076171875\n",
            "Step [41/275]:\tLoss: 0.3848564028739929\n",
            "Step [42/275]:\tLoss: 0.8820884227752686\n",
            "Step [43/275]:\tLoss: 0.7596193552017212\n",
            "Step [44/275]:\tLoss: 0.9769045114517212\n",
            "Step [45/275]:\tLoss: 0.6631944179534912\n",
            "Step [46/275]:\tLoss: 0.964323878288269\n",
            "Step [47/275]:\tLoss: 0.9770772457122803\n",
            "Step [48/275]:\tLoss: 0.9069594144821167\n",
            "Step [49/275]:\tLoss: 0.3741978108882904\n",
            "Step [50/275]:\tLoss: 0.6591947674751282\n",
            "Step [51/275]:\tLoss: 1.1338531970977783\n",
            "Step [52/275]:\tLoss: 0.45254772901535034\n",
            "Step [53/275]:\tLoss: 1.107414722442627\n",
            "Step [54/275]:\tLoss: 1.0530683994293213\n",
            "Step [55/275]:\tLoss: 0.8771535754203796\n",
            "Step [56/275]:\tLoss: 0.7369691133499146\n",
            "Step [57/275]:\tLoss: 1.0735234022140503\n",
            "Step [58/275]:\tLoss: 0.8921178579330444\n",
            "Step [59/275]:\tLoss: 0.6272984743118286\n",
            "Step [60/275]:\tLoss: 1.0464950799942017\n",
            "Step [61/275]:\tLoss: 0.44155603647232056\n",
            "Step [62/275]:\tLoss: 1.5684465169906616\n",
            "Step [63/275]:\tLoss: 0.7562286853790283\n",
            "Step [64/275]:\tLoss: 1.5101906061172485\n",
            "Step [65/275]:\tLoss: 0.19868528842926025\n",
            "Step [66/275]:\tLoss: 0.8475291728973389\n",
            "Step [67/275]:\tLoss: 0.3726317286491394\n",
            "Step [68/275]:\tLoss: 0.8289632201194763\n",
            "Step [69/275]:\tLoss: 0.8624365925788879\n",
            "Step [70/275]:\tLoss: 0.8094741702079773\n",
            "Step [71/275]:\tLoss: 1.990530252456665\n",
            "Step [72/275]:\tLoss: 0.7887734174728394\n",
            "Step [73/275]:\tLoss: 0.23897694051265717\n",
            "Step [74/275]:\tLoss: 1.2458057403564453\n",
            "Step [75/275]:\tLoss: 0.9688682556152344\n",
            "Step [76/275]:\tLoss: 0.7231506109237671\n",
            "Step [77/275]:\tLoss: 0.7895737886428833\n",
            "Step [78/275]:\tLoss: 0.5871131420135498\n",
            "Step [79/275]:\tLoss: 0.5648306012153625\n",
            "Step [80/275]:\tLoss: 0.6247276067733765\n",
            "Step [81/275]:\tLoss: 0.8229825496673584\n",
            "Step [82/275]:\tLoss: 0.7369469404220581\n",
            "Step [83/275]:\tLoss: 0.5583856105804443\n",
            "Step [84/275]:\tLoss: 0.2887960374355316\n",
            "Step [85/275]:\tLoss: 0.6174638867378235\n",
            "Step [86/275]:\tLoss: 0.6559510827064514\n",
            "Step [87/275]:\tLoss: 0.38692137598991394\n",
            "Step [88/275]:\tLoss: 0.346230149269104\n",
            "Step [89/275]:\tLoss: 0.7408111095428467\n",
            "Step [90/275]:\tLoss: 0.19102883338928223\n",
            "Step [91/275]:\tLoss: 0.7434180974960327\n",
            "Step [92/275]:\tLoss: 0.6043784618377686\n",
            "Step [93/275]:\tLoss: 0.6673630475997925\n",
            "Step [94/275]:\tLoss: 1.0338315963745117\n",
            "Step [95/275]:\tLoss: 1.1512179374694824\n",
            "Step [96/275]:\tLoss: 0.5913734436035156\n",
            "Step [97/275]:\tLoss: 1.0373185873031616\n",
            "Step [98/275]:\tLoss: 0.7318023443222046\n",
            "Step [99/275]:\tLoss: 0.6130191087722778\n",
            "Step [100/275]:\tLoss: 1.2552003860473633\n",
            "Step [101/275]:\tLoss: 0.8823961019515991\n",
            "Step [102/275]:\tLoss: 0.6014693975448608\n",
            "Step [103/275]:\tLoss: 0.9221644997596741\n",
            "Step [104/275]:\tLoss: 0.8501561284065247\n",
            "Step [105/275]:\tLoss: 0.7065178751945496\n",
            "Step [106/275]:\tLoss: 0.6864160895347595\n",
            "Step [107/275]:\tLoss: 0.5251619219779968\n",
            "Step [108/275]:\tLoss: 0.9393751621246338\n",
            "Step [109/275]:\tLoss: 0.7562159299850464\n",
            "Step [110/275]:\tLoss: 2.096575975418091\n",
            "Step [111/275]:\tLoss: 2.82759952545166\n",
            "Step [112/275]:\tLoss: 2.4919261932373047\n",
            "Step [113/275]:\tLoss: 1.0584155321121216\n",
            "Step [114/275]:\tLoss: 1.9336620569229126\n",
            "Step [115/275]:\tLoss: 2.58186674118042\n",
            "Step [116/275]:\tLoss: 1.5182948112487793\n",
            "Step [117/275]:\tLoss: 1.3133680820465088\n",
            "Step [118/275]:\tLoss: 2.6529622077941895\n",
            "Step [119/275]:\tLoss: 1.255113124847412\n",
            "Step [120/275]:\tLoss: 1.2452373504638672\n",
            "Step [121/275]:\tLoss: 1.3976974487304688\n",
            "Step [122/275]:\tLoss: 1.5641162395477295\n",
            "Step [123/275]:\tLoss: 1.2448678016662598\n",
            "Step [124/275]:\tLoss: 1.6551016569137573\n",
            "Step [125/275]:\tLoss: 2.2233200073242188\n",
            "Step [126/275]:\tLoss: 2.532740354537964\n",
            "Step [127/275]:\tLoss: 1.1967432498931885\n",
            "Step [128/275]:\tLoss: 1.9937396049499512\n",
            "Step [129/275]:\tLoss: 0.9475834369659424\n",
            "Step [130/275]:\tLoss: 2.6857190132141113\n",
            "Step [131/275]:\tLoss: 1.4581176042556763\n",
            "Step [132/275]:\tLoss: 1.959905982017517\n",
            "Step [133/275]:\tLoss: 1.0410661697387695\n",
            "Step [134/275]:\tLoss: 2.2115345001220703\n",
            "Step [135/275]:\tLoss: 1.7451837062835693\n",
            "Step [136/275]:\tLoss: 2.0951173305511475\n",
            "Step [137/275]:\tLoss: 1.2894439697265625\n",
            "Step [138/275]:\tLoss: 1.476388931274414\n",
            "Step [139/275]:\tLoss: 0.6631392240524292\n",
            "Step [140/275]:\tLoss: 0.4876202642917633\n",
            "Step [141/275]:\tLoss: 0.7199726104736328\n",
            "Step [142/275]:\tLoss: 0.8280211687088013\n",
            "Step [143/275]:\tLoss: 0.28289249539375305\n",
            "Step [144/275]:\tLoss: 1.051138997077942\n",
            "Step [145/275]:\tLoss: 0.8943345546722412\n",
            "Step [146/275]:\tLoss: 0.7138195633888245\n",
            "Step [147/275]:\tLoss: 0.9728394746780396\n",
            "Step [148/275]:\tLoss: 0.6746141910552979\n",
            "Step [149/275]:\tLoss: 0.7214664816856384\n",
            "Step [150/275]:\tLoss: 0.6891793012619019\n",
            "Step [151/275]:\tLoss: 0.25678518414497375\n",
            "Step [152/275]:\tLoss: 0.8897295594215393\n",
            "Step [153/275]:\tLoss: 0.4266333281993866\n",
            "Step [154/275]:\tLoss: 0.7719389200210571\n",
            "Step [155/275]:\tLoss: 0.7500777244567871\n",
            "Step [156/275]:\tLoss: 0.8819236755371094\n",
            "Step [157/275]:\tLoss: 0.3026679754257202\n",
            "Step [158/275]:\tLoss: 0.565645694732666\n",
            "Step [159/275]:\tLoss: 0.9120643138885498\n",
            "Step [160/275]:\tLoss: 0.6949893832206726\n",
            "Step [161/275]:\tLoss: 0.8381763100624084\n",
            "Step [162/275]:\tLoss: 0.8716009855270386\n",
            "Step [163/275]:\tLoss: 0.6578995585441589\n",
            "Step [164/275]:\tLoss: 0.6523627042770386\n",
            "Step [165/275]:\tLoss: 0.8553516864776611\n",
            "Step [166/275]:\tLoss: 0.909062922000885\n",
            "Step [167/275]:\tLoss: 0.22090134024620056\n",
            "Step [168/275]:\tLoss: 0.6964064240455627\n",
            "Step [169/275]:\tLoss: 0.6105234622955322\n",
            "Step [170/275]:\tLoss: 1.70225191116333\n",
            "Step [171/275]:\tLoss: 0.727006733417511\n",
            "Step [172/275]:\tLoss: 1.0506155490875244\n",
            "Step [173/275]:\tLoss: 0.9301213622093201\n",
            "Step [174/275]:\tLoss: 0.752252459526062\n",
            "Step [175/275]:\tLoss: 0.8834117650985718\n",
            "Step [176/275]:\tLoss: 0.5168766975402832\n",
            "Step [177/275]:\tLoss: 0.6529181003570557\n",
            "Step [178/275]:\tLoss: 0.9736301898956299\n",
            "Step [179/275]:\tLoss: 0.7887716889381409\n",
            "Step [180/275]:\tLoss: 0.39445245265960693\n",
            "Step [181/275]:\tLoss: 0.6725431084632874\n",
            "Step [182/275]:\tLoss: 0.7202901840209961\n",
            "Step [183/275]:\tLoss: 1.093705177307129\n",
            "Step [184/275]:\tLoss: 0.6534936428070068\n",
            "Step [185/275]:\tLoss: 0.6911572217941284\n",
            "Step [186/275]:\tLoss: 0.8178503513336182\n",
            "Step [187/275]:\tLoss: 0.3602653443813324\n",
            "Step [188/275]:\tLoss: 0.53086256980896\n",
            "Step [189/275]:\tLoss: 0.6839783191680908\n",
            "Step [190/275]:\tLoss: 0.2981308698654175\n",
            "Step [191/275]:\tLoss: 0.21224403381347656\n",
            "Step [192/275]:\tLoss: 2.8244833946228027\n",
            "Step [193/275]:\tLoss: 2.9019546508789062\n",
            "Step [194/275]:\tLoss: 2.902008295059204\n",
            "Step [195/275]:\tLoss: 2.807485580444336\n",
            "Step [196/275]:\tLoss: 2.6881890296936035\n",
            "Step [197/275]:\tLoss: 2.5193657875061035\n",
            "Step [198/275]:\tLoss: 2.0815322399139404\n",
            "Step [199/275]:\tLoss: 2.2758290767669678\n",
            "Step [200/275]:\tLoss: 1.8741531372070312\n",
            "Step [201/275]:\tLoss: 2.1670351028442383\n",
            "Step [202/275]:\tLoss: 2.615279197692871\n",
            "Step [203/275]:\tLoss: 2.4705557823181152\n",
            "Step [204/275]:\tLoss: 1.2801604270935059\n",
            "Step [205/275]:\tLoss: 2.239345073699951\n",
            "Step [206/275]:\tLoss: 2.459214210510254\n",
            "Step [207/275]:\tLoss: 2.205733060836792\n",
            "Step [208/275]:\tLoss: 1.9864475727081299\n",
            "Step [209/275]:\tLoss: 1.2945661544799805\n",
            "Step [210/275]:\tLoss: 1.7683632373809814\n",
            "Step [211/275]:\tLoss: 2.2072954177856445\n",
            "Step [212/275]:\tLoss: 1.8099545240402222\n",
            "Step [213/275]:\tLoss: 1.7076303958892822\n",
            "Step [214/275]:\tLoss: 1.9467906951904297\n",
            "Step [215/275]:\tLoss: 1.6195707321166992\n",
            "Step [216/275]:\tLoss: 1.711366891860962\n",
            "Step [217/275]:\tLoss: 1.8055014610290527\n",
            "Step [218/275]:\tLoss: 1.8970166444778442\n",
            "Step [219/275]:\tLoss: 2.1276800632476807\n",
            "Step [220/275]:\tLoss: 1.1854591369628906\n",
            "Step [221/275]:\tLoss: 1.588475227355957\n",
            "Step [222/275]:\tLoss: 1.5473659038543701\n",
            "Step [223/275]:\tLoss: 1.3150792121887207\n",
            "Step [224/275]:\tLoss: 1.51590895652771\n",
            "Step [225/275]:\tLoss: 1.4215772151947021\n",
            "Step [226/275]:\tLoss: 1.9520164728164673\n",
            "Step [227/275]:\tLoss: 1.5874788761138916\n",
            "Step [228/275]:\tLoss: 1.278322458267212\n",
            "Step [229/275]:\tLoss: 1.9081993103027344\n",
            "Step [230/275]:\tLoss: 1.163138508796692\n",
            "Step [231/275]:\tLoss: 1.298492670059204\n",
            "Step [232/275]:\tLoss: 1.3969569206237793\n",
            "Step [233/275]:\tLoss: 1.9678292274475098\n",
            "Step [234/275]:\tLoss: 1.5963175296783447\n",
            "Step [235/275]:\tLoss: 1.4756402969360352\n",
            "Step [236/275]:\tLoss: 1.1532037258148193\n",
            "Step [237/275]:\tLoss: 1.6723127365112305\n",
            "Step [238/275]:\tLoss: 1.1037263870239258\n",
            "Step [239/275]:\tLoss: 1.2540616989135742\n",
            "Step [240/275]:\tLoss: 1.9825594425201416\n",
            "Step [241/275]:\tLoss: 1.5841189622879028\n",
            "Step [242/275]:\tLoss: 1.6916816234588623\n",
            "Step [243/275]:\tLoss: 1.7231316566467285\n",
            "Step [244/275]:\tLoss: 1.3669023513793945\n",
            "Step [245/275]:\tLoss: 1.7524175643920898\n",
            "Step [246/275]:\tLoss: 1.6976262331008911\n",
            "Step [247/275]:\tLoss: 1.1447203159332275\n",
            "Step [248/275]:\tLoss: 1.081982135772705\n",
            "Step [249/275]:\tLoss: 1.6776764392852783\n",
            "Step [250/275]:\tLoss: 1.1657360792160034\n",
            "Step [251/275]:\tLoss: 2.5532217025756836\n",
            "Step [252/275]:\tLoss: 1.284451961517334\n",
            "Step [253/275]:\tLoss: 2.1086292266845703\n",
            "Step [254/275]:\tLoss: 2.082092046737671\n",
            "Step [255/275]:\tLoss: 1.9370499849319458\n",
            "Step [256/275]:\tLoss: 2.453592300415039\n",
            "Step [257/275]:\tLoss: 1.743177890777588\n",
            "Step [258/275]:\tLoss: 1.5785802602767944\n",
            "Step [259/275]:\tLoss: 0.9295389652252197\n",
            "Step [260/275]:\tLoss: 1.597581148147583\n",
            "Step [261/275]:\tLoss: 1.575932264328003\n",
            "Step [262/275]:\tLoss: 1.6032447814941406\n",
            "Step [263/275]:\tLoss: 1.6102689504623413\n",
            "Step [264/275]:\tLoss: 1.1289632320404053\n",
            "Step [265/275]:\tLoss: 2.965818166732788\n",
            "Step [266/275]:\tLoss: 1.6451973915100098\n",
            "Step [267/275]:\tLoss: 1.6775562763214111\n",
            "Step [268/275]:\tLoss: 1.0557281970977783\n",
            "Step [269/275]:\tLoss: 1.4436343908309937\n",
            "Step [270/275]:\tLoss: 1.130443811416626\n",
            "Step [271/275]:\tLoss: 1.6634087562561035\n",
            "Step [272/275]:\tLoss: 1.38640296459198\n",
            "Step [273/275]:\tLoss: 1.0946311950683594\n",
            "Step [274/275]:\tLoss: 1.4480499029159546\n",
            "epoch number 107\n",
            "Step [0/275]:\tLoss: 0.48045146465301514\n",
            "Step [1/275]:\tLoss: 1.769458532333374\n",
            "Step [2/275]:\tLoss: 1.4727712869644165\n",
            "Step [3/275]:\tLoss: 1.6762934923171997\n",
            "Step [4/275]:\tLoss: 0.603097140789032\n",
            "Step [5/275]:\tLoss: 1.2817021608352661\n",
            "Step [6/275]:\tLoss: 0.8651991486549377\n",
            "Step [7/275]:\tLoss: 1.9024574756622314\n",
            "Step [8/275]:\tLoss: 2.710944175720215\n",
            "Step [9/275]:\tLoss: 1.2660400867462158\n",
            "Step [10/275]:\tLoss: 0.4853495955467224\n",
            "Step [11/275]:\tLoss: 0.7517478466033936\n",
            "Step [12/275]:\tLoss: 2.544004440307617\n",
            "Step [13/275]:\tLoss: 2.0607852935791016\n",
            "Step [14/275]:\tLoss: 0.8467047810554504\n",
            "Step [15/275]:\tLoss: 0.759141206741333\n",
            "Step [16/275]:\tLoss: 2.373049736022949\n",
            "Step [17/275]:\tLoss: 1.5801191329956055\n",
            "Step [18/275]:\tLoss: 1.973761796951294\n",
            "Step [19/275]:\tLoss: 0.8854339122772217\n",
            "Step [20/275]:\tLoss: 0.6316790580749512\n",
            "Step [21/275]:\tLoss: 2.817850112915039\n",
            "Step [22/275]:\tLoss: 1.0307209491729736\n",
            "Step [23/275]:\tLoss: 2.3049111366271973\n",
            "Step [24/275]:\tLoss: 1.255967378616333\n",
            "Step [25/275]:\tLoss: 0.8540685772895813\n",
            "Step [26/275]:\tLoss: 0.7217304706573486\n",
            "Step [27/275]:\tLoss: 1.3061250448226929\n",
            "Step [28/275]:\tLoss: 3.158787727355957\n",
            "Step [29/275]:\tLoss: 0.6362453699111938\n",
            "Step [30/275]:\tLoss: 1.5621062517166138\n",
            "Step [31/275]:\tLoss: 0.8343508839607239\n",
            "Step [32/275]:\tLoss: 0.6431381702423096\n",
            "Step [33/275]:\tLoss: 1.0933842658996582\n",
            "Step [34/275]:\tLoss: 0.8448123931884766\n",
            "Step [35/275]:\tLoss: 0.5721801519393921\n",
            "Step [36/275]:\tLoss: 0.7495580911636353\n",
            "Step [37/275]:\tLoss: 0.7127631306648254\n",
            "Step [38/275]:\tLoss: 0.6920926570892334\n",
            "Step [39/275]:\tLoss: 0.6741628646850586\n",
            "Step [40/275]:\tLoss: 0.9131211042404175\n",
            "Step [41/275]:\tLoss: 0.6358919143676758\n",
            "Step [42/275]:\tLoss: 0.6554655432701111\n",
            "Step [43/275]:\tLoss: 0.5775126218795776\n",
            "Step [44/275]:\tLoss: 0.6350666284561157\n",
            "Step [45/275]:\tLoss: 0.6299516558647156\n",
            "Step [46/275]:\tLoss: 0.6972078680992126\n",
            "Step [47/275]:\tLoss: 0.6288912296295166\n",
            "Step [48/275]:\tLoss: 0.20218497514724731\n",
            "Step [49/275]:\tLoss: 0.7690237760543823\n",
            "Step [50/275]:\tLoss: 0.6573194265365601\n",
            "Step [51/275]:\tLoss: 0.823424220085144\n",
            "Step [52/275]:\tLoss: 0.414118230342865\n",
            "Step [53/275]:\tLoss: 0.38339346647262573\n",
            "Step [54/275]:\tLoss: 0.9798641204833984\n",
            "Step [55/275]:\tLoss: 0.4353691041469574\n",
            "Step [56/275]:\tLoss: 0.8022830486297607\n",
            "Step [57/275]:\tLoss: 0.15852192044258118\n",
            "Step [58/275]:\tLoss: 0.3848808705806732\n",
            "Step [59/275]:\tLoss: 0.3446352481842041\n",
            "Step [60/275]:\tLoss: 0.3646472692489624\n",
            "Step [61/275]:\tLoss: 0.1571681797504425\n",
            "Step [62/275]:\tLoss: 1.1510088443756104\n",
            "Step [63/275]:\tLoss: 0.540108323097229\n",
            "Step [64/275]:\tLoss: 0.6458592414855957\n",
            "Step [65/275]:\tLoss: 0.7789295315742493\n",
            "Step [66/275]:\tLoss: 0.5692042112350464\n",
            "Step [67/275]:\tLoss: 0.5313928127288818\n",
            "Step [68/275]:\tLoss: 0.584723949432373\n",
            "Step [69/275]:\tLoss: 0.5912666916847229\n",
            "Step [70/275]:\tLoss: 0.6029065251350403\n",
            "Step [71/275]:\tLoss: 0.7724059224128723\n",
            "Step [72/275]:\tLoss: 0.5049042701721191\n",
            "Step [73/275]:\tLoss: 0.6176209449768066\n",
            "Step [74/275]:\tLoss: 0.35287535190582275\n",
            "Step [75/275]:\tLoss: 0.7750519514083862\n",
            "Step [76/275]:\tLoss: 0.9133820533752441\n",
            "Step [77/275]:\tLoss: 0.5913466215133667\n",
            "Step [78/275]:\tLoss: 0.5896947979927063\n",
            "Step [79/275]:\tLoss: 0.15242275595664978\n",
            "Step [80/275]:\tLoss: 0.43748149275779724\n",
            "Step [81/275]:\tLoss: 0.6654578447341919\n",
            "Step [82/275]:\tLoss: 0.7843466401100159\n",
            "Step [83/275]:\tLoss: 0.26859790086746216\n",
            "Step [84/275]:\tLoss: 0.15429165959358215\n",
            "Step [85/275]:\tLoss: 0.7493038773536682\n",
            "Step [86/275]:\tLoss: 0.572374165058136\n",
            "Step [87/275]:\tLoss: 0.5818633437156677\n",
            "Step [88/275]:\tLoss: 0.6467825174331665\n",
            "Step [89/275]:\tLoss: 0.5739963054656982\n",
            "Step [90/275]:\tLoss: 0.5524608492851257\n",
            "Step [91/275]:\tLoss: 0.15169483423233032\n",
            "Step [92/275]:\tLoss: 0.46100983023643494\n",
            "Step [93/275]:\tLoss: 0.5198041200637817\n",
            "Step [94/275]:\tLoss: 0.4373452365398407\n",
            "Step [95/275]:\tLoss: 0.5639463663101196\n",
            "Step [96/275]:\tLoss: 0.8860388994216919\n",
            "Step [97/275]:\tLoss: 0.4227240979671478\n",
            "Step [98/275]:\tLoss: 0.3266642987728119\n",
            "Step [99/275]:\tLoss: 0.7376922369003296\n",
            "Step [100/275]:\tLoss: 0.5319961309432983\n",
            "Step [101/275]:\tLoss: 0.6154114007949829\n",
            "Step [102/275]:\tLoss: 0.36590245366096497\n",
            "Step [103/275]:\tLoss: 0.578506350517273\n",
            "Step [104/275]:\tLoss: 0.7955037355422974\n",
            "Step [105/275]:\tLoss: 0.6803892850875854\n",
            "Step [106/275]:\tLoss: 0.16142694652080536\n",
            "Step [107/275]:\tLoss: 0.6020828485488892\n",
            "Step [108/275]:\tLoss: 0.14692959189414978\n",
            "Step [109/275]:\tLoss: 0.5816779136657715\n",
            "Step [110/275]:\tLoss: 2.812427520751953\n",
            "Step [111/275]:\tLoss: 2.579160690307617\n",
            "Step [112/275]:\tLoss: 4.007184028625488\n",
            "Step [113/275]:\tLoss: 0.7657298445701599\n",
            "Step [114/275]:\tLoss: 3.494328737258911\n",
            "Step [115/275]:\tLoss: 2.71746826171875\n",
            "Step [116/275]:\tLoss: 1.9127898216247559\n",
            "Step [117/275]:\tLoss: 3.8020195960998535\n",
            "Step [118/275]:\tLoss: 1.1826317310333252\n",
            "Step [119/275]:\tLoss: 0.8246601819992065\n",
            "Step [120/275]:\tLoss: 1.6673580408096313\n",
            "Step [121/275]:\tLoss: 2.621832847595215\n",
            "Step [122/275]:\tLoss: 2.2190892696380615\n",
            "Step [123/275]:\tLoss: 1.4149205684661865\n",
            "Step [124/275]:\tLoss: 1.9702173471450806\n",
            "Step [125/275]:\tLoss: 1.2701019048690796\n",
            "Step [126/275]:\tLoss: 1.5660083293914795\n",
            "Step [127/275]:\tLoss: 1.8506522178649902\n",
            "Step [128/275]:\tLoss: 2.5425167083740234\n",
            "Step [129/275]:\tLoss: 2.212296485900879\n",
            "Step [130/275]:\tLoss: 2.0074310302734375\n",
            "Step [131/275]:\tLoss: 1.5802264213562012\n",
            "Step [132/275]:\tLoss: 2.45963191986084\n",
            "Step [133/275]:\tLoss: 2.158761978149414\n",
            "Step [134/275]:\tLoss: 1.4927067756652832\n",
            "Step [135/275]:\tLoss: 1.8715684413909912\n",
            "Step [136/275]:\tLoss: 1.9054431915283203\n",
            "Step [137/275]:\tLoss: 1.8384699821472168\n",
            "Step [138/275]:\tLoss: 0.896931529045105\n",
            "Step [139/275]:\tLoss: 1.0589662790298462\n",
            "Step [140/275]:\tLoss: 1.2320666313171387\n",
            "Step [141/275]:\tLoss: 0.7282339334487915\n",
            "Step [142/275]:\tLoss: 0.6018025875091553\n",
            "Step [143/275]:\tLoss: 0.9564428329467773\n",
            "Step [144/275]:\tLoss: 0.5552771091461182\n",
            "Step [145/275]:\tLoss: 0.5786185264587402\n",
            "Step [146/275]:\tLoss: 0.40684381127357483\n",
            "Step [147/275]:\tLoss: 0.8769097924232483\n",
            "Step [148/275]:\tLoss: 0.6494451761245728\n",
            "Step [149/275]:\tLoss: 0.3809909224510193\n",
            "Step [150/275]:\tLoss: 0.7327494621276855\n",
            "Step [151/275]:\tLoss: 0.746566116809845\n",
            "Step [152/275]:\tLoss: 0.4954356849193573\n",
            "Step [153/275]:\tLoss: 0.5565693378448486\n",
            "Step [154/275]:\tLoss: 0.7255045175552368\n",
            "Step [155/275]:\tLoss: 0.5556204319000244\n",
            "Step [156/275]:\tLoss: 0.682253360748291\n",
            "Step [157/275]:\tLoss: 0.5613692402839661\n",
            "Step [158/275]:\tLoss: 0.1806674301624298\n",
            "Step [159/275]:\tLoss: 0.6068246364593506\n",
            "Step [160/275]:\tLoss: 0.8308876752853394\n",
            "Step [161/275]:\tLoss: 0.8757998943328857\n",
            "Step [162/275]:\tLoss: 0.5831927061080933\n",
            "Step [163/275]:\tLoss: 0.17932751774787903\n",
            "Step [164/275]:\tLoss: 0.7208448648452759\n",
            "Step [165/275]:\tLoss: 0.6173604726791382\n",
            "Step [166/275]:\tLoss: 0.7518014311790466\n",
            "Step [167/275]:\tLoss: 0.612786591053009\n",
            "Step [168/275]:\tLoss: 0.8463468551635742\n",
            "Step [169/275]:\tLoss: 0.6748995780944824\n",
            "Step [170/275]:\tLoss: 0.27821725606918335\n",
            "Step [171/275]:\tLoss: 0.5748490691184998\n",
            "Step [172/275]:\tLoss: 0.6094560027122498\n",
            "Step [173/275]:\tLoss: 0.5896623730659485\n",
            "Step [174/275]:\tLoss: 0.712850034236908\n",
            "Step [175/275]:\tLoss: 1.281336784362793\n",
            "Step [176/275]:\tLoss: 0.45437169075012207\n",
            "Step [177/275]:\tLoss: 0.9801396727561951\n",
            "Step [178/275]:\tLoss: 0.6158236265182495\n",
            "Step [179/275]:\tLoss: 0.8319574594497681\n",
            "Step [180/275]:\tLoss: 0.5131314396858215\n",
            "Step [181/275]:\tLoss: 0.3581651449203491\n",
            "Step [182/275]:\tLoss: 0.7336629629135132\n",
            "Step [183/275]:\tLoss: 0.5881098508834839\n",
            "Step [184/275]:\tLoss: 0.520875871181488\n",
            "Step [185/275]:\tLoss: 0.18476244807243347\n",
            "Step [186/275]:\tLoss: 0.18362432718276978\n",
            "Step [187/275]:\tLoss: 0.19654297828674316\n",
            "Step [188/275]:\tLoss: 0.8295819163322449\n",
            "Step [189/275]:\tLoss: 0.5241886377334595\n",
            "Step [190/275]:\tLoss: 0.47268736362457275\n",
            "Step [191/275]:\tLoss: 0.8350929021835327\n",
            "Step [192/275]:\tLoss: 2.5538885593414307\n",
            "Step [193/275]:\tLoss: 2.9062883853912354\n",
            "Step [194/275]:\tLoss: 2.9045863151550293\n",
            "Step [195/275]:\tLoss: 2.7146146297454834\n",
            "Step [196/275]:\tLoss: 3.061641216278076\n",
            "Step [197/275]:\tLoss: 2.0873188972473145\n",
            "Step [198/275]:\tLoss: 2.1150026321411133\n",
            "Step [199/275]:\tLoss: 2.4008631706237793\n",
            "Step [200/275]:\tLoss: 2.613081932067871\n",
            "Step [201/275]:\tLoss: 1.810666561126709\n",
            "Step [202/275]:\tLoss: 1.909512996673584\n",
            "Step [203/275]:\tLoss: 2.0096230506896973\n",
            "Step [204/275]:\tLoss: 1.5902316570281982\n",
            "Step [205/275]:\tLoss: 1.2893586158752441\n",
            "Step [206/275]:\tLoss: 1.4793941974639893\n",
            "Step [207/275]:\tLoss: 1.9929015636444092\n",
            "Step [208/275]:\tLoss: 1.8525378704071045\n",
            "Step [209/275]:\tLoss: 1.6900076866149902\n",
            "Step [210/275]:\tLoss: 1.5205824375152588\n",
            "Step [211/275]:\tLoss: 1.547692060470581\n",
            "Step [212/275]:\tLoss: 1.913773536682129\n",
            "Step [213/275]:\tLoss: 1.1917930841445923\n",
            "Step [214/275]:\tLoss: 1.4031317234039307\n",
            "Step [215/275]:\tLoss: 1.7157258987426758\n",
            "Step [216/275]:\tLoss: 1.0128140449523926\n",
            "Step [217/275]:\tLoss: 2.1284384727478027\n",
            "Step [218/275]:\tLoss: 1.0424082279205322\n",
            "Step [219/275]:\tLoss: 1.2913224697113037\n",
            "Step [220/275]:\tLoss: 1.9509003162384033\n",
            "Step [221/275]:\tLoss: 1.8368808031082153\n",
            "Step [222/275]:\tLoss: 1.0191792249679565\n",
            "Step [223/275]:\tLoss: 1.0853407382965088\n",
            "Step [224/275]:\tLoss: 2.6166160106658936\n",
            "Step [225/275]:\tLoss: 1.7869354486465454\n",
            "Step [226/275]:\tLoss: 1.2668852806091309\n",
            "Step [227/275]:\tLoss: 1.5263886451721191\n",
            "Step [228/275]:\tLoss: 1.1297132968902588\n",
            "Step [229/275]:\tLoss: 1.2815320491790771\n",
            "Step [230/275]:\tLoss: 1.6464033126831055\n",
            "Step [231/275]:\tLoss: 1.1396788358688354\n",
            "Step [232/275]:\tLoss: 1.0494965314865112\n",
            "Step [233/275]:\tLoss: 0.9089303016662598\n",
            "Step [234/275]:\tLoss: 1.0679106712341309\n",
            "Step [235/275]:\tLoss: 1.5593910217285156\n",
            "Step [236/275]:\tLoss: 0.8800338506698608\n",
            "Step [237/275]:\tLoss: 1.345961332321167\n",
            "Step [238/275]:\tLoss: 1.9984210729599\n",
            "Step [239/275]:\tLoss: 1.5386384725570679\n",
            "Step [240/275]:\tLoss: 1.1478321552276611\n",
            "Step [241/275]:\tLoss: 2.2449676990509033\n",
            "Step [242/275]:\tLoss: 1.8344323635101318\n",
            "Step [243/275]:\tLoss: 1.360055923461914\n",
            "Step [244/275]:\tLoss: 1.701584815979004\n",
            "Step [245/275]:\tLoss: 2.20656681060791\n",
            "Step [246/275]:\tLoss: 2.0969748497009277\n",
            "Step [247/275]:\tLoss: 1.6006499528884888\n",
            "Step [248/275]:\tLoss: 1.7476179599761963\n",
            "Step [249/275]:\tLoss: 1.3541828393936157\n",
            "Step [250/275]:\tLoss: 1.921741247177124\n",
            "Step [251/275]:\tLoss: 2.420292854309082\n",
            "Step [252/275]:\tLoss: 1.830129623413086\n",
            "Step [253/275]:\tLoss: 1.9324843883514404\n",
            "Step [254/275]:\tLoss: 2.854624032974243\n",
            "Step [255/275]:\tLoss: 1.0962903499603271\n",
            "Step [256/275]:\tLoss: 1.4079198837280273\n",
            "Step [257/275]:\tLoss: 1.4586135149002075\n",
            "Step [258/275]:\tLoss: 1.5446884632110596\n",
            "Step [259/275]:\tLoss: 1.2696523666381836\n",
            "Step [260/275]:\tLoss: 2.624821186065674\n",
            "Step [261/275]:\tLoss: 2.0196659564971924\n",
            "Step [262/275]:\tLoss: 3.240516185760498\n",
            "Step [263/275]:\tLoss: 1.278895616531372\n",
            "Step [264/275]:\tLoss: 1.9388355016708374\n",
            "Step [265/275]:\tLoss: 1.9257557392120361\n",
            "Step [266/275]:\tLoss: 1.9264174699783325\n",
            "Step [267/275]:\tLoss: 1.7027798891067505\n",
            "Step [268/275]:\tLoss: 1.2095677852630615\n",
            "Step [269/275]:\tLoss: 2.8390390872955322\n",
            "Step [270/275]:\tLoss: 1.629960536956787\n",
            "Step [271/275]:\tLoss: 1.3161399364471436\n",
            "Step [272/275]:\tLoss: 2.1550302505493164\n",
            "Step [273/275]:\tLoss: 2.040764093399048\n",
            "Step [274/275]:\tLoss: 1.8724782466888428\n",
            "epoch number 108\n",
            "Step [0/275]:\tLoss: 3.0452470779418945\n",
            "Step [1/275]:\tLoss: 3.1278748512268066\n",
            "Step [2/275]:\tLoss: 1.991849422454834\n",
            "Step [3/275]:\tLoss: 3.09761643409729\n",
            "Step [4/275]:\tLoss: 1.8467659950256348\n",
            "Step [5/275]:\tLoss: 2.8968541622161865\n",
            "Step [6/275]:\tLoss: 1.9400098323822021\n",
            "Step [7/275]:\tLoss: 1.4769972562789917\n",
            "Step [8/275]:\tLoss: 2.169595241546631\n",
            "Step [9/275]:\tLoss: 2.2217891216278076\n",
            "Step [10/275]:\tLoss: 1.6730411052703857\n",
            "Step [11/275]:\tLoss: 1.2035408020019531\n",
            "Step [12/275]:\tLoss: 1.4418421983718872\n",
            "Step [13/275]:\tLoss: 0.9909713864326477\n",
            "Step [14/275]:\tLoss: 0.9795910120010376\n",
            "Step [15/275]:\tLoss: 0.7509579062461853\n",
            "Step [16/275]:\tLoss: 0.8318256139755249\n",
            "Step [17/275]:\tLoss: 2.5063607692718506\n",
            "Step [18/275]:\tLoss: 0.30136311054229736\n",
            "Step [19/275]:\tLoss: 0.6590322256088257\n",
            "Step [20/275]:\tLoss: 0.6886799335479736\n",
            "Step [21/275]:\tLoss: 3.7237091064453125\n",
            "Step [22/275]:\tLoss: 0.5656031966209412\n",
            "Step [23/275]:\tLoss: 3.0544912815093994\n",
            "Step [24/275]:\tLoss: 0.7445238828659058\n",
            "Step [25/275]:\tLoss: 1.8952630758285522\n",
            "Step [26/275]:\tLoss: 0.3512958586215973\n",
            "Step [27/275]:\tLoss: 1.688302755355835\n",
            "Step [28/275]:\tLoss: 2.5154972076416016\n",
            "Step [29/275]:\tLoss: 0.23862460255622864\n",
            "Step [30/275]:\tLoss: 1.2795498371124268\n",
            "Step [31/275]:\tLoss: 0.5982025861740112\n",
            "Step [32/275]:\tLoss: 0.27501556277275085\n",
            "Step [33/275]:\tLoss: 0.5085824131965637\n",
            "Step [34/275]:\tLoss: 0.44846493005752563\n",
            "Step [35/275]:\tLoss: 0.7545090913772583\n",
            "Step [36/275]:\tLoss: 0.6394742131233215\n",
            "Step [37/275]:\tLoss: 0.4257194697856903\n",
            "Step [38/275]:\tLoss: 0.4933162331581116\n",
            "Step [39/275]:\tLoss: 0.5554755926132202\n",
            "Step [40/275]:\tLoss: 0.6125979423522949\n",
            "Step [41/275]:\tLoss: 0.8776845932006836\n",
            "Step [42/275]:\tLoss: 0.5839872360229492\n",
            "Step [43/275]:\tLoss: 0.6565669178962708\n",
            "Step [44/275]:\tLoss: 0.5571835041046143\n",
            "Step [45/275]:\tLoss: 0.6222966909408569\n",
            "Step [46/275]:\tLoss: 1.1022908687591553\n",
            "Step [47/275]:\tLoss: 0.5043327808380127\n",
            "Step [48/275]:\tLoss: 0.7573443651199341\n",
            "Step [49/275]:\tLoss: 0.5997123122215271\n",
            "Step [50/275]:\tLoss: 0.6679424047470093\n",
            "Step [51/275]:\tLoss: 0.6087376475334167\n",
            "Step [52/275]:\tLoss: 0.807525634765625\n",
            "Step [53/275]:\tLoss: 0.8478763699531555\n",
            "Step [54/275]:\tLoss: 0.5860108733177185\n",
            "Step [55/275]:\tLoss: 0.5311852693557739\n",
            "Step [56/275]:\tLoss: 0.4047517478466034\n",
            "Step [57/275]:\tLoss: 0.3867751359939575\n",
            "Step [58/275]:\tLoss: 0.5830665230751038\n",
            "Step [59/275]:\tLoss: 0.6070199012756348\n",
            "Step [60/275]:\tLoss: 0.6542932987213135\n",
            "Step [61/275]:\tLoss: 0.767785906791687\n",
            "Step [62/275]:\tLoss: 0.5773131847381592\n",
            "Step [63/275]:\tLoss: 0.5311541557312012\n",
            "Step [64/275]:\tLoss: 0.5881253480911255\n",
            "Step [65/275]:\tLoss: 0.5742026567459106\n",
            "Step [66/275]:\tLoss: 0.5754523873329163\n",
            "Step [67/275]:\tLoss: 0.3523441553115845\n",
            "Step [68/275]:\tLoss: 0.45328032970428467\n",
            "Step [69/275]:\tLoss: 0.7205787897109985\n",
            "Step [70/275]:\tLoss: 0.56592857837677\n",
            "Step [71/275]:\tLoss: 0.18189644813537598\n",
            "Step [72/275]:\tLoss: 0.7468876242637634\n",
            "Step [73/275]:\tLoss: 1.335677146911621\n",
            "Step [74/275]:\tLoss: 0.7623848915100098\n",
            "Step [75/275]:\tLoss: 0.34054479002952576\n",
            "Step [76/275]:\tLoss: 0.5484026074409485\n",
            "Step [77/275]:\tLoss: 0.7942212820053101\n",
            "Step [78/275]:\tLoss: 0.561439037322998\n",
            "Step [79/275]:\tLoss: 0.7657022476196289\n",
            "Step [80/275]:\tLoss: 0.5749384164810181\n",
            "Step [81/275]:\tLoss: 0.7329789400100708\n",
            "Step [82/275]:\tLoss: 0.4589236378669739\n",
            "Step [83/275]:\tLoss: 0.7715017199516296\n",
            "Step [84/275]:\tLoss: 0.5723108053207397\n",
            "Step [85/275]:\tLoss: 0.5505895614624023\n",
            "Step [86/275]:\tLoss: 1.054422378540039\n",
            "Step [87/275]:\tLoss: 0.7510921955108643\n",
            "Step [88/275]:\tLoss: 0.5977783203125\n",
            "Step [89/275]:\tLoss: 0.8248775005340576\n",
            "Step [90/275]:\tLoss: 0.6903579831123352\n",
            "Step [91/275]:\tLoss: 0.6729831695556641\n",
            "Step [92/275]:\tLoss: 0.6813894510269165\n",
            "Step [93/275]:\tLoss: 0.6049469709396362\n",
            "Step [94/275]:\tLoss: 0.7615126371383667\n",
            "Step [95/275]:\tLoss: 0.8368238210678101\n",
            "Step [96/275]:\tLoss: 0.6856465935707092\n",
            "Step [97/275]:\tLoss: 1.067410945892334\n",
            "Step [98/275]:\tLoss: 0.5697072744369507\n",
            "Step [99/275]:\tLoss: 0.6545224189758301\n",
            "Step [100/275]:\tLoss: 0.1813836395740509\n",
            "Step [101/275]:\tLoss: 0.8982772827148438\n",
            "Step [102/275]:\tLoss: 0.5610545873641968\n",
            "Step [103/275]:\tLoss: 0.7820914387702942\n",
            "Step [104/275]:\tLoss: 0.3711256980895996\n",
            "Step [105/275]:\tLoss: 0.5687127113342285\n",
            "Step [106/275]:\tLoss: 0.8743205666542053\n",
            "Step [107/275]:\tLoss: 0.6553958654403687\n",
            "Step [108/275]:\tLoss: 0.6645883917808533\n",
            "Step [109/275]:\tLoss: 0.5360391736030579\n",
            "Step [110/275]:\tLoss: 1.9184625148773193\n",
            "Step [111/275]:\tLoss: 3.639875650405884\n",
            "Step [112/275]:\tLoss: 2.8649649620056152\n",
            "Step [113/275]:\tLoss: 1.9602102041244507\n",
            "Step [114/275]:\tLoss: 0.7214987874031067\n",
            "Step [115/275]:\tLoss: 3.385565757751465\n",
            "Step [116/275]:\tLoss: 2.160707712173462\n",
            "Step [117/275]:\tLoss: 2.681211471557617\n",
            "Step [118/275]:\tLoss: 1.7790498733520508\n",
            "Step [119/275]:\tLoss: 0.5896891951560974\n",
            "Step [120/275]:\tLoss: 0.21495631337165833\n",
            "Step [121/275]:\tLoss: 0.7054665088653564\n",
            "Step [122/275]:\tLoss: 1.7537109851837158\n",
            "Step [123/275]:\tLoss: 0.45320022106170654\n",
            "Step [124/275]:\tLoss: 1.701023817062378\n",
            "Step [125/275]:\tLoss: 1.316438913345337\n",
            "Step [126/275]:\tLoss: 2.9024713039398193\n",
            "Step [127/275]:\tLoss: 1.2619506120681763\n",
            "Step [128/275]:\tLoss: 0.26164761185646057\n",
            "Step [129/275]:\tLoss: 0.7069909572601318\n",
            "Step [130/275]:\tLoss: 1.1418342590332031\n",
            "Step [131/275]:\tLoss: 2.208449125289917\n",
            "Step [132/275]:\tLoss: 0.7158414125442505\n",
            "Step [133/275]:\tLoss: 0.47123822569847107\n",
            "Step [134/275]:\tLoss: 1.047808051109314\n",
            "Step [135/275]:\tLoss: 2.9576234817504883\n",
            "Step [136/275]:\tLoss: 1.6292381286621094\n",
            "Step [137/275]:\tLoss: 0.9608235955238342\n",
            "Step [138/275]:\tLoss: 0.6623830795288086\n",
            "Step [139/275]:\tLoss: 0.7770546674728394\n",
            "Step [140/275]:\tLoss: 0.7790805101394653\n",
            "Step [141/275]:\tLoss: 0.48514115810394287\n",
            "Step [142/275]:\tLoss: 0.26240330934524536\n",
            "Step [143/275]:\tLoss: 0.6043314933776855\n",
            "Step [144/275]:\tLoss: 0.5409212112426758\n",
            "Step [145/275]:\tLoss: 0.5642118453979492\n",
            "Step [146/275]:\tLoss: 0.4525662362575531\n",
            "Step [147/275]:\tLoss: 0.8754658699035645\n",
            "Step [148/275]:\tLoss: 0.2791716456413269\n",
            "Step [149/275]:\tLoss: 0.7624018788337708\n",
            "Step [150/275]:\tLoss: 0.3189759850502014\n",
            "Step [151/275]:\tLoss: 0.746462345123291\n",
            "Step [152/275]:\tLoss: 0.5911674499511719\n",
            "Step [153/275]:\tLoss: 0.48530369997024536\n",
            "Step [154/275]:\tLoss: 0.24066436290740967\n",
            "Step [155/275]:\tLoss: 0.3443794250488281\n",
            "Step [156/275]:\tLoss: 0.7640823125839233\n",
            "Step [157/275]:\tLoss: 0.6059503555297852\n",
            "Step [158/275]:\tLoss: 0.4034595191478729\n",
            "Step [159/275]:\tLoss: 0.686166524887085\n",
            "Step [160/275]:\tLoss: 0.24673020839691162\n",
            "Step [161/275]:\tLoss: 0.43202725052833557\n",
            "Step [162/275]:\tLoss: 0.1998029500246048\n",
            "Step [163/275]:\tLoss: 0.951262891292572\n",
            "Step [164/275]:\tLoss: 0.9802236557006836\n",
            "Step [165/275]:\tLoss: 1.0122236013412476\n",
            "Step [166/275]:\tLoss: 0.5584271550178528\n",
            "Step [167/275]:\tLoss: 0.3941043019294739\n",
            "Step [168/275]:\tLoss: 0.549500048160553\n",
            "Step [169/275]:\tLoss: 0.46199852228164673\n",
            "Step [170/275]:\tLoss: 0.5328591465950012\n",
            "Step [171/275]:\tLoss: 0.8428068161010742\n",
            "Step [172/275]:\tLoss: 0.5738683938980103\n",
            "Step [173/275]:\tLoss: 0.5612053275108337\n",
            "Step [174/275]:\tLoss: 0.558018147945404\n",
            "Step [175/275]:\tLoss: 0.19604723155498505\n",
            "Step [176/275]:\tLoss: 0.5836145281791687\n",
            "Step [177/275]:\tLoss: 0.6481125354766846\n",
            "Step [178/275]:\tLoss: 0.5463091135025024\n",
            "Step [179/275]:\tLoss: 0.5940409898757935\n",
            "Step [180/275]:\tLoss: 0.9222259521484375\n",
            "Step [181/275]:\tLoss: 0.42515993118286133\n",
            "Step [182/275]:\tLoss: 0.7772216200828552\n",
            "Step [183/275]:\tLoss: 0.6259373426437378\n",
            "Step [184/275]:\tLoss: 0.5911905169487\n",
            "Step [185/275]:\tLoss: 0.6097984313964844\n",
            "Step [186/275]:\tLoss: 1.054534673690796\n",
            "Step [187/275]:\tLoss: 0.7604079246520996\n",
            "Step [188/275]:\tLoss: 0.649882972240448\n",
            "Step [189/275]:\tLoss: 0.47763705253601074\n",
            "Step [190/275]:\tLoss: 0.5526297092437744\n",
            "Step [191/275]:\tLoss: 0.7145379781723022\n",
            "Step [192/275]:\tLoss: 1.337621808052063\n",
            "Step [193/275]:\tLoss: 2.57064151763916\n",
            "Step [194/275]:\tLoss: 2.3438587188720703\n",
            "Step [195/275]:\tLoss: 2.161170721054077\n",
            "Step [196/275]:\tLoss: 1.8937106132507324\n",
            "Step [197/275]:\tLoss: 2.7767958641052246\n",
            "Step [198/275]:\tLoss: 1.4831469058990479\n",
            "Step [199/275]:\tLoss: 1.9881083965301514\n",
            "Step [200/275]:\tLoss: 1.927984356880188\n",
            "Step [201/275]:\tLoss: 1.6935920715332031\n",
            "Step [202/275]:\tLoss: 1.6384773254394531\n",
            "Step [203/275]:\tLoss: 3.1309728622436523\n",
            "Step [204/275]:\tLoss: 1.7379422187805176\n",
            "Step [205/275]:\tLoss: 1.5103662014007568\n",
            "Step [206/275]:\tLoss: 1.9973386526107788\n",
            "Step [207/275]:\tLoss: 1.4240896701812744\n",
            "Step [208/275]:\tLoss: 1.8083038330078125\n",
            "Step [209/275]:\tLoss: 1.7475701570510864\n",
            "Step [210/275]:\tLoss: 1.5207284688949585\n",
            "Step [211/275]:\tLoss: 2.015383243560791\n",
            "Step [212/275]:\tLoss: 2.173830032348633\n",
            "Step [213/275]:\tLoss: 2.09407639503479\n",
            "Step [214/275]:\tLoss: 1.6617381572723389\n",
            "Step [215/275]:\tLoss: 1.315475583076477\n",
            "Step [216/275]:\tLoss: 1.6527929306030273\n",
            "Step [217/275]:\tLoss: 1.5047547817230225\n",
            "Step [218/275]:\tLoss: 1.1105165481567383\n",
            "Step [219/275]:\tLoss: 2.3933465480804443\n",
            "Step [220/275]:\tLoss: 2.110548257827759\n",
            "Step [221/275]:\tLoss: 1.7403112649917603\n",
            "Step [222/275]:\tLoss: 2.9233555793762207\n",
            "Step [223/275]:\tLoss: 1.402986764907837\n",
            "Step [224/275]:\tLoss: 1.3227763175964355\n",
            "Step [225/275]:\tLoss: 1.2979868650436401\n",
            "Step [226/275]:\tLoss: 1.4408988952636719\n",
            "Step [227/275]:\tLoss: 1.53228759765625\n",
            "Step [228/275]:\tLoss: 1.2115755081176758\n",
            "Step [229/275]:\tLoss: 1.575601577758789\n",
            "Step [230/275]:\tLoss: 1.0585191249847412\n",
            "Step [231/275]:\tLoss: 1.1222070455551147\n",
            "Step [232/275]:\tLoss: 2.259629726409912\n",
            "Step [233/275]:\tLoss: 1.4990077018737793\n",
            "Step [234/275]:\tLoss: 2.0830910205841064\n",
            "Step [235/275]:\tLoss: 1.7384891510009766\n",
            "Step [236/275]:\tLoss: 0.9956427812576294\n",
            "Step [237/275]:\tLoss: 1.4360706806182861\n",
            "Step [238/275]:\tLoss: 1.2746315002441406\n",
            "Step [239/275]:\tLoss: 1.0633972883224487\n",
            "Step [240/275]:\tLoss: 1.5268338918685913\n",
            "Step [241/275]:\tLoss: 2.1733181476593018\n",
            "Step [242/275]:\tLoss: 0.9663318395614624\n",
            "Step [243/275]:\tLoss: 1.4827311038970947\n",
            "Step [244/275]:\tLoss: 1.3286452293395996\n",
            "Step [245/275]:\tLoss: 2.1323928833007812\n",
            "Step [246/275]:\tLoss: 1.2025673389434814\n",
            "Step [247/275]:\tLoss: 1.2206904888153076\n",
            "Step [248/275]:\tLoss: 1.382938027381897\n",
            "Step [249/275]:\tLoss: 1.163827657699585\n",
            "Step [250/275]:\tLoss: 1.2566568851470947\n",
            "Step [251/275]:\tLoss: 2.9587340354919434\n",
            "Step [252/275]:\tLoss: 0.7757729887962341\n",
            "Step [253/275]:\tLoss: 2.3384363651275635\n",
            "Step [254/275]:\tLoss: 2.260948657989502\n",
            "Step [255/275]:\tLoss: 1.3156269788742065\n",
            "Step [256/275]:\tLoss: 2.1719207763671875\n",
            "Step [257/275]:\tLoss: 2.431567668914795\n",
            "Step [258/275]:\tLoss: 1.6220955848693848\n",
            "Step [259/275]:\tLoss: 1.69480562210083\n",
            "Step [260/275]:\tLoss: 1.6353166103363037\n",
            "Step [261/275]:\tLoss: 1.4159979820251465\n",
            "Step [262/275]:\tLoss: 1.6376864910125732\n",
            "Step [263/275]:\tLoss: 1.17721426486969\n",
            "Step [264/275]:\tLoss: 1.570953369140625\n",
            "Step [265/275]:\tLoss: 1.4264159202575684\n",
            "Step [266/275]:\tLoss: 2.9319047927856445\n",
            "Step [267/275]:\tLoss: 1.2339961528778076\n",
            "Step [268/275]:\tLoss: 1.5358216762542725\n",
            "Step [269/275]:\tLoss: 2.8760461807250977\n",
            "Step [270/275]:\tLoss: 2.5174643993377686\n",
            "Step [271/275]:\tLoss: 1.9413808584213257\n",
            "Step [272/275]:\tLoss: 0.49948936700820923\n",
            "Step [273/275]:\tLoss: 1.4642841815948486\n",
            "Step [274/275]:\tLoss: 2.365628719329834\n",
            "epoch number 109\n",
            "Step [0/275]:\tLoss: 4.144731521606445\n",
            "Step [1/275]:\tLoss: 1.5692732334136963\n",
            "Step [2/275]:\tLoss: 1.5357160568237305\n",
            "Step [3/275]:\tLoss: 0.9752715229988098\n",
            "Step [4/275]:\tLoss: 1.3952784538269043\n",
            "Step [5/275]:\tLoss: 2.208455801010132\n",
            "Step [6/275]:\tLoss: 0.6123824119567871\n",
            "Step [7/275]:\tLoss: 0.6490538716316223\n",
            "Step [8/275]:\tLoss: 2.2804343700408936\n",
            "Step [9/275]:\tLoss: 3.2101659774780273\n",
            "Step [10/275]:\tLoss: 0.6175442934036255\n",
            "Step [11/275]:\tLoss: 1.9626132249832153\n",
            "Step [12/275]:\tLoss: 0.6452103853225708\n",
            "Step [13/275]:\tLoss: 1.4226452112197876\n",
            "Step [14/275]:\tLoss: 0.6591083407402039\n",
            "Step [15/275]:\tLoss: 1.159055471420288\n",
            "Step [16/275]:\tLoss: 0.452480286359787\n",
            "Step [17/275]:\tLoss: 3.083782196044922\n",
            "Step [18/275]:\tLoss: 0.26245054602622986\n",
            "Step [19/275]:\tLoss: 1.8573577404022217\n",
            "Step [20/275]:\tLoss: 0.5542339086532593\n",
            "Step [21/275]:\tLoss: 0.9169523119926453\n",
            "Step [22/275]:\tLoss: 1.2569925785064697\n",
            "Step [23/275]:\tLoss: 2.1301188468933105\n",
            "Step [24/275]:\tLoss: 0.6185351610183716\n",
            "Step [25/275]:\tLoss: 2.2167136669158936\n",
            "Step [26/275]:\tLoss: 0.9458132982254028\n",
            "Step [27/275]:\tLoss: 0.290652871131897\n",
            "Step [28/275]:\tLoss: 1.2565850019454956\n",
            "Step [29/275]:\tLoss: 0.23245292901992798\n",
            "Step [30/275]:\tLoss: 0.5499764680862427\n",
            "Step [31/275]:\tLoss: 0.8151379823684692\n",
            "Step [32/275]:\tLoss: 0.6624558568000793\n",
            "Step [33/275]:\tLoss: 1.1988482475280762\n",
            "Step [34/275]:\tLoss: 0.6020424962043762\n",
            "Step [35/275]:\tLoss: 0.2313908338546753\n",
            "Step [36/275]:\tLoss: 0.22509503364562988\n",
            "Step [37/275]:\tLoss: 0.5680263042449951\n",
            "Step [38/275]:\tLoss: 0.2735616862773895\n",
            "Step [39/275]:\tLoss: 0.564173698425293\n",
            "Step [40/275]:\tLoss: 0.22920477390289307\n",
            "Step [41/275]:\tLoss: 0.19878539443016052\n",
            "Step [42/275]:\tLoss: 0.598112940788269\n",
            "Step [43/275]:\tLoss: 0.6440210342407227\n",
            "Step [44/275]:\tLoss: 0.193916454911232\n",
            "Step [45/275]:\tLoss: 0.6071768999099731\n",
            "Step [46/275]:\tLoss: 1.0150343179702759\n",
            "Step [47/275]:\tLoss: 0.6119471192359924\n",
            "Step [48/275]:\tLoss: 0.6109500527381897\n",
            "Step [49/275]:\tLoss: 0.5612304210662842\n",
            "Step [50/275]:\tLoss: 0.19234958291053772\n",
            "Step [51/275]:\tLoss: 0.1829906702041626\n",
            "Step [52/275]:\tLoss: 0.5658172369003296\n",
            "Step [53/275]:\tLoss: 0.17123912274837494\n",
            "Step [54/275]:\tLoss: 0.6082776784896851\n",
            "Step [55/275]:\tLoss: 0.17511054873466492\n",
            "Step [56/275]:\tLoss: 0.47492289543151855\n",
            "Step [57/275]:\tLoss: 0.5699267387390137\n",
            "Step [58/275]:\tLoss: 0.5864730477333069\n",
            "Step [59/275]:\tLoss: 0.24633242189884186\n",
            "Step [60/275]:\tLoss: 0.18149255216121674\n",
            "Step [61/275]:\tLoss: 0.5773259401321411\n",
            "Step [62/275]:\tLoss: 0.7605689167976379\n",
            "Step [63/275]:\tLoss: 0.6197220087051392\n",
            "Step [64/275]:\tLoss: 0.1693824678659439\n",
            "Step [65/275]:\tLoss: 0.7608888149261475\n",
            "Step [66/275]:\tLoss: 0.15475010871887207\n",
            "Step [67/275]:\tLoss: 0.6573761701583862\n",
            "Step [68/275]:\tLoss: 0.5884993076324463\n",
            "Step [69/275]:\tLoss: 0.5768156051635742\n",
            "Step [70/275]:\tLoss: 0.6317052841186523\n",
            "Step [71/275]:\tLoss: 0.3435133099555969\n",
            "Step [72/275]:\tLoss: 0.5569113492965698\n",
            "Step [73/275]:\tLoss: 0.5693709850311279\n",
            "Step [74/275]:\tLoss: 0.5771729946136475\n",
            "Step [75/275]:\tLoss: 0.5889939069747925\n",
            "Step [76/275]:\tLoss: 0.5652008056640625\n",
            "Step [77/275]:\tLoss: 0.5761621594429016\n",
            "Step [78/275]:\tLoss: 0.5672887563705444\n",
            "Step [79/275]:\tLoss: 0.548302948474884\n",
            "Step [80/275]:\tLoss: 0.6270654201507568\n",
            "Step [81/275]:\tLoss: 0.7649950981140137\n",
            "Step [82/275]:\tLoss: 0.3699657618999481\n",
            "Step [83/275]:\tLoss: 0.5533850193023682\n",
            "Step [84/275]:\tLoss: 0.5505871176719666\n",
            "Step [85/275]:\tLoss: 0.5699887275695801\n",
            "Step [86/275]:\tLoss: 0.5644537210464478\n",
            "Step [87/275]:\tLoss: 0.7488534450531006\n",
            "Step [88/275]:\tLoss: 0.5801746845245361\n",
            "Step [89/275]:\tLoss: 0.5513380765914917\n",
            "Step [90/275]:\tLoss: 0.5611708760261536\n",
            "Step [91/275]:\tLoss: 0.7404268383979797\n",
            "Step [92/275]:\tLoss: 0.5705812573432922\n",
            "Step [93/275]:\tLoss: 0.6577355861663818\n",
            "Step [94/275]:\tLoss: 0.6117546558380127\n",
            "Step [95/275]:\tLoss: 0.5378575325012207\n",
            "Step [96/275]:\tLoss: 0.9507522583007812\n",
            "Step [97/275]:\tLoss: 0.516560971736908\n",
            "Step [98/275]:\tLoss: 0.7593157887458801\n",
            "Step [99/275]:\tLoss: 0.18677833676338196\n",
            "Step [100/275]:\tLoss: 0.18905699253082275\n",
            "Step [101/275]:\tLoss: 0.6570942401885986\n",
            "Step [102/275]:\tLoss: 0.5847277045249939\n",
            "Step [103/275]:\tLoss: 0.32092201709747314\n",
            "Step [104/275]:\tLoss: 0.16546416282653809\n",
            "Step [105/275]:\tLoss: 0.16998496651649475\n",
            "Step [106/275]:\tLoss: 0.7741072773933411\n",
            "Step [107/275]:\tLoss: 0.5624244213104248\n",
            "Step [108/275]:\tLoss: 0.16992220282554626\n",
            "Step [109/275]:\tLoss: 0.605696439743042\n",
            "Step [110/275]:\tLoss: 2.869537353515625\n",
            "Step [111/275]:\tLoss: 2.3074231147766113\n",
            "Step [112/275]:\tLoss: 1.9203312397003174\n",
            "Step [113/275]:\tLoss: 0.7551503777503967\n",
            "Step [114/275]:\tLoss: 0.4476945102214813\n",
            "Step [115/275]:\tLoss: 1.8205218315124512\n",
            "Step [116/275]:\tLoss: 2.1742258071899414\n",
            "Step [117/275]:\tLoss: 2.9166626930236816\n",
            "Step [118/275]:\tLoss: 0.5845150947570801\n",
            "Step [119/275]:\tLoss: 0.5544114112854004\n",
            "Step [120/275]:\tLoss: 1.2879630327224731\n",
            "Step [121/275]:\tLoss: 0.5849528312683105\n",
            "Step [122/275]:\tLoss: 0.9492000937461853\n",
            "Step [123/275]:\tLoss: 0.7153187394142151\n",
            "Step [124/275]:\tLoss: 1.6804914474487305\n",
            "Step [125/275]:\tLoss: 0.5772719979286194\n",
            "Step [126/275]:\tLoss: 2.920841693878174\n",
            "Step [127/275]:\tLoss: 0.6503856182098389\n",
            "Step [128/275]:\tLoss: 2.2257771492004395\n",
            "Step [129/275]:\tLoss: 0.8807991743087769\n",
            "Step [130/275]:\tLoss: 2.5679643154144287\n",
            "Step [131/275]:\tLoss: 3.089047431945801\n",
            "Step [132/275]:\tLoss: 0.7273613214492798\n",
            "Step [133/275]:\tLoss: 1.3274292945861816\n",
            "Step [134/275]:\tLoss: 2.2551190853118896\n",
            "Step [135/275]:\tLoss: 0.5318847894668579\n",
            "Step [136/275]:\tLoss: 2.725752353668213\n",
            "Step [137/275]:\tLoss: 0.5759890675544739\n",
            "Step [138/275]:\tLoss: 0.2556065320968628\n",
            "Step [139/275]:\tLoss: 0.5599552392959595\n",
            "Step [140/275]:\tLoss: 0.6388713717460632\n",
            "Step [141/275]:\tLoss: 0.5819213390350342\n",
            "Step [142/275]:\tLoss: 0.5524387359619141\n",
            "Step [143/275]:\tLoss: 0.2567538619041443\n",
            "Step [144/275]:\tLoss: 0.6824080944061279\n",
            "Step [145/275]:\tLoss: 0.5853487253189087\n",
            "Step [146/275]:\tLoss: 0.4509218633174896\n",
            "Step [147/275]:\tLoss: 0.23161211609840393\n",
            "Step [148/275]:\tLoss: 0.4772685170173645\n",
            "Step [149/275]:\tLoss: 0.535765528678894\n",
            "Step [150/275]:\tLoss: 0.5875003933906555\n",
            "Step [151/275]:\tLoss: 0.7611159086227417\n",
            "Step [152/275]:\tLoss: 0.6972657442092896\n",
            "Step [153/275]:\tLoss: 0.23103761672973633\n",
            "Step [154/275]:\tLoss: 0.6072225570678711\n",
            "Step [155/275]:\tLoss: 0.6000896692276001\n",
            "Step [156/275]:\tLoss: 0.657901406288147\n",
            "Step [157/275]:\tLoss: 0.20255304872989655\n",
            "Step [158/275]:\tLoss: 0.5280926823616028\n",
            "Step [159/275]:\tLoss: 0.4371527135372162\n",
            "Step [160/275]:\tLoss: 0.5900036096572876\n",
            "Step [161/275]:\tLoss: 0.7842833995819092\n",
            "Step [162/275]:\tLoss: 0.5844160914421082\n",
            "Step [163/275]:\tLoss: 0.7099618911743164\n",
            "Step [164/275]:\tLoss: 0.6668269634246826\n",
            "Step [165/275]:\tLoss: 0.4596528112888336\n",
            "Step [166/275]:\tLoss: 0.58392333984375\n",
            "Step [167/275]:\tLoss: 0.4146876931190491\n",
            "Step [168/275]:\tLoss: 0.28195619583129883\n",
            "Step [169/275]:\tLoss: 0.517103910446167\n",
            "Step [170/275]:\tLoss: 0.8327321410179138\n",
            "Step [171/275]:\tLoss: 0.5979903936386108\n",
            "Step [172/275]:\tLoss: 0.5233714580535889\n",
            "Step [173/275]:\tLoss: 0.5131231546401978\n",
            "Step [174/275]:\tLoss: 0.5000130534172058\n",
            "Step [175/275]:\tLoss: 0.5675891041755676\n",
            "Step [176/275]:\tLoss: 0.6812936067581177\n",
            "Step [177/275]:\tLoss: 0.5613406300544739\n",
            "Step [178/275]:\tLoss: 0.7387737035751343\n",
            "Step [179/275]:\tLoss: 0.6535841226577759\n",
            "Step [180/275]:\tLoss: 0.502181887626648\n",
            "Step [181/275]:\tLoss: 0.5646232962608337\n",
            "Step [182/275]:\tLoss: 0.6171671152114868\n",
            "Step [183/275]:\tLoss: 0.5980935096740723\n",
            "Step [184/275]:\tLoss: 0.5448527336120605\n",
            "Step [185/275]:\tLoss: 0.5472373962402344\n",
            "Step [186/275]:\tLoss: 0.5318894386291504\n",
            "Step [187/275]:\tLoss: 0.5346346497535706\n",
            "Step [188/275]:\tLoss: 0.41151124238967896\n",
            "Step [189/275]:\tLoss: 0.7258960008621216\n",
            "Step [190/275]:\tLoss: 0.7160136699676514\n",
            "Step [191/275]:\tLoss: 0.5882811546325684\n",
            "Step [192/275]:\tLoss: 0.48446333408355713\n",
            "Step [193/275]:\tLoss: 1.8775324821472168\n",
            "Step [194/275]:\tLoss: 2.521531105041504\n",
            "Step [195/275]:\tLoss: 2.543853282928467\n",
            "Step [196/275]:\tLoss: 2.044205904006958\n",
            "Step [197/275]:\tLoss: 2.013542413711548\n",
            "Step [198/275]:\tLoss: 1.305999755859375\n",
            "Step [199/275]:\tLoss: 1.236961007118225\n",
            "Step [200/275]:\tLoss: 1.4230196475982666\n",
            "Step [201/275]:\tLoss: 3.0961642265319824\n",
            "Step [202/275]:\tLoss: 1.2010998725891113\n",
            "Step [203/275]:\tLoss: 1.8427072763442993\n",
            "Step [204/275]:\tLoss: 2.172179937362671\n",
            "Step [205/275]:\tLoss: 1.272237777709961\n",
            "Step [206/275]:\tLoss: 1.2325247526168823\n",
            "Step [207/275]:\tLoss: 1.6560686826705933\n",
            "Step [208/275]:\tLoss: 1.4593982696533203\n",
            "Step [209/275]:\tLoss: 1.1454559564590454\n",
            "Step [210/275]:\tLoss: 1.2303768396377563\n",
            "Step [211/275]:\tLoss: 2.415527820587158\n",
            "Step [212/275]:\tLoss: 1.6092697381973267\n",
            "Step [213/275]:\tLoss: 1.643073558807373\n",
            "Step [214/275]:\tLoss: 1.4295073747634888\n",
            "Step [215/275]:\tLoss: 1.13139009475708\n",
            "Step [216/275]:\tLoss: 1.4499049186706543\n",
            "Step [217/275]:\tLoss: 1.6550023555755615\n",
            "Step [218/275]:\tLoss: 1.3994605541229248\n",
            "Step [219/275]:\tLoss: 1.9519386291503906\n",
            "Step [220/275]:\tLoss: 1.9464547634124756\n",
            "Step [221/275]:\tLoss: 1.314197301864624\n",
            "Step [222/275]:\tLoss: 0.8587491512298584\n",
            "Step [223/275]:\tLoss: 2.078404188156128\n",
            "Step [224/275]:\tLoss: 1.8058663606643677\n",
            "Step [225/275]:\tLoss: 1.9015169143676758\n",
            "Step [226/275]:\tLoss: 1.5410845279693604\n",
            "Step [227/275]:\tLoss: 1.5084435939788818\n",
            "Step [228/275]:\tLoss: 1.5817978382110596\n",
            "Step [229/275]:\tLoss: 1.0697689056396484\n",
            "Step [230/275]:\tLoss: 1.4470441341400146\n",
            "Step [231/275]:\tLoss: 1.5811264514923096\n",
            "Step [232/275]:\tLoss: 1.4511511325836182\n",
            "Step [233/275]:\tLoss: 0.9291055798530579\n",
            "Step [234/275]:\tLoss: 1.233078956604004\n",
            "Step [235/275]:\tLoss: 1.3238364458084106\n",
            "Step [236/275]:\tLoss: 1.2246028184890747\n",
            "Step [237/275]:\tLoss: 1.49208664894104\n",
            "Step [238/275]:\tLoss: 1.1463292837142944\n",
            "Step [239/275]:\tLoss: 0.7198912501335144\n",
            "Step [240/275]:\tLoss: 1.2272298336029053\n",
            "Step [241/275]:\tLoss: 1.2947282791137695\n",
            "Step [242/275]:\tLoss: 2.3525259494781494\n",
            "Step [243/275]:\tLoss: 1.4016121625900269\n",
            "Step [244/275]:\tLoss: 1.3843835592269897\n",
            "Step [245/275]:\tLoss: 2.0746960639953613\n",
            "Step [246/275]:\tLoss: 1.1240732669830322\n",
            "Step [247/275]:\tLoss: 1.546613097190857\n",
            "Step [248/275]:\tLoss: 0.9311413168907166\n",
            "Step [249/275]:\tLoss: 1.607344150543213\n",
            "Step [250/275]:\tLoss: 2.310512065887451\n",
            "Step [251/275]:\tLoss: 3.0166940689086914\n",
            "Step [252/275]:\tLoss: 1.1563334465026855\n",
            "Step [253/275]:\tLoss: 0.7917091250419617\n",
            "Step [254/275]:\tLoss: 0.5009205341339111\n",
            "Step [255/275]:\tLoss: 2.6805953979492188\n",
            "Step [256/275]:\tLoss: 0.4988245368003845\n",
            "Step [257/275]:\tLoss: 1.6943855285644531\n",
            "Step [258/275]:\tLoss: 2.7132344245910645\n",
            "Step [259/275]:\tLoss: 1.626176357269287\n",
            "Step [260/275]:\tLoss: 1.0772088766098022\n",
            "Step [261/275]:\tLoss: 2.0074524879455566\n",
            "Step [262/275]:\tLoss: 1.084256649017334\n",
            "Step [263/275]:\tLoss: 1.0158913135528564\n",
            "Step [264/275]:\tLoss: 1.7714234590530396\n",
            "Step [265/275]:\tLoss: 1.0682172775268555\n",
            "Step [266/275]:\tLoss: 1.9781150817871094\n",
            "Step [267/275]:\tLoss: 1.86952543258667\n",
            "Step [268/275]:\tLoss: 1.003840684890747\n",
            "Step [269/275]:\tLoss: 3.382449150085449\n",
            "Step [270/275]:\tLoss: 1.1746329069137573\n",
            "Step [271/275]:\tLoss: 1.3964300155639648\n",
            "Step [272/275]:\tLoss: 2.001969814300537\n",
            "Step [273/275]:\tLoss: 1.611085295677185\n",
            "Step [274/275]:\tLoss: 1.2889654636383057\n",
            "epoch number 110\n",
            "Step [0/275]:\tLoss: 0.4502362012863159\n",
            "Step [1/275]:\tLoss: 0.7466717958450317\n",
            "Step [2/275]:\tLoss: 2.109376907348633\n",
            "Step [3/275]:\tLoss: 0.6700299978256226\n",
            "Step [4/275]:\tLoss: 1.337112307548523\n",
            "Step [5/275]:\tLoss: 0.6091232895851135\n",
            "Step [6/275]:\tLoss: 0.9437459707260132\n",
            "Step [7/275]:\tLoss: 1.0215564966201782\n",
            "Step [8/275]:\tLoss: 2.4836316108703613\n",
            "Step [9/275]:\tLoss: 4.012082576751709\n",
            "Step [10/275]:\tLoss: 0.589483380317688\n",
            "Step [11/275]:\tLoss: 0.40839463472366333\n",
            "Step [12/275]:\tLoss: 0.5583087801933289\n",
            "Step [13/275]:\tLoss: 0.648182213306427\n",
            "Step [14/275]:\tLoss: 0.5515353679656982\n",
            "Step [15/275]:\tLoss: 1.1927622556686401\n",
            "Step [16/275]:\tLoss: 1.1473824977874756\n",
            "Step [17/275]:\tLoss: 0.6254100203514099\n",
            "Step [18/275]:\tLoss: 0.5781643390655518\n",
            "Step [19/275]:\tLoss: 0.6865164637565613\n",
            "Step [20/275]:\tLoss: 0.6003053188323975\n",
            "Step [21/275]:\tLoss: 0.46421951055526733\n",
            "Step [22/275]:\tLoss: 0.6638184785842896\n",
            "Step [23/275]:\tLoss: 2.8000004291534424\n",
            "Step [24/275]:\tLoss: 0.5302683115005493\n",
            "Step [25/275]:\tLoss: 1.1688520908355713\n",
            "Step [26/275]:\tLoss: 0.7101219296455383\n",
            "Step [27/275]:\tLoss: 0.5671652555465698\n",
            "Step [28/275]:\tLoss: 2.2261743545532227\n",
            "Step [29/275]:\tLoss: 0.5728179216384888\n",
            "Step [30/275]:\tLoss: 0.30967217683792114\n",
            "Step [31/275]:\tLoss: 0.8114433288574219\n",
            "Step [32/275]:\tLoss: 0.6875311732292175\n",
            "Step [33/275]:\tLoss: 0.5629434585571289\n",
            "Step [34/275]:\tLoss: 0.7484611868858337\n",
            "Step [35/275]:\tLoss: 0.26226019859313965\n",
            "Step [36/275]:\tLoss: 0.29125961661338806\n",
            "Step [37/275]:\tLoss: 0.5420733690261841\n",
            "Step [38/275]:\tLoss: 0.731626033782959\n",
            "Step [39/275]:\tLoss: 0.7014894485473633\n",
            "Step [40/275]:\tLoss: 0.8088062405586243\n",
            "Step [41/275]:\tLoss: 0.5751512050628662\n",
            "Step [42/275]:\tLoss: 0.5269324779510498\n",
            "Step [43/275]:\tLoss: 0.3561340868473053\n",
            "Step [44/275]:\tLoss: 0.5524621605873108\n",
            "Step [45/275]:\tLoss: 0.4393632113933563\n",
            "Step [46/275]:\tLoss: 0.19991838932037354\n",
            "Step [47/275]:\tLoss: 0.512881338596344\n",
            "Step [48/275]:\tLoss: 0.5257821083068848\n",
            "Step [49/275]:\tLoss: 0.6879374980926514\n",
            "Step [50/275]:\tLoss: 0.7580184936523438\n",
            "Step [51/275]:\tLoss: 0.5634862184524536\n",
            "Step [52/275]:\tLoss: 0.8350005149841309\n",
            "Step [53/275]:\tLoss: 0.36245986819267273\n",
            "Step [54/275]:\tLoss: 0.571342408657074\n",
            "Step [55/275]:\tLoss: 0.5053972601890564\n",
            "Step [56/275]:\tLoss: 0.7097018957138062\n",
            "Step [57/275]:\tLoss: 0.5457249283790588\n",
            "Step [58/275]:\tLoss: 0.5709964036941528\n",
            "Step [59/275]:\tLoss: 0.87309730052948\n",
            "Step [60/275]:\tLoss: 0.1961061954498291\n",
            "Step [61/275]:\tLoss: 0.7048839330673218\n",
            "Step [62/275]:\tLoss: 0.6702299118041992\n",
            "Step [63/275]:\tLoss: 0.19075803458690643\n",
            "Step [64/275]:\tLoss: 0.5445976853370667\n",
            "Step [65/275]:\tLoss: 0.7124677896499634\n",
            "Step [66/275]:\tLoss: 0.7411066293716431\n",
            "Step [67/275]:\tLoss: 0.5897729992866516\n",
            "Step [68/275]:\tLoss: 0.49203285574913025\n",
            "Step [69/275]:\tLoss: 0.18499350547790527\n",
            "Step [70/275]:\tLoss: 0.5217520594596863\n",
            "Step [71/275]:\tLoss: 0.5052562952041626\n",
            "Step [72/275]:\tLoss: 0.5204546451568604\n",
            "Step [73/275]:\tLoss: 0.48750728368759155\n",
            "Step [74/275]:\tLoss: 0.510697603225708\n",
            "Step [75/275]:\tLoss: 0.5279204845428467\n",
            "Step [76/275]:\tLoss: 0.4181356132030487\n",
            "Step [77/275]:\tLoss: 0.7663103342056274\n",
            "Step [78/275]:\tLoss: 0.202938050031662\n",
            "Step [79/275]:\tLoss: 0.514743447303772\n",
            "Step [80/275]:\tLoss: 0.7032603621482849\n",
            "Step [81/275]:\tLoss: 0.7990514039993286\n",
            "Step [82/275]:\tLoss: 0.31067901849746704\n",
            "Step [83/275]:\tLoss: 0.8451523780822754\n",
            "Step [84/275]:\tLoss: 0.39922863245010376\n",
            "Step [85/275]:\tLoss: 0.7331134080886841\n",
            "Step [86/275]:\tLoss: 0.5232118368148804\n",
            "Step [87/275]:\tLoss: 0.8730781078338623\n",
            "Step [88/275]:\tLoss: 0.5965079069137573\n",
            "Step [89/275]:\tLoss: 0.5865093469619751\n",
            "Step [90/275]:\tLoss: 0.5076130032539368\n",
            "Step [91/275]:\tLoss: 0.4976688325405121\n",
            "Step [92/275]:\tLoss: 0.5086581110954285\n",
            "Step [93/275]:\tLoss: 0.5496029853820801\n",
            "Step [94/275]:\tLoss: 0.45288047194480896\n",
            "Step [95/275]:\tLoss: 0.19509504735469818\n",
            "Step [96/275]:\tLoss: 0.48280373215675354\n",
            "Step [97/275]:\tLoss: 0.6405519247055054\n",
            "Step [98/275]:\tLoss: 0.510712742805481\n",
            "Step [99/275]:\tLoss: 0.57026207447052\n",
            "Step [100/275]:\tLoss: 0.6730542182922363\n",
            "Step [101/275]:\tLoss: 0.5409565567970276\n",
            "Step [102/275]:\tLoss: 0.4888821840286255\n",
            "Step [103/275]:\tLoss: 0.6577949523925781\n",
            "Step [104/275]:\tLoss: 0.5136977434158325\n",
            "Step [105/275]:\tLoss: 0.49536293745040894\n",
            "Step [106/275]:\tLoss: 0.5558497905731201\n",
            "Step [107/275]:\tLoss: 0.9057444334030151\n",
            "Step [108/275]:\tLoss: 0.3262478709220886\n",
            "Step [109/275]:\tLoss: 0.6715036630630493\n",
            "Step [110/275]:\tLoss: 1.552126407623291\n",
            "Step [111/275]:\tLoss: 1.3287523984909058\n",
            "Step [112/275]:\tLoss: 1.7680916786193848\n",
            "Step [113/275]:\tLoss: 0.23699665069580078\n",
            "Step [114/275]:\tLoss: 1.284395694732666\n",
            "Step [115/275]:\tLoss: 3.1838765144348145\n",
            "Step [116/275]:\tLoss: 2.130406379699707\n",
            "Step [117/275]:\tLoss: 0.5383082628250122\n",
            "Step [118/275]:\tLoss: 2.0106701850891113\n",
            "Step [119/275]:\tLoss: 0.5029494166374207\n",
            "Step [120/275]:\tLoss: 0.8018049597740173\n",
            "Step [121/275]:\tLoss: 0.49576425552368164\n",
            "Step [122/275]:\tLoss: 0.6196921467781067\n",
            "Step [123/275]:\tLoss: 1.322427749633789\n",
            "Step [124/275]:\tLoss: 1.961207628250122\n",
            "Step [125/275]:\tLoss: 1.8893914222717285\n",
            "Step [126/275]:\tLoss: 2.1125826835632324\n",
            "Step [127/275]:\tLoss: 1.3740458488464355\n",
            "Step [128/275]:\tLoss: 0.42768290638923645\n",
            "Step [129/275]:\tLoss: 2.2235732078552246\n",
            "Step [130/275]:\tLoss: 0.5753776431083679\n",
            "Step [131/275]:\tLoss: 2.4804999828338623\n",
            "Step [132/275]:\tLoss: 0.48564398288726807\n",
            "Step [133/275]:\tLoss: 1.3753234148025513\n",
            "Step [134/275]:\tLoss: 1.875597357749939\n",
            "Step [135/275]:\tLoss: 0.7384016513824463\n",
            "Step [136/275]:\tLoss: 1.9233217239379883\n",
            "Step [137/275]:\tLoss: 0.22162047028541565\n",
            "Step [138/275]:\tLoss: 0.4821808636188507\n",
            "Step [139/275]:\tLoss: 0.4850393235683441\n",
            "Step [140/275]:\tLoss: 0.2191450446844101\n",
            "Step [141/275]:\tLoss: 0.5101159811019897\n",
            "Step [142/275]:\tLoss: 0.4670085310935974\n",
            "Step [143/275]:\tLoss: 0.23083187639713287\n",
            "Step [144/275]:\tLoss: 0.7605326175689697\n",
            "Step [145/275]:\tLoss: 0.6453202962875366\n",
            "Step [146/275]:\tLoss: 0.5615200996398926\n",
            "Step [147/275]:\tLoss: 0.5082384943962097\n",
            "Step [148/275]:\tLoss: 0.6942574977874756\n",
            "Step [149/275]:\tLoss: 0.33984413743019104\n",
            "Step [150/275]:\tLoss: 0.25851085782051086\n",
            "Step [151/275]:\tLoss: 0.22974133491516113\n",
            "Step [152/275]:\tLoss: 0.746597409248352\n",
            "Step [153/275]:\tLoss: 0.48391106724739075\n",
            "Step [154/275]:\tLoss: 0.21175755560398102\n",
            "Step [155/275]:\tLoss: 0.4516122341156006\n",
            "Step [156/275]:\tLoss: 0.5018392205238342\n",
            "Step [157/275]:\tLoss: 0.6687142848968506\n",
            "Step [158/275]:\tLoss: 0.5501778721809387\n",
            "Step [159/275]:\tLoss: 0.7932022213935852\n",
            "Step [160/275]:\tLoss: 0.4174509644508362\n",
            "Step [161/275]:\tLoss: 0.5452775955200195\n",
            "Step [162/275]:\tLoss: 0.470637708902359\n",
            "Step [163/275]:\tLoss: 0.2140215039253235\n",
            "Step [164/275]:\tLoss: 0.7260072231292725\n",
            "Step [165/275]:\tLoss: 0.6805511116981506\n",
            "Step [166/275]:\tLoss: 0.6545379161834717\n",
            "Step [167/275]:\tLoss: 0.6343956589698792\n",
            "Step [168/275]:\tLoss: 0.49291089177131653\n",
            "Step [169/275]:\tLoss: 0.21380497515201569\n",
            "Step [170/275]:\tLoss: 0.6988469362258911\n",
            "Step [171/275]:\tLoss: 0.6859006881713867\n",
            "Step [172/275]:\tLoss: 0.4691520929336548\n",
            "Step [173/275]:\tLoss: 0.49314239621162415\n",
            "Step [174/275]:\tLoss: 0.6866536140441895\n",
            "Step [175/275]:\tLoss: 1.1991808414459229\n",
            "Step [176/275]:\tLoss: 0.2622950077056885\n",
            "Step [177/275]:\tLoss: 0.46732693910598755\n",
            "Step [178/275]:\tLoss: 0.7331900596618652\n",
            "Step [179/275]:\tLoss: 0.23548606038093567\n",
            "Step [180/275]:\tLoss: 0.47189387679100037\n",
            "Step [181/275]:\tLoss: 0.4785577058792114\n",
            "Step [182/275]:\tLoss: 0.4558148980140686\n",
            "Step [183/275]:\tLoss: 0.4797104597091675\n",
            "Step [184/275]:\tLoss: 0.4876035153865814\n",
            "Step [185/275]:\tLoss: 0.47673943638801575\n",
            "Step [186/275]:\tLoss: 0.4863224923610687\n",
            "Step [187/275]:\tLoss: 0.5983594655990601\n",
            "Step [188/275]:\tLoss: 0.6303713321685791\n",
            "Step [189/275]:\tLoss: 0.9260063171386719\n",
            "Step [190/275]:\tLoss: 0.40718939900398254\n",
            "Step [191/275]:\tLoss: 0.9010415077209473\n",
            "Step [192/275]:\tLoss: 1.1840565204620361\n",
            "Step [193/275]:\tLoss: 2.2210521697998047\n",
            "Step [194/275]:\tLoss: 1.8456072807312012\n",
            "Step [195/275]:\tLoss: 2.142324447631836\n",
            "Step [196/275]:\tLoss: 2.093885898590088\n",
            "Step [197/275]:\tLoss: 1.960108995437622\n",
            "Step [198/275]:\tLoss: 1.568184494972229\n",
            "Step [199/275]:\tLoss: 1.3380656242370605\n",
            "Step [200/275]:\tLoss: 2.1105661392211914\n",
            "Step [201/275]:\tLoss: 1.7939732074737549\n",
            "Step [202/275]:\tLoss: 2.3203022480010986\n",
            "Step [203/275]:\tLoss: 1.3043946027755737\n",
            "Step [204/275]:\tLoss: 1.4406850337982178\n",
            "Step [205/275]:\tLoss: 1.2403912544250488\n",
            "Step [206/275]:\tLoss: 2.155376434326172\n",
            "Step [207/275]:\tLoss: 2.460200548171997\n",
            "Step [208/275]:\tLoss: 1.201425552368164\n",
            "Step [209/275]:\tLoss: 1.3479013442993164\n",
            "Step [210/275]:\tLoss: 1.472894310951233\n",
            "Step [211/275]:\tLoss: 2.3389012813568115\n",
            "Step [212/275]:\tLoss: 0.9872753620147705\n",
            "Step [213/275]:\tLoss: 1.3539464473724365\n",
            "Step [214/275]:\tLoss: 1.612962007522583\n",
            "Step [215/275]:\tLoss: 1.5493240356445312\n",
            "Step [216/275]:\tLoss: 1.5874229669570923\n",
            "Step [217/275]:\tLoss: 1.5822871923446655\n",
            "Step [218/275]:\tLoss: 1.315449833869934\n",
            "Step [219/275]:\tLoss: 3.544790267944336\n",
            "Step [220/275]:\tLoss: 1.8903765678405762\n",
            "Step [221/275]:\tLoss: 0.9906383752822876\n",
            "Step [222/275]:\tLoss: 1.0866084098815918\n",
            "Step [223/275]:\tLoss: 1.7642792463302612\n",
            "Step [224/275]:\tLoss: 0.8184704184532166\n",
            "Step [225/275]:\tLoss: 1.4441035985946655\n",
            "Step [226/275]:\tLoss: 1.1401649713516235\n",
            "Step [227/275]:\tLoss: 1.029737949371338\n",
            "Step [228/275]:\tLoss: 1.550866961479187\n",
            "Step [229/275]:\tLoss: 1.460432767868042\n",
            "Step [230/275]:\tLoss: 1.1682924032211304\n",
            "Step [231/275]:\tLoss: 1.618696689605713\n",
            "Step [232/275]:\tLoss: 1.5361398458480835\n",
            "Step [233/275]:\tLoss: 1.3316669464111328\n",
            "Step [234/275]:\tLoss: 2.08591890335083\n",
            "Step [235/275]:\tLoss: 1.6937291622161865\n",
            "Step [236/275]:\tLoss: 1.5667511224746704\n",
            "Step [237/275]:\tLoss: 0.8784934282302856\n",
            "Step [238/275]:\tLoss: 1.9605588912963867\n",
            "Step [239/275]:\tLoss: 1.370124101638794\n",
            "Step [240/275]:\tLoss: 1.5511302947998047\n",
            "Step [241/275]:\tLoss: 1.315110683441162\n",
            "Step [242/275]:\tLoss: 2.064568519592285\n",
            "Step [243/275]:\tLoss: 0.6127744913101196\n",
            "Step [244/275]:\tLoss: 2.0607006549835205\n",
            "Step [245/275]:\tLoss: 1.5835678577423096\n",
            "Step [246/275]:\tLoss: 1.4188987016677856\n",
            "Step [247/275]:\tLoss: 2.2266783714294434\n",
            "Step [248/275]:\tLoss: 1.5397906303405762\n",
            "Step [249/275]:\tLoss: 2.2851295471191406\n",
            "Step [250/275]:\tLoss: 1.3091100454330444\n",
            "Step [251/275]:\tLoss: 1.7046005725860596\n",
            "Step [252/275]:\tLoss: 1.501296043395996\n",
            "Step [253/275]:\tLoss: 2.645674467086792\n",
            "Step [254/275]:\tLoss: 1.2174259424209595\n",
            "Step [255/275]:\tLoss: 2.365844249725342\n",
            "Step [256/275]:\tLoss: 0.7912720441818237\n",
            "Step [257/275]:\tLoss: 1.1212387084960938\n",
            "Step [258/275]:\tLoss: 1.1094882488250732\n",
            "Step [259/275]:\tLoss: 1.5800226926803589\n",
            "Step [260/275]:\tLoss: 1.6607186794281006\n",
            "Step [261/275]:\tLoss: 2.264971971511841\n",
            "Step [262/275]:\tLoss: 0.782278299331665\n",
            "Step [263/275]:\tLoss: 1.3532321453094482\n",
            "Step [264/275]:\tLoss: 1.5376708507537842\n",
            "Step [265/275]:\tLoss: 1.2817387580871582\n",
            "Step [266/275]:\tLoss: 1.8718793392181396\n",
            "Step [267/275]:\tLoss: 1.7625782489776611\n",
            "Step [268/275]:\tLoss: 0.9875204563140869\n",
            "Step [269/275]:\tLoss: 1.692710518836975\n",
            "Step [270/275]:\tLoss: 1.1745685338974\n",
            "Step [271/275]:\tLoss: 2.101844310760498\n",
            "Step [272/275]:\tLoss: 0.9802749156951904\n",
            "Step [273/275]:\tLoss: 1.1771957874298096\n",
            "Step [274/275]:\tLoss: 1.9140766859054565\n",
            "epoch number 111\n",
            "Step [0/275]:\tLoss: 0.9757227897644043\n",
            "Step [1/275]:\tLoss: 1.1483612060546875\n",
            "Step [2/275]:\tLoss: 0.9333248138427734\n",
            "Step [3/275]:\tLoss: 1.0280429124832153\n",
            "Step [4/275]:\tLoss: 0.7633659243583679\n",
            "Step [5/275]:\tLoss: 2.9330129623413086\n",
            "Step [6/275]:\tLoss: 1.0053865909576416\n",
            "Step [7/275]:\tLoss: 0.6996713876724243\n",
            "Step [8/275]:\tLoss: 1.9865384101867676\n",
            "Step [9/275]:\tLoss: 1.1109685897827148\n",
            "Step [10/275]:\tLoss: 0.7432130575180054\n",
            "Step [11/275]:\tLoss: 0.5084997415542603\n",
            "Step [12/275]:\tLoss: 0.5884286761283875\n",
            "Step [13/275]:\tLoss: 0.7283756732940674\n",
            "Step [14/275]:\tLoss: 1.2363417148590088\n",
            "Step [15/275]:\tLoss: 1.2783420085906982\n",
            "Step [16/275]:\tLoss: 0.9052565097808838\n",
            "Step [17/275]:\tLoss: 0.44724327325820923\n",
            "Step [18/275]:\tLoss: 0.8425186276435852\n",
            "Step [19/275]:\tLoss: 1.06886887550354\n",
            "Step [20/275]:\tLoss: 0.788202166557312\n",
            "Step [21/275]:\tLoss: 1.1158829927444458\n",
            "Step [22/275]:\tLoss: 0.9339445233345032\n",
            "Step [23/275]:\tLoss: 1.953426718711853\n",
            "Step [24/275]:\tLoss: 0.32642146944999695\n",
            "Step [25/275]:\tLoss: 2.304659843444824\n",
            "Step [26/275]:\tLoss: 0.5927336812019348\n",
            "Step [27/275]:\tLoss: 0.7864411473274231\n",
            "Step [28/275]:\tLoss: 1.402963638305664\n",
            "Step [29/275]:\tLoss: 0.5963283777236938\n",
            "Step [30/275]:\tLoss: 0.5250498056411743\n",
            "Step [31/275]:\tLoss: 0.23793958127498627\n",
            "Step [32/275]:\tLoss: 0.522232174873352\n",
            "Step [33/275]:\tLoss: 0.8650506734848022\n",
            "Step [34/275]:\tLoss: 0.5623661279678345\n",
            "Step [35/275]:\tLoss: 0.572035014629364\n",
            "Step [36/275]:\tLoss: 0.8071684837341309\n",
            "Step [37/275]:\tLoss: 0.5357620120048523\n",
            "Step [38/275]:\tLoss: 0.6327478885650635\n",
            "Step [39/275]:\tLoss: 0.7078176140785217\n",
            "Step [40/275]:\tLoss: 0.7436802983283997\n",
            "Step [41/275]:\tLoss: 0.6683743596076965\n",
            "Step [42/275]:\tLoss: 0.9724392294883728\n",
            "Step [43/275]:\tLoss: 0.4343471825122833\n",
            "Step [44/275]:\tLoss: 0.6230058670043945\n",
            "Step [45/275]:\tLoss: 0.2383025586605072\n",
            "Step [46/275]:\tLoss: 0.5502771735191345\n",
            "Step [47/275]:\tLoss: 0.29870837926864624\n",
            "Step [48/275]:\tLoss: 0.7726055383682251\n",
            "Step [49/275]:\tLoss: 0.665274977684021\n",
            "Step [50/275]:\tLoss: 0.23536349833011627\n",
            "Step [51/275]:\tLoss: 0.5824337601661682\n",
            "Step [52/275]:\tLoss: 0.7243554592132568\n",
            "Step [53/275]:\tLoss: 0.5400134921073914\n",
            "Step [54/275]:\tLoss: 0.5378456115722656\n",
            "Step [55/275]:\tLoss: 0.6007200479507446\n",
            "Step [56/275]:\tLoss: 0.9314565062522888\n",
            "Step [57/275]:\tLoss: 0.541137158870697\n",
            "Step [58/275]:\tLoss: 0.20036660134792328\n",
            "Step [59/275]:\tLoss: 0.527766227722168\n",
            "Step [60/275]:\tLoss: 0.4591309726238251\n",
            "Step [61/275]:\tLoss: 0.4445597529411316\n",
            "Step [62/275]:\tLoss: 0.558505654335022\n",
            "Step [63/275]:\tLoss: 0.8190239071846008\n",
            "Step [64/275]:\tLoss: 0.6915415525436401\n",
            "Step [65/275]:\tLoss: 0.5068563222885132\n",
            "Step [66/275]:\tLoss: 0.5728525519371033\n",
            "Step [67/275]:\tLoss: 0.45376282930374146\n",
            "Step [68/275]:\tLoss: 0.5378258228302002\n",
            "Step [69/275]:\tLoss: 0.2025991976261139\n",
            "Step [70/275]:\tLoss: 0.5152970552444458\n",
            "Step [71/275]:\tLoss: 0.5476661920547485\n",
            "Step [72/275]:\tLoss: 1.0489211082458496\n",
            "Step [73/275]:\tLoss: 0.5971202850341797\n",
            "Step [74/275]:\tLoss: 0.517781674861908\n",
            "Step [75/275]:\tLoss: 0.7219363451004028\n",
            "Step [76/275]:\tLoss: 0.5520188212394714\n",
            "Step [77/275]:\tLoss: 0.7157082557678223\n",
            "Step [78/275]:\tLoss: 0.19438618421554565\n",
            "Step [79/275]:\tLoss: 0.3226363956928253\n",
            "Step [80/275]:\tLoss: 0.5250445604324341\n",
            "Step [81/275]:\tLoss: 0.7283546924591064\n",
            "Step [82/275]:\tLoss: 0.9288594722747803\n",
            "Step [83/275]:\tLoss: 0.5422799587249756\n",
            "Step [84/275]:\tLoss: 0.453229695558548\n",
            "Step [85/275]:\tLoss: 0.5933941602706909\n",
            "Step [86/275]:\tLoss: 0.5791686177253723\n",
            "Step [87/275]:\tLoss: 0.605275571346283\n",
            "Step [88/275]:\tLoss: 0.6459163427352905\n",
            "Step [89/275]:\tLoss: 0.21106141805648804\n",
            "Step [90/275]:\tLoss: 0.8919808268547058\n",
            "Step [91/275]:\tLoss: 0.657446026802063\n",
            "Step [92/275]:\tLoss: 0.5110187530517578\n",
            "Step [93/275]:\tLoss: 0.6341638565063477\n",
            "Step [94/275]:\tLoss: 0.5550928115844727\n",
            "Step [95/275]:\tLoss: 0.8460035920143127\n",
            "Step [96/275]:\tLoss: 0.528268575668335\n",
            "Step [97/275]:\tLoss: 0.5417879223823547\n",
            "Step [98/275]:\tLoss: 0.4660014510154724\n",
            "Step [99/275]:\tLoss: 0.7305881977081299\n",
            "Step [100/275]:\tLoss: 0.5043976902961731\n",
            "Step [101/275]:\tLoss: 0.766069769859314\n",
            "Step [102/275]:\tLoss: 0.5343774557113647\n",
            "Step [103/275]:\tLoss: 0.5180137157440186\n",
            "Step [104/275]:\tLoss: 0.4644371271133423\n",
            "Step [105/275]:\tLoss: 0.5052153468132019\n",
            "Step [106/275]:\tLoss: 0.6630956530570984\n",
            "Step [107/275]:\tLoss: 0.22671163082122803\n",
            "Step [108/275]:\tLoss: 0.7803136110305786\n",
            "Step [109/275]:\tLoss: 0.8691154718399048\n",
            "Step [110/275]:\tLoss: 2.5557141304016113\n",
            "Step [111/275]:\tLoss: 2.5603556632995605\n",
            "Step [112/275]:\tLoss: 3.3999319076538086\n",
            "Step [113/275]:\tLoss: 1.7054340839385986\n",
            "Step [114/275]:\tLoss: 1.7865018844604492\n",
            "Step [115/275]:\tLoss: 2.614047050476074\n",
            "Step [116/275]:\tLoss: 1.761911392211914\n",
            "Step [117/275]:\tLoss: 1.335409164428711\n",
            "Step [118/275]:\tLoss: 0.8238261342048645\n",
            "Step [119/275]:\tLoss: 0.7196682691574097\n",
            "Step [120/275]:\tLoss: 1.1812756061553955\n",
            "Step [121/275]:\tLoss: 0.647784411907196\n",
            "Step [122/275]:\tLoss: 1.1341702938079834\n",
            "Step [123/275]:\tLoss: 1.0765769481658936\n",
            "Step [124/275]:\tLoss: 2.176748275756836\n",
            "Step [125/275]:\tLoss: 2.0831992626190186\n",
            "Step [126/275]:\tLoss: 1.0516055822372437\n",
            "Step [127/275]:\tLoss: 0.9631509780883789\n",
            "Step [128/275]:\tLoss: 2.2538094520568848\n",
            "Step [129/275]:\tLoss: 2.688338041305542\n",
            "Step [130/275]:\tLoss: 1.6335854530334473\n",
            "Step [131/275]:\tLoss: 1.486492395401001\n",
            "Step [132/275]:\tLoss: 1.4683700799942017\n",
            "Step [133/275]:\tLoss: 0.6202545166015625\n",
            "Step [134/275]:\tLoss: 2.3326029777526855\n",
            "Step [135/275]:\tLoss: 2.1175615787506104\n",
            "Step [136/275]:\tLoss: 1.6684566736221313\n",
            "Step [137/275]:\tLoss: 0.6327001452445984\n",
            "Step [138/275]:\tLoss: 0.29418981075286865\n",
            "Step [139/275]:\tLoss: 0.3058229088783264\n",
            "Step [140/275]:\tLoss: 0.2922908663749695\n",
            "Step [141/275]:\tLoss: 0.6883314847946167\n",
            "Step [142/275]:\tLoss: 0.28390878438949585\n",
            "Step [143/275]:\tLoss: 0.26762399077415466\n",
            "Step [144/275]:\tLoss: 0.7450042963027954\n",
            "Step [145/275]:\tLoss: 0.6539738178253174\n",
            "Step [146/275]:\tLoss: 0.7598482370376587\n",
            "Step [147/275]:\tLoss: 0.49303215742111206\n",
            "Step [148/275]:\tLoss: 0.2686012387275696\n",
            "Step [149/275]:\tLoss: 0.6082855463027954\n",
            "Step [150/275]:\tLoss: 0.7559155821800232\n",
            "Step [151/275]:\tLoss: 1.0463999509811401\n",
            "Step [152/275]:\tLoss: 0.9679471850395203\n",
            "Step [153/275]:\tLoss: 0.5391205549240112\n",
            "Step [154/275]:\tLoss: 0.667743444442749\n",
            "Step [155/275]:\tLoss: 0.50126051902771\n",
            "Step [156/275]:\tLoss: 0.5493185520172119\n",
            "Step [157/275]:\tLoss: 0.8273798227310181\n",
            "Step [158/275]:\tLoss: 0.5148617029190063\n",
            "Step [159/275]:\tLoss: 0.8517842292785645\n",
            "Step [160/275]:\tLoss: 0.37887588143348694\n",
            "Step [161/275]:\tLoss: 0.7267916798591614\n",
            "Step [162/275]:\tLoss: 0.4635113477706909\n",
            "Step [163/275]:\tLoss: 0.5070808529853821\n",
            "Step [164/275]:\tLoss: 0.6884897351264954\n",
            "Step [165/275]:\tLoss: 1.0141124725341797\n",
            "Step [166/275]:\tLoss: 0.5826984643936157\n",
            "Step [167/275]:\tLoss: 0.2837159037590027\n",
            "Step [168/275]:\tLoss: 0.7062984704971313\n",
            "Step [169/275]:\tLoss: 0.9857348799705505\n",
            "Step [170/275]:\tLoss: 0.6180721521377563\n",
            "Step [171/275]:\tLoss: 0.44810885190963745\n",
            "Step [172/275]:\tLoss: 0.6296024322509766\n",
            "Step [173/275]:\tLoss: 0.41884127259254456\n",
            "Step [174/275]:\tLoss: 0.7986831665039062\n",
            "Step [175/275]:\tLoss: 1.163608193397522\n",
            "Step [176/275]:\tLoss: 0.5931955575942993\n",
            "Step [177/275]:\tLoss: 1.051814317703247\n",
            "Step [178/275]:\tLoss: 0.9364759922027588\n",
            "Step [179/275]:\tLoss: 0.9024187326431274\n",
            "Step [180/275]:\tLoss: 0.5364269018173218\n",
            "Step [181/275]:\tLoss: 0.3262372612953186\n",
            "Step [182/275]:\tLoss: 0.5669270157814026\n",
            "Step [183/275]:\tLoss: 0.5169748067855835\n",
            "Step [184/275]:\tLoss: 0.762453019618988\n",
            "Step [185/275]:\tLoss: 0.9148266315460205\n",
            "Step [186/275]:\tLoss: 0.5086567401885986\n",
            "Step [187/275]:\tLoss: 0.4463539123535156\n",
            "Step [188/275]:\tLoss: 0.7610429525375366\n",
            "Step [189/275]:\tLoss: 0.5111650228500366\n",
            "Step [190/275]:\tLoss: 0.7944495677947998\n",
            "Step [191/275]:\tLoss: 0.8977623581886292\n",
            "Step [192/275]:\tLoss: 1.0402312278747559\n",
            "Step [193/275]:\tLoss: 2.4192116260528564\n",
            "Step [194/275]:\tLoss: 2.5876870155334473\n",
            "Step [195/275]:\tLoss: 2.5362300872802734\n",
            "Step [196/275]:\tLoss: 1.9192242622375488\n",
            "Step [197/275]:\tLoss: 2.1575984954833984\n",
            "Step [198/275]:\tLoss: 3.1070022583007812\n",
            "Step [199/275]:\tLoss: 2.365607261657715\n",
            "Step [200/275]:\tLoss: 1.947348952293396\n",
            "Step [201/275]:\tLoss: 2.3434157371520996\n",
            "Step [202/275]:\tLoss: 1.2783859968185425\n",
            "Step [203/275]:\tLoss: 1.7976012229919434\n",
            "Step [204/275]:\tLoss: 1.47084379196167\n",
            "Step [205/275]:\tLoss: 1.4999439716339111\n",
            "Step [206/275]:\tLoss: 2.593764305114746\n",
            "Step [207/275]:\tLoss: 1.563735008239746\n",
            "Step [208/275]:\tLoss: 1.8617898225784302\n",
            "Step [209/275]:\tLoss: 1.8920567035675049\n",
            "Step [210/275]:\tLoss: 2.0023512840270996\n",
            "Step [211/275]:\tLoss: 1.9026086330413818\n",
            "Step [212/275]:\tLoss: 2.069953680038452\n",
            "Step [213/275]:\tLoss: 1.248551368713379\n",
            "Step [214/275]:\tLoss: 2.3211607933044434\n",
            "Step [215/275]:\tLoss: 2.5535359382629395\n",
            "Step [216/275]:\tLoss: 2.0449438095092773\n",
            "Step [217/275]:\tLoss: 1.9725947380065918\n",
            "Step [218/275]:\tLoss: 1.6885393857955933\n",
            "Step [219/275]:\tLoss: 1.3149909973144531\n",
            "Step [220/275]:\tLoss: 2.4297165870666504\n",
            "Step [221/275]:\tLoss: 2.096975088119507\n",
            "Step [222/275]:\tLoss: 1.4870173931121826\n",
            "Step [223/275]:\tLoss: 2.077639579772949\n",
            "Step [224/275]:\tLoss: 1.5051047801971436\n",
            "Step [225/275]:\tLoss: 1.9151239395141602\n",
            "Step [226/275]:\tLoss: 1.7925100326538086\n",
            "Step [227/275]:\tLoss: 1.4421803951263428\n",
            "Step [228/275]:\tLoss: 1.4220117330551147\n",
            "Step [229/275]:\tLoss: 1.618269681930542\n",
            "Step [230/275]:\tLoss: 1.7635440826416016\n",
            "Step [231/275]:\tLoss: 2.1504616737365723\n",
            "Step [232/275]:\tLoss: 1.8811402320861816\n",
            "Step [233/275]:\tLoss: 1.6515146493911743\n",
            "Step [234/275]:\tLoss: 1.856379508972168\n",
            "Step [235/275]:\tLoss: 1.6874496936798096\n",
            "Step [236/275]:\tLoss: 1.4164834022521973\n",
            "Step [237/275]:\tLoss: 2.5298657417297363\n",
            "Step [238/275]:\tLoss: 2.0454111099243164\n",
            "Step [239/275]:\tLoss: 1.3863787651062012\n",
            "Step [240/275]:\tLoss: 1.1909831762313843\n",
            "Step [241/275]:\tLoss: 1.585809350013733\n",
            "Step [242/275]:\tLoss: 1.1477712392807007\n",
            "Step [243/275]:\tLoss: 1.5566751956939697\n",
            "Step [244/275]:\tLoss: 2.0094408988952637\n",
            "Step [245/275]:\tLoss: 1.2934762239456177\n",
            "Step [246/275]:\tLoss: 1.398331642150879\n",
            "Step [247/275]:\tLoss: 1.0473443269729614\n",
            "Step [248/275]:\tLoss: 2.6629638671875\n",
            "Step [249/275]:\tLoss: 2.161531448364258\n",
            "Step [250/275]:\tLoss: 1.162327766418457\n",
            "Step [251/275]:\tLoss: 1.7216750383377075\n",
            "Step [252/275]:\tLoss: 2.5824360847473145\n",
            "Step [253/275]:\tLoss: 1.5184389352798462\n",
            "Step [254/275]:\tLoss: 2.0078258514404297\n",
            "Step [255/275]:\tLoss: 1.3300467729568481\n",
            "Step [256/275]:\tLoss: 1.449791669845581\n",
            "Step [257/275]:\tLoss: 3.068664789199829\n",
            "Step [258/275]:\tLoss: 1.3318036794662476\n",
            "Step [259/275]:\tLoss: 1.3819098472595215\n",
            "Step [260/275]:\tLoss: 1.2635836601257324\n",
            "Step [261/275]:\tLoss: 1.3533555269241333\n",
            "Step [262/275]:\tLoss: 1.5866413116455078\n",
            "Step [263/275]:\tLoss: 1.7942601442337036\n",
            "Step [264/275]:\tLoss: 1.3761980533599854\n",
            "Step [265/275]:\tLoss: 1.3615772724151611\n",
            "Step [266/275]:\tLoss: 1.8852750062942505\n",
            "Step [267/275]:\tLoss: 1.4194653034210205\n",
            "Step [268/275]:\tLoss: 1.5139480829238892\n",
            "Step [269/275]:\tLoss: 1.3283112049102783\n",
            "Step [270/275]:\tLoss: 1.7750240564346313\n",
            "Step [271/275]:\tLoss: 1.5465607643127441\n",
            "Step [272/275]:\tLoss: 1.309765100479126\n",
            "Step [273/275]:\tLoss: 2.578017234802246\n",
            "Step [274/275]:\tLoss: 1.9032566547393799\n",
            "epoch number 112\n",
            "Step [0/275]:\tLoss: 1.6627695560455322\n",
            "Step [1/275]:\tLoss: 1.9738377332687378\n",
            "Step [2/275]:\tLoss: 1.2933592796325684\n",
            "Step [3/275]:\tLoss: 3.4165866374969482\n",
            "Step [4/275]:\tLoss: 1.3000946044921875\n",
            "Step [5/275]:\tLoss: 0.5406336188316345\n",
            "Step [6/275]:\tLoss: 0.958258867263794\n",
            "Step [7/275]:\tLoss: 2.1295952796936035\n",
            "Step [8/275]:\tLoss: 1.2403173446655273\n",
            "Step [9/275]:\tLoss: 2.104543685913086\n",
            "Step [10/275]:\tLoss: 0.24010738730430603\n",
            "Step [11/275]:\tLoss: 0.8094514012336731\n",
            "Step [12/275]:\tLoss: 1.1988441944122314\n",
            "Step [13/275]:\tLoss: 1.0579180717468262\n",
            "Step [14/275]:\tLoss: 0.9895349740982056\n",
            "Step [15/275]:\tLoss: 1.4243090152740479\n",
            "Step [16/275]:\tLoss: 1.551760196685791\n",
            "Step [17/275]:\tLoss: 1.324041485786438\n",
            "Step [18/275]:\tLoss: 0.8768705725669861\n",
            "Step [19/275]:\tLoss: 1.5778412818908691\n",
            "Step [20/275]:\tLoss: 1.1075764894485474\n",
            "Step [21/275]:\tLoss: 1.2212477922439575\n",
            "Step [22/275]:\tLoss: 0.9262485504150391\n",
            "Step [23/275]:\tLoss: 1.8723623752593994\n",
            "Step [24/275]:\tLoss: 0.7837191820144653\n",
            "Step [25/275]:\tLoss: 0.723212480545044\n",
            "Step [26/275]:\tLoss: 0.7962256669998169\n",
            "Step [27/275]:\tLoss: 1.0710581541061401\n",
            "Step [28/275]:\tLoss: 2.3107845783233643\n",
            "Step [29/275]:\tLoss: 0.560688853263855\n",
            "Step [30/275]:\tLoss: 0.9328380823135376\n",
            "Step [31/275]:\tLoss: 0.9118920564651489\n",
            "Step [32/275]:\tLoss: 0.8947399854660034\n",
            "Step [33/275]:\tLoss: 0.5496416091918945\n",
            "Step [34/275]:\tLoss: 0.733356237411499\n",
            "Step [35/275]:\tLoss: 0.5206292867660522\n",
            "Step [36/275]:\tLoss: 0.2629983425140381\n",
            "Step [37/275]:\tLoss: 0.9633145332336426\n",
            "Step [38/275]:\tLoss: 0.6224991679191589\n",
            "Step [39/275]:\tLoss: 0.2676381468772888\n",
            "Step [40/275]:\tLoss: 0.2620755434036255\n",
            "Step [41/275]:\tLoss: 0.5599753856658936\n",
            "Step [42/275]:\tLoss: 0.7832862734794617\n",
            "Step [43/275]:\tLoss: 0.6393114924430847\n",
            "Step [44/275]:\tLoss: 0.7463135123252869\n",
            "Step [45/275]:\tLoss: 0.5407828092575073\n",
            "Step [46/275]:\tLoss: 0.48358505964279175\n",
            "Step [47/275]:\tLoss: 0.5286557674407959\n",
            "Step [48/275]:\tLoss: 0.6737566590309143\n",
            "Step [49/275]:\tLoss: 0.36398330330848694\n",
            "Step [50/275]:\tLoss: 0.4097689688205719\n",
            "Step [51/275]:\tLoss: 0.5372569561004639\n",
            "Step [52/275]:\tLoss: 0.7535037994384766\n",
            "Step [53/275]:\tLoss: 0.5018934607505798\n",
            "Step [54/275]:\tLoss: 0.9644652009010315\n",
            "Step [55/275]:\tLoss: 0.30559059977531433\n",
            "Step [56/275]:\tLoss: 0.3075436055660248\n",
            "Step [57/275]:\tLoss: 0.783588171005249\n",
            "Step [58/275]:\tLoss: 0.22272469103336334\n",
            "Step [59/275]:\tLoss: 0.5653322339057922\n",
            "Step [60/275]:\tLoss: 0.776587963104248\n",
            "Step [61/275]:\tLoss: 0.7049241065979004\n",
            "Step [62/275]:\tLoss: 0.527438223361969\n",
            "Step [63/275]:\tLoss: 0.805637001991272\n",
            "Step [64/275]:\tLoss: 0.7564705610275269\n",
            "Step [65/275]:\tLoss: 0.5599271059036255\n",
            "Step [66/275]:\tLoss: 0.46896451711654663\n",
            "Step [67/275]:\tLoss: 0.4408670663833618\n",
            "Step [68/275]:\tLoss: 0.9330359697341919\n",
            "Step [69/275]:\tLoss: 0.8132144808769226\n",
            "Step [70/275]:\tLoss: 1.0661755800247192\n",
            "Step [71/275]:\tLoss: 0.4895970821380615\n",
            "Step [72/275]:\tLoss: 0.7655706405639648\n",
            "Step [73/275]:\tLoss: 0.4680539667606354\n",
            "Step [74/275]:\tLoss: 0.24741466343402863\n",
            "Step [75/275]:\tLoss: 0.6702196598052979\n",
            "Step [76/275]:\tLoss: 0.5480828285217285\n",
            "Step [77/275]:\tLoss: 0.7164660096168518\n",
            "Step [78/275]:\tLoss: 0.5360997915267944\n",
            "Step [79/275]:\tLoss: 0.8224047422409058\n",
            "Step [80/275]:\tLoss: 0.48256731033325195\n",
            "Step [81/275]:\tLoss: 0.2198743373155594\n",
            "Step [82/275]:\tLoss: 0.6297746300697327\n",
            "Step [83/275]:\tLoss: 0.6019413471221924\n",
            "Step [84/275]:\tLoss: 0.7330852746963501\n",
            "Step [85/275]:\tLoss: 0.9009149670600891\n",
            "Step [86/275]:\tLoss: 0.8233827352523804\n",
            "Step [87/275]:\tLoss: 0.5332038402557373\n",
            "Step [88/275]:\tLoss: 0.47416773438453674\n",
            "Step [89/275]:\tLoss: 0.6197431087493896\n",
            "Step [90/275]:\tLoss: 0.45686158537864685\n",
            "Step [91/275]:\tLoss: 0.6164523959159851\n",
            "Step [92/275]:\tLoss: 0.5062103867530823\n",
            "Step [93/275]:\tLoss: 0.44162559509277344\n",
            "Step [94/275]:\tLoss: 0.6198502779006958\n",
            "Step [95/275]:\tLoss: 0.6653419137001038\n",
            "Step [96/275]:\tLoss: 0.7861592769622803\n",
            "Step [97/275]:\tLoss: 0.6013565063476562\n",
            "Step [98/275]:\tLoss: 0.531593918800354\n",
            "Step [99/275]:\tLoss: 0.7322937846183777\n",
            "Step [100/275]:\tLoss: 0.8591418266296387\n",
            "Step [101/275]:\tLoss: 0.5522841215133667\n",
            "Step [102/275]:\tLoss: 0.8530310392379761\n",
            "Step [103/275]:\tLoss: 0.2154437005519867\n",
            "Step [104/275]:\tLoss: 0.2970825135707855\n",
            "Step [105/275]:\tLoss: 0.6348732113838196\n",
            "Step [106/275]:\tLoss: 0.7749489545822144\n",
            "Step [107/275]:\tLoss: 0.7709784507751465\n",
            "Step [108/275]:\tLoss: 0.5194281339645386\n",
            "Step [109/275]:\tLoss: 0.6396182775497437\n",
            "Step [110/275]:\tLoss: 2.2297215461730957\n",
            "Step [111/275]:\tLoss: 2.4071195125579834\n",
            "Step [112/275]:\tLoss: 1.7132596969604492\n",
            "Step [113/275]:\tLoss: 0.9725568294525146\n",
            "Step [114/275]:\tLoss: 2.3687658309936523\n",
            "Step [115/275]:\tLoss: 2.3018922805786133\n",
            "Step [116/275]:\tLoss: 2.493614673614502\n",
            "Step [117/275]:\tLoss: 1.4969371557235718\n",
            "Step [118/275]:\tLoss: 1.7945390939712524\n",
            "Step [119/275]:\tLoss: 1.3089812994003296\n",
            "Step [120/275]:\tLoss: 1.5099543333053589\n",
            "Step [121/275]:\tLoss: 1.4194847345352173\n",
            "Step [122/275]:\tLoss: 2.9808120727539062\n",
            "Step [123/275]:\tLoss: 1.050668478012085\n",
            "Step [124/275]:\tLoss: 1.9570872783660889\n",
            "Step [125/275]:\tLoss: 1.7433812618255615\n",
            "Step [126/275]:\tLoss: 1.841910719871521\n",
            "Step [127/275]:\tLoss: 1.4633095264434814\n",
            "Step [128/275]:\tLoss: 1.183148741722107\n",
            "Step [129/275]:\tLoss: 1.327955961227417\n",
            "Step [130/275]:\tLoss: 1.6544549465179443\n",
            "Step [131/275]:\tLoss: 1.697683572769165\n",
            "Step [132/275]:\tLoss: 2.0193371772766113\n",
            "Step [133/275]:\tLoss: 2.473585367202759\n",
            "Step [134/275]:\tLoss: 1.7230546474456787\n",
            "Step [135/275]:\tLoss: 1.4935328960418701\n",
            "Step [136/275]:\tLoss: 2.100125789642334\n",
            "Step [137/275]:\tLoss: 1.5274704694747925\n",
            "Step [138/275]:\tLoss: 1.1943483352661133\n",
            "Step [139/275]:\tLoss: 0.7180997133255005\n",
            "Step [140/275]:\tLoss: 0.6082325577735901\n",
            "Step [141/275]:\tLoss: 0.8109562993049622\n",
            "Step [142/275]:\tLoss: 1.4460797309875488\n",
            "Step [143/275]:\tLoss: 0.6207742691040039\n",
            "Step [144/275]:\tLoss: 0.5633825063705444\n",
            "Step [145/275]:\tLoss: 0.29444536566734314\n",
            "Step [146/275]:\tLoss: 0.4287058115005493\n",
            "Step [147/275]:\tLoss: 0.8723212480545044\n",
            "Step [148/275]:\tLoss: 0.4883607029914856\n",
            "Step [149/275]:\tLoss: 0.7023137807846069\n",
            "Step [150/275]:\tLoss: 0.49319854378700256\n",
            "Step [151/275]:\tLoss: 0.5831312537193298\n",
            "Step [152/275]:\tLoss: 0.5775406956672668\n",
            "Step [153/275]:\tLoss: 0.6706942915916443\n",
            "Step [154/275]:\tLoss: 1.0332105159759521\n",
            "Step [155/275]:\tLoss: 0.7417913675308228\n",
            "Step [156/275]:\tLoss: 1.057737946510315\n",
            "Step [157/275]:\tLoss: 0.3495873510837555\n",
            "Step [158/275]:\tLoss: 0.4976428151130676\n",
            "Step [159/275]:\tLoss: 0.5911386609077454\n",
            "Step [160/275]:\tLoss: 0.6338315010070801\n",
            "Step [161/275]:\tLoss: 0.4778101146221161\n",
            "Step [162/275]:\tLoss: 0.28567665815353394\n",
            "Step [163/275]:\tLoss: 0.23314784467220306\n",
            "Step [164/275]:\tLoss: 0.7152020931243896\n",
            "Step [165/275]:\tLoss: 1.0994309186935425\n",
            "Step [166/275]:\tLoss: 0.9377558827400208\n",
            "Step [167/275]:\tLoss: 0.4923470616340637\n",
            "Step [168/275]:\tLoss: 0.7724771499633789\n",
            "Step [169/275]:\tLoss: 0.6262432932853699\n",
            "Step [170/275]:\tLoss: 0.7793644666671753\n",
            "Step [171/275]:\tLoss: 0.3581849932670593\n",
            "Step [172/275]:\tLoss: 0.622250497341156\n",
            "Step [173/275]:\tLoss: 0.5384436249732971\n",
            "Step [174/275]:\tLoss: 0.5338618755340576\n",
            "Step [175/275]:\tLoss: 2.401015043258667\n",
            "Step [176/275]:\tLoss: 0.645616352558136\n",
            "Step [177/275]:\tLoss: 0.7265655398368835\n",
            "Step [178/275]:\tLoss: 0.7988499999046326\n",
            "Step [179/275]:\tLoss: 0.4891008734703064\n",
            "Step [180/275]:\tLoss: 0.6500836610794067\n",
            "Step [181/275]:\tLoss: 0.9336101412773132\n",
            "Step [182/275]:\tLoss: 0.6062561273574829\n",
            "Step [183/275]:\tLoss: 0.32190561294555664\n",
            "Step [184/275]:\tLoss: 0.6751257181167603\n",
            "Step [185/275]:\tLoss: 0.24587410688400269\n",
            "Step [186/275]:\tLoss: 0.676934003829956\n",
            "Step [187/275]:\tLoss: 0.5717192888259888\n",
            "Step [188/275]:\tLoss: 0.8154960870742798\n",
            "Step [189/275]:\tLoss: 0.7655737400054932\n",
            "Step [190/275]:\tLoss: 0.29921388626098633\n",
            "Step [191/275]:\tLoss: 0.5307299494743347\n",
            "Step [192/275]:\tLoss: 1.5975219011306763\n",
            "Step [193/275]:\tLoss: 2.235496997833252\n",
            "Step [194/275]:\tLoss: 2.5346274375915527\n",
            "Step [195/275]:\tLoss: 2.03320050239563\n",
            "Step [196/275]:\tLoss: 1.927189826965332\n",
            "Step [197/275]:\tLoss: 2.2879154682159424\n",
            "Step [198/275]:\tLoss: 1.6024537086486816\n",
            "Step [199/275]:\tLoss: 2.342555522918701\n",
            "Step [200/275]:\tLoss: 1.8086071014404297\n",
            "Step [201/275]:\tLoss: 1.788453459739685\n",
            "Step [202/275]:\tLoss: 2.1616921424865723\n",
            "Step [203/275]:\tLoss: 1.6833851337432861\n",
            "Step [204/275]:\tLoss: 1.5689849853515625\n",
            "Step [205/275]:\tLoss: 2.1115264892578125\n",
            "Step [206/275]:\tLoss: 1.5301368236541748\n",
            "Step [207/275]:\tLoss: 1.6219326257705688\n",
            "Step [208/275]:\tLoss: 1.3121370077133179\n",
            "Step [209/275]:\tLoss: 1.5270674228668213\n",
            "Step [210/275]:\tLoss: 1.6296321153640747\n",
            "Step [211/275]:\tLoss: 1.8403077125549316\n",
            "Step [212/275]:\tLoss: 2.138871431350708\n",
            "Step [213/275]:\tLoss: 2.0984528064727783\n",
            "Step [214/275]:\tLoss: 1.546595573425293\n",
            "Step [215/275]:\tLoss: 1.2135510444641113\n",
            "Step [216/275]:\tLoss: 1.5448286533355713\n",
            "Step [217/275]:\tLoss: 1.9674901962280273\n",
            "Step [218/275]:\tLoss: 1.5978046655654907\n",
            "Step [219/275]:\tLoss: 1.3424266576766968\n",
            "Step [220/275]:\tLoss: 1.5594627857208252\n",
            "Step [221/275]:\tLoss: 1.6389257907867432\n",
            "Step [222/275]:\tLoss: 1.990665078163147\n",
            "Step [223/275]:\tLoss: 1.624405860900879\n",
            "Step [224/275]:\tLoss: 1.3811969757080078\n",
            "Step [225/275]:\tLoss: 2.136946439743042\n",
            "Step [226/275]:\tLoss: 1.66887629032135\n",
            "Step [227/275]:\tLoss: 1.4131357669830322\n",
            "Step [228/275]:\tLoss: 1.6099656820297241\n",
            "Step [229/275]:\tLoss: 1.2798051834106445\n",
            "Step [230/275]:\tLoss: 1.6749958992004395\n",
            "Step [231/275]:\tLoss: 1.2176191806793213\n",
            "Step [232/275]:\tLoss: 1.2117358446121216\n",
            "Step [233/275]:\tLoss: 1.7713935375213623\n",
            "Step [234/275]:\tLoss: 1.5357606410980225\n",
            "Step [235/275]:\tLoss: 1.6806966066360474\n",
            "Step [236/275]:\tLoss: 1.322847843170166\n",
            "Step [237/275]:\tLoss: 0.8119360208511353\n",
            "Step [238/275]:\tLoss: 1.633655309677124\n",
            "Step [239/275]:\tLoss: 0.9628262519836426\n",
            "Step [240/275]:\tLoss: 1.1353704929351807\n",
            "Step [241/275]:\tLoss: 1.3384900093078613\n",
            "Step [242/275]:\tLoss: 1.4778554439544678\n",
            "Step [243/275]:\tLoss: 1.2906450033187866\n",
            "Step [244/275]:\tLoss: 1.5961339473724365\n",
            "Step [245/275]:\tLoss: 1.4391167163848877\n",
            "Step [246/275]:\tLoss: 1.384537696838379\n",
            "Step [247/275]:\tLoss: 1.424615502357483\n",
            "Step [248/275]:\tLoss: 1.9690680503845215\n",
            "Step [249/275]:\tLoss: 1.8383252620697021\n",
            "Step [250/275]:\tLoss: 1.5070488452911377\n",
            "Step [251/275]:\tLoss: 2.173795700073242\n",
            "Step [252/275]:\tLoss: 1.4467092752456665\n",
            "Step [253/275]:\tLoss: 1.5254647731781006\n",
            "Step [254/275]:\tLoss: 2.1342625617980957\n",
            "Step [255/275]:\tLoss: 1.3603688478469849\n",
            "Step [256/275]:\tLoss: 1.1190354824066162\n",
            "Step [257/275]:\tLoss: 1.150658130645752\n",
            "Step [258/275]:\tLoss: 1.8774302005767822\n",
            "Step [259/275]:\tLoss: 1.2999857664108276\n",
            "Step [260/275]:\tLoss: 1.1109864711761475\n",
            "Step [261/275]:\tLoss: 1.6737289428710938\n",
            "Step [262/275]:\tLoss: 1.1052727699279785\n",
            "Step [263/275]:\tLoss: 1.4289827346801758\n",
            "Step [264/275]:\tLoss: 1.1296942234039307\n",
            "Step [265/275]:\tLoss: 3.0987257957458496\n",
            "Step [266/275]:\tLoss: 2.359807014465332\n",
            "Step [267/275]:\tLoss: 1.348092794418335\n",
            "Step [268/275]:\tLoss: 1.3887261152267456\n",
            "Step [269/275]:\tLoss: 1.4857441186904907\n",
            "Step [270/275]:\tLoss: 1.430922269821167\n",
            "Step [271/275]:\tLoss: 1.838809609413147\n",
            "Step [272/275]:\tLoss: 1.71433687210083\n",
            "Step [273/275]:\tLoss: 1.2566605806350708\n",
            "Step [274/275]:\tLoss: 3.604804039001465\n",
            "epoch number 113\n",
            "Step [0/275]:\tLoss: 1.4932291507720947\n",
            "Step [1/275]:\tLoss: 1.4146089553833008\n",
            "Step [2/275]:\tLoss: 1.8826899528503418\n",
            "Step [3/275]:\tLoss: 1.4131691455841064\n",
            "Step [4/275]:\tLoss: 1.6227543354034424\n",
            "Step [5/275]:\tLoss: 2.5813047885894775\n",
            "Step [6/275]:\tLoss: 1.045492172241211\n",
            "Step [7/275]:\tLoss: 1.1663203239440918\n",
            "Step [8/275]:\tLoss: 2.3275561332702637\n",
            "Step [9/275]:\tLoss: 2.9572672843933105\n",
            "Step [10/275]:\tLoss: 0.3035926818847656\n",
            "Step [11/275]:\tLoss: 0.38818788528442383\n",
            "Step [12/275]:\tLoss: 0.7965822219848633\n",
            "Step [13/275]:\tLoss: 0.7315368056297302\n",
            "Step [14/275]:\tLoss: 0.7219476699829102\n",
            "Step [15/275]:\tLoss: 1.2306277751922607\n",
            "Step [16/275]:\tLoss: 1.4794896841049194\n",
            "Step [17/275]:\tLoss: 2.7658562660217285\n",
            "Step [18/275]:\tLoss: 1.0617705583572388\n",
            "Step [19/275]:\tLoss: 2.083557605743408\n",
            "Step [20/275]:\tLoss: 0.8836269378662109\n",
            "Step [21/275]:\tLoss: 2.3926076889038086\n",
            "Step [22/275]:\tLoss: 0.8318360447883606\n",
            "Step [23/275]:\tLoss: 2.733889102935791\n",
            "Step [24/275]:\tLoss: 1.5768542289733887\n",
            "Step [25/275]:\tLoss: 1.0143893957138062\n",
            "Step [26/275]:\tLoss: 1.0403938293457031\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/BYOL-ViT-Hourglass/BYOL/Train_BYOL.py\", line 452, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/BYOL-ViT-Hourglass/BYOL/Train_BYOL.py\", line 434, in main\n",
            "    print(f\"Step [{step}/{len(train_loader)}]:\\tLoss: {loss.item()}\")\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# train BYOL\n",
        "!python /content/drive/MyDrive/BYOL-ViT-Hourglass/BYOL/Train_BYOL.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hourglass definition (modified)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fXNunraDP5va"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9f6S9BjHF0V",
        "outputId": "65d3d3b2-0787-48db-827a-eeef726be168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ2ooIOmLP-W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, reduce, repeat\n",
        "\n",
        "# helpers\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "def pad_to_multiple(tensor, multiple, dim = -1, value = 0):\n",
        "    seq_len = tensor.shape[dim]\n",
        "    m = seq_len / multiple\n",
        "    if m.is_integer():\n",
        "        return tensor\n",
        "    remainder = math.ceil(m) * multiple - seq_len\n",
        "    pad_offset = (0,) * (-1 - dim) * 2\n",
        "    return F.pad(tensor, (*pad_offset, 0, remainder), value = value)\n",
        "\n",
        "def cast_tuple(val, depth = 1):\n",
        "    return val if isinstance(val, tuple) else ((val,) * depth)\n",
        "\n",
        "# factory\n",
        "\n",
        "def get_hourglass_transformer(\n",
        "    dim,\n",
        "    *,\n",
        "    depth,\n",
        "    shorten_factor,\n",
        "    attn_resampling,\n",
        "    updown_sample_type,\n",
        "    **kwargs\n",
        "):\n",
        "    assert isinstance(depth, int) or (isinstance(depth, tuple)  and len(depth) == 3), 'depth must be either an integer or a tuple of 3, indicating (pre_transformer_depth, <nested-hour-glass-config>, post_transformer_depth)'\n",
        "    assert not (isinstance(depth, int) and shorten_factor), 'there does not need to be a shortening factor when only a single transformer block is indicated (depth of one integer value)'\n",
        "\n",
        "    if isinstance(depth, int):\n",
        "        return Transformer(dim = dim, depth = depth, **kwargs)\n",
        "\n",
        "    return HourglassTransformer(dim = dim, depth = depth, shorten_factor = shorten_factor, attn_resampling = attn_resampling, updown_sample_type = updown_sample_type, **kwargs)\n",
        "\n",
        "# up and down sample classes\n",
        "\n",
        "class NaiveDownsample(nn.Module):\n",
        "    def __init__(self, shorten_factor):\n",
        "        super().__init__()\n",
        "        self.shorten_factor = shorten_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        return reduce(x, 'b (n s) d -> b n d', 'mean', s = self.shorten_factor)\n",
        "\n",
        "class NaiveUpsample(nn.Module):\n",
        "    def __init__(self, shorten_factor):\n",
        "        super().__init__()\n",
        "        self.shorten_factor = shorten_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        return repeat(x, 'b n d -> b (n s) d', s = self.shorten_factor)\n",
        "\n",
        "class LinearDownsample(nn.Module):\n",
        "    def __init__(self, dim, shorten_factor):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(dim * shorten_factor, dim)\n",
        "        self.shorten_factor = shorten_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = rearrange(x, 'b (n s) d -> b n (s d)', s = self.shorten_factor)\n",
        "        return self.proj(x)\n",
        "\n",
        "class LinearUpsample(nn.Module):\n",
        "    def __init__(self, dim, shorten_factor):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(dim, dim * shorten_factor)\n",
        "        self.shorten_factor = shorten_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        return rearrange(x, 'b n (s d) -> b (n s) d', s = self.shorten_factor)\n",
        "\n",
        "# classes\n",
        "\n",
        "class PreNormResidual(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs) + x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        heads = 8,\n",
        "        dim_head = 64,\n",
        "        dropout = 0.,\n",
        "        causal = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.causal = causal\n",
        "        self.scale = dim_head ** -0.5\n",
        "        inner_dim = heads * dim_head\n",
        "\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, context = None, mask = None):\n",
        "        h, device = self.heads, x.device\n",
        "        kv_input = default(context, x)\n",
        "\n",
        "        q, k, v = self.to_q(x), *self.to_kv(kv_input).chunk(2, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
        "\n",
        "        q = q * self.scale\n",
        "\n",
        "        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
        "        mask_value = -torch.finfo(sim.dtype).max\n",
        "\n",
        "        if exists(mask):\n",
        "            mask = rearrange(mask, 'b j -> b () () j')\n",
        "            sim = sim.masked_fill(~mask, mask_value)\n",
        "\n",
        "        if self.causal:\n",
        "            i, j = sim.shape[-2:]\n",
        "            mask = torch.ones(i, j, device = device, dtype = torch.bool).triu_(j - i + 1)\n",
        "            mask = rearrange(mask, 'i j -> () () i j')\n",
        "            sim = sim.masked_fill(mask, mask_value)\n",
        "\n",
        "        attn = sim.softmax(dim = -1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
        "        return self.to_out(out)\n",
        "\n",
        "def FeedForward(dim, mult = 4, dropout = 0.):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(dim, dim * mult),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(dim * mult, dim)\n",
        "    )\n",
        "\n",
        "# transformer classes\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        *,\n",
        "        depth,\n",
        "        causal = False,\n",
        "        heads = 8,\n",
        "        dim_head = 64,\n",
        "        attn_dropout = 0.,\n",
        "        ff_mult = 4,\n",
        "        ff_dropout = 0.,\n",
        "        norm_out = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNormResidual(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout, causal = causal)),\n",
        "                PreNormResidual(dim, FeedForward(dim, mult = ff_mult, dropout = ff_dropout))\n",
        "            ]))\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim) if norm_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, context = None, mask = None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, context = context, mask = mask)\n",
        "            x = ff(x)\n",
        "\n",
        "        return self.norm(x)\n",
        "\n",
        "class HourglassTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        *,\n",
        "        depth,\n",
        "        shorten_factor = 2,\n",
        "        attn_resampling = True,\n",
        "        updown_sample_type = 'naive',\n",
        "        heads = 8,\n",
        "        dim_head = 64,\n",
        "        causal = False,\n",
        "        norm_out = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert len(depth) == 3, 'depth should be a tuple of length 3'\n",
        "        assert updown_sample_type in {'naive', 'linear'}, 'downsample / upsample type must be either naive (average pool and repeat) or linear (linear projection and reshape)'\n",
        "\n",
        "        pre_layers_depth, valley_depth, post_layers_depth = depth\n",
        "\n",
        "        if isinstance(shorten_factor, (tuple, list)):\n",
        "            shorten_factor, *rest_shorten_factor = shorten_factor\n",
        "        elif isinstance(valley_depth, int):\n",
        "            shorten_factor, rest_shorten_factor = shorten_factor, None\n",
        "        else:\n",
        "            shorten_factor, rest_shorten_factor = shorten_factor, shorten_factor\n",
        "\n",
        "        transformer_kwargs = dict(\n",
        "            dim = dim,\n",
        "            heads = heads,\n",
        "            dim_head = dim_head\n",
        "        )\n",
        "\n",
        "        self.causal = causal\n",
        "        self.shorten_factor = shorten_factor\n",
        "\n",
        "        if updown_sample_type == 'naive':\n",
        "            self.downsample = NaiveDownsample(shorten_factor)\n",
        "            self.upsample   = NaiveUpsample(shorten_factor)\n",
        "        elif updown_sample_type == 'linear':\n",
        "            self.downsample = LinearDownsample(dim, shorten_factor)\n",
        "            self.upsample   = LinearUpsample(dim, shorten_factor)\n",
        "        else:\n",
        "            raise ValueError(f'unknown updown_sample_type keyword value - must be either naive or linear for now')\n",
        "\n",
        "        self.valley_transformer = get_hourglass_transformer(\n",
        "            shorten_factor = rest_shorten_factor,\n",
        "            depth = valley_depth,\n",
        "            attn_resampling = attn_resampling,\n",
        "            updown_sample_type = updown_sample_type,\n",
        "            causal = causal,\n",
        "            **transformer_kwargs\n",
        "        )\n",
        "\n",
        "        self.attn_resampling_pre_valley = Transformer(depth = 1, **transformer_kwargs) if attn_resampling else None\n",
        "        self.attn_resampling_post_valley = Transformer(depth = 1, **transformer_kwargs) if attn_resampling else None\n",
        "\n",
        "        self.pre_transformer = Transformer(depth = pre_layers_depth, causal = causal, **transformer_kwargs)\n",
        "        self.post_transformer = Transformer(depth = post_layers_depth, causal = causal, **transformer_kwargs)\n",
        "        self.norm_out = nn.LayerNorm(dim) if norm_out else nn.Identity()\n",
        "        self.s = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        # b : batch, n : sequence length, d : feature dimension, s : shortening factor\n",
        "\n",
        "        s, b, n = self.shorten_factor, *x.shape[:2]\n",
        "\n",
        "        # top half of hourglass, pre-transformer layers\n",
        "\n",
        "        x = self.pre_transformer(x, mask = mask)\n",
        "\n",
        "        # pad to multiple of shortening factor, in preparation for pooling\n",
        "\n",
        "        x = pad_to_multiple(x, s, dim = -2)\n",
        "\n",
        "        if exists(mask):\n",
        "            padded_mask = pad_to_multiple(mask, s, dim = -1, value = False)\n",
        "\n",
        "        # save the residual, and for \"attention resampling\" at downsample and upsample\n",
        "\n",
        "        x_residual = x.clone()\n",
        "\n",
        "        # if autoregressive, do the shift by shortening factor minus one\n",
        "\n",
        "        if self.causal:\n",
        "            shift = s - 1\n",
        "            x = F.pad(x, (0, 0, shift, -shift), value = 0.)\n",
        "\n",
        "            if exists(mask):\n",
        "                padded_mask = F.pad(padded_mask, (shift, -shift), value = False)\n",
        "\n",
        "        # naive average pool\n",
        "\n",
        "        downsampled = self.downsample(x)\n",
        "\n",
        "        if exists(mask):\n",
        "            downsampled_mask = reduce(padded_mask, 'b (n s) -> b n', 'sum', s = s) > 0\n",
        "        else:\n",
        "            downsampled_mask = None\n",
        "\n",
        "        # pre-valley \"attention resampling\" - they have the pooled token in each bucket attend to the tokens pre-pooled\n",
        "\n",
        "        if exists(self.attn_resampling_pre_valley):\n",
        "            if exists(mask):\n",
        "                attn_resampling_mask = rearrange(padded_mask, 'b (n s) -> (b n) s', s = s)\n",
        "            else:\n",
        "                attn_resampling_mask = None\n",
        "\n",
        "            downsampled = self.attn_resampling_pre_valley(\n",
        "                rearrange(downsampled, 'b n d -> (b n) () d'),\n",
        "                rearrange(x, 'b (n s) d -> (b n) s d', s = s),\n",
        "                mask = attn_resampling_mask\n",
        "            )\n",
        "\n",
        "            downsampled = rearrange(downsampled, '(b n) () d -> b n d', b = b)\n",
        "\n",
        "        # the \"valley\" - either a regular transformer or another hourglass\n",
        "\n",
        "        x = self.valley_transformer(downsampled, mask = downsampled_mask)\n",
        "\n",
        "        valley_out = x.clone()\n",
        "\n",
        "        # naive repeat upsample\n",
        "\n",
        "        x = self.upsample(x)\n",
        "\n",
        "        # add the residual\n",
        "\n",
        "        x = x + x_residual\n",
        "\n",
        "        # post-valley \"attention resampling\"\n",
        "\n",
        "        if exists(self.attn_resampling_post_valley):\n",
        "            x = self.attn_resampling_post_valley(\n",
        "                rearrange(x, 'b (n s) d -> (b n) s d', s = s),\n",
        "                rearrange(valley_out, 'b n d -> (b n) () d')\n",
        "            )\n",
        "\n",
        "            x = rearrange(x, '(b n) s d -> b (n s) d', b = b)\n",
        "\n",
        "        # bring sequence back to original length, if it were padded for pooling\n",
        "\n",
        "        x = x[:, :n]\n",
        "\n",
        "        # post-valley transformers\n",
        "\n",
        "        x = self.post_transformer(x, mask = mask)\n",
        "\n",
        "        x = self.norm_out(x)\n",
        "        #with torch.no_grad():\n",
        "        x = torch.flatten(x)\n",
        "        \n",
        "        x = nn.Linear(512*144,10)(x)\n",
        "  \n",
        "        x = self.s(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# main class\n",
        "\n",
        "class HourglassTransformerLM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        num_tokens,\n",
        "        dim,\n",
        "        max_seq_len,\n",
        "        depth,\n",
        "        shorten_factor = None,\n",
        "        heads = 8,\n",
        "        dim_head = 64,\n",
        "        attn_resampling = True,\n",
        "        updown_sample_type = 'naive',\n",
        "        causal = True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        self.token_emb = nn.Embedding(num_tokens, dim)\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, dim)\n",
        "\n",
        "        self.transformer = get_hourglass_transformer(\n",
        "            dim = dim,\n",
        "            depth = depth,\n",
        "            shorten_factor = shorten_factor,\n",
        "            attn_resampling = attn_resampling,\n",
        "            updown_sample_type = updown_sample_type,\n",
        "            dim_head = dim_head,\n",
        "            heads = heads,\n",
        "            causal = causal,\n",
        "            norm_out = True\n",
        "        )\n",
        "\n",
        "        self.to_logits = nn.Linear(dim, num_tokens)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        device = x.device\n",
        "        x = self.token_emb(x)\n",
        "        pos_emb = self.pos_emb(torch.arange(x.shape[-2], device = device))\n",
        "        x = x + rearrange(pos_emb, 'n d -> () n d')\n",
        "\n",
        "        x = self.transformer(x, mask = mask)\n",
        "        return self.to_logits(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features loading"
      ],
      "metadata": {
        "id": "dHM1Kjq_QgeZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1-o8U8KQ29k"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch import functional as F\n",
        "from torch import optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo-TetqIKAfw",
        "outputId": "d1d848db-7eb9-4a75-94cf-b660f52e43cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU:  Tesla K80\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models import resnet50\n",
        "import torch\n",
        "import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU: ', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU2AgW86HE3I"
      },
      "outputs": [],
      "source": [
        "#from torch.utils.data import Dataset\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    \n",
        "    \n",
        "    def __init__(self,cfg, annotation_file,data_type='train', \\\n",
        "                 transform=None):\n",
        "        \n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (string):  directory with images\n",
        "            annotation_file (string):  csv/txt file which has the \n",
        "                                        dataset labels\n",
        "            transforms: The trasforms to apply to images\n",
        "        \"\"\"\n",
        "        \n",
        "        self.data_path = os.path.join(cfg.data_path,cfg.imgs_dir)\n",
        "        self.label_path = os.path.join(cfg.data_path,cfg.labels_dir,annotation_file)\n",
        "        self.transform=transform\n",
        "        self.pretext = cfg.pretext\n",
        "        if self.pretext == 'rotation':\n",
        "            self.num_rot = cfg.num_rot\n",
        "        self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        '''\n",
        "        function to load the data in the format of [[img_name_1,label_1],\n",
        "        [img_name_2,label_2],.....[img_name_n,label_n]]\n",
        "        '''\n",
        "        self.labels = pd.read_csv(self.label_path)\n",
        "        \n",
        "        self.loaded_data = []\n",
        "#        self.read_data=[]\n",
        "        for i in range(self.labels.shape[0]):\n",
        "            img_name = self.labels['Filename'][i]#os.path.join(self.data_path, self.labels['Category'][i],self.labels['FileName'][i])\n",
        "            #print(img_name)\n",
        "            #data.append(io.imread(os.path.join(self.image_dir, self.labels['img_name'][i])))\n",
        "            label = self.labels['Label'][i]\n",
        "            img = Image.open(img_name)\n",
        "            img = img.convert('RGB')\n",
        "            self.loaded_data.append((img,label,img_name))\n",
        "            img.load()#This closes the image object or else you will get too many open file error\n",
        "#            self.read_data.append((img,label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.loaded_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        idx = idx % len(self.loaded_data)\n",
        "        img,label,img_name = self.loaded_data[idx]\n",
        "        img,label = self._read_data(img,label)\n",
        "        \n",
        "        return img,label\n",
        "\n",
        "    def _read_data(self,img,label):\n",
        "        \n",
        "            # supervised mode; if in supervised mode define a loader function \n",
        "            #that given the index of an image it returns the image and its \n",
        "            #categorical label\n",
        "        img = self.transform(img)\n",
        "        return img, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39PvR168LAbu"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "class dotdict(dict):\n",
        "   \n",
        "        __getattr__ = dict.get\n",
        "        __setattr__ = dict.__setitem__\n",
        "        __delattr__ = dict.__delitem__\n",
        "\n",
        "def load_yaml(config_file,config_type='dict'):\n",
        "    with open(config_file) as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "        #params = yaml.load(f,Loader=yaml.FullLoader)\n",
        "        \n",
        "    if config_type=='object':\n",
        "          cfg = dotdict(cfg)\n",
        "    return cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gjoq1bwNKDLi"
      },
      "outputs": [],
      "source": [
        "device = 'cpu'\n",
        "class ResNetFeatures(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(ResNetFeatures, self).__init__()\n",
        "\n",
        "        #encoder = get_model()\n",
        "\n",
        "        model = resnet50(pretrained=False)\n",
        "        model.fc = nn.Linear(in_features=model.fc.in_features,out_features=1000,bias=True)\n",
        "        encoder = model\n",
        "\n",
        "        Pkl_Filename = \"/content/drive/MyDrive/BYOL-ViT-Hourglass/BYOL/experiments/res50_cct100.pth\"  \n",
        "        \n",
        "        pretrained_path =  os.path.join('/content/drive/MyDrive/BYOL-ViT-Hourglass/BYOL/experiments/res50_cct100.pth')\n",
        "        state_dict = torch.load(pretrained_path,map_location=device)\n",
        "        #print(encoder)\n",
        "        encoder.load_state_dict(state_dict, strict=False)\n",
        "        #print(encoder)\n",
        "        encoder.to(device)\n",
        "        \n",
        "        self.feature_extractor = torch.nn.Sequential(*list(encoder.children())[:6]) \n",
        "        \n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, 3, 224, 224)\n",
        "\n",
        "        out = self.feature_extractor(inp)\n",
        "\n",
        "        # out: (batch_size, 64, 56, 56)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEvXWyACF3RS",
        "outputId": "636d787b-a329-426c-974a-6d650988cb18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train data load success\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "config_path = r'/content/drive/MyDrive/BYOL-ViT-Hourglass/BYOL-ViT/config_sl.yaml'\n",
        "cfg = load_yaml(config_path,config_type='object') \n",
        "annotation_file = 'stl.csv'\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "transform_2 = transforms.Compose([               \n",
        "                transforms.Resize((cfg.img_sz,cfg.img_sz)),\n",
        "                transforms.ToTensor(),                \n",
        "            ])\n",
        "\n",
        "\n",
        "train_dataset = Dataset(cfg,annotation_file,\n",
        "                            data_type='train',transform=transform_2)\n",
        "print('train data load success')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-g9tpNzF3DC"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataloader import default_collate\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "collate_func=default_collate\n",
        "\n",
        "dataset_size = len(train_dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(0.15 * dataset_size))\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloader_train = DataLoader(train_dataset, batch_size=cfg.batch_size, \n",
        "                                  collate_fn=collate_func,sampler=train_sampler)\n",
        "dataloader_val = DataLoader(train_dataset, batch_size=cfg.batch_size,\n",
        "                                  collate_fn=collate_func,sampler=valid_sampler)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hourglass training"
      ],
      "metadata": {
        "id": "NuAlxaARQkf-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pJLWaItp3TS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# from hourglass_transformer_pytorch import HourglassTransformer => we run modified hourglass above instead\n",
        "\n",
        "model = HourglassTransformer(\n",
        "    dim = 12*12,\n",
        "    shorten_factor = 2,\n",
        "    depth = (4, 2, 4),\n",
        "    updown_sample_type = 'linear'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnsYYQLmHFHb"
      },
      "outputs": [],
      "source": [
        "# TEST CELL\n",
        "\n",
        "resnet_features = ResNetFeatures()\n",
        "lr=0.01\n",
        "#loss_record = RunningAverage()\n",
        "model.train()\n",
        "classes=['airplane','bird','car','cat','gazelle','boat','dog','horse','monkey','truck']\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.2, weight_decay=0.05, nesterov=True)#0.2\n",
        "{device = 'cpu'\n",
        "for data, target in tqdm.tqdm(dataloader_train):\n",
        "     d=resnet_features(data).to(device)\n",
        "     for x, label in zip(d,target): # for batch\n",
        "       #for y in x:      # for channel\n",
        "        y=x\n",
        "        #y = torch.unsqueeze(y, dim=0)\n",
        "        y=torch.reshape(y,(1,512,12*12))\n",
        "        #print(' y.shape',y.shape)\n",
        "        \n",
        "        output=model(y).float() # (1, 1024, 512)\n",
        "        output = torch.unsqueeze(output, dim=0)\n",
        "        \n",
        "        label = torch.nn.functional.one_hot(label-1,num_classes=10)\n",
        "        label = torch.unsqueeze(label, dim=0)\n",
        "        #label = label.type(torch.float64)\n",
        "        #print(' label',label)\n",
        "        #print(' label',label.size())\n",
        "\n",
        "        #print(' output',output)\n",
        "        #print(' output.size()',output.size())\n",
        "\n",
        "        l = criterion(output, label.float())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        l.backward(retain_graph=True)\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yluy8hwKXgz7"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "logs=os.path.join('/content/drive/MyDrive/BYOL-ViT-Hourglass/logs') #tboard\n",
        "writer = SummaryWriter(logs + '/vit_resnet_pretrained')\n",
        "\n",
        "def train(epoch, model, device, dataloader, optimizer, criterion,writer):\n",
        "    \"\"\" Train loop, predict rotations. \"\"\"\n",
        "    global iter_cnt\n",
        "    #progbar = tqdm(total=len(dataloader), desc='Train')\n",
        "\n",
        "    loss_record = RunningAverage()\n",
        "    acc_record = RunningAverage()\n",
        "    correct=0\n",
        "    total = 0\n",
        "    model.train()\n",
        "    classes=['airplane','bird','car','cat','gazelle','boat','dog','horse','monkey','truck']\n",
        "\n",
        "    for data, target in tqdm.tqdm(dataloader_train):\n",
        "      d = resnet_features(data)\n",
        "      for x, label in zip(d.to(device), target.to(device)): # for batch     \n",
        "  \n",
        "          x=torch.reshape(x,(1,512,12*12))\n",
        "          output=model(x).float() # (1, 1024, 512)\n",
        "          output = torch.unsqueeze(output, dim=0)\n",
        "          \n",
        "          label = torch.nn.functional.one_hot(label-1,num_classes=10)\n",
        "          label = torch.unsqueeze(label, dim=0)\n",
        "\n",
        "          l = criterion(output, label.float())\n",
        "          \n",
        "          # measure accuracy and record loss\n",
        "          confidence, predicted = output.max(1)\n",
        "          correct += predicted.eq(label).sum().item()\n",
        "          #acc = utils.compute_acc(output, label)\n",
        "          total+=label.size(0)\n",
        "          acc = correct/total\n",
        "          \n",
        "          acc_record.update(100*acc)\n",
        "          loss_record.update(l.item())\n",
        "\n",
        "          iter_cnt+=1\n",
        "\n",
        "          # compute gradient and do optimizer step\n",
        "          optimizer.zero_grad()\n",
        "          l.backward(retain_graph=True)\n",
        "          optimizer.step()\n",
        "        \n",
        "          \n",
        "    LR=optimizer.param_groups[0]['lr']\n",
        "    writer.add_scalar('train/Loss_epoch', loss_record(), epoch)\n",
        "    writer.add_scalar('train/Acc_epoch', acc_record(), epoch)\n",
        "    \n",
        "    print('Train Epoch: {} LR: {:.4f} Avg Loss: {:.4f}; Avg Acc: {:.4f}'.format(epoch,LR, loss_record(), acc_record()))\n",
        "\n",
        "    return loss_record,acc_record\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TH0gvVLoZ9O"
      },
      "outputs": [],
      "source": [
        "def validate(epoch, model, device, dataloader, criterion,writer):\n",
        "    \"\"\" Test loop, print metrics \"\"\"\n",
        "    #progbar = tqdm(total=len(dataloader), desc='Val')\n",
        "\n",
        "    global iter_cnt\n",
        "    loss_record = RunningAverage()\n",
        "    acc_record = RunningAverage()\n",
        "    correct=0\n",
        "    total=0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for data, target in tqdm.tqdm(dataloader_train):\n",
        "          d = resnet_features(data)\n",
        "          for x, label in zip(d.to(device), target.to(device)): # for batch     \n",
        "              x=torch.reshape(x,(1,512,12*12))\n",
        "\n",
        "              output=model(x).float() # (1, 1024, 512)\n",
        "              output = torch.unsqueeze(output, dim=0)\n",
        "              \n",
        "              label = torch.nn.functional.one_hot(label-1,num_classes=10)\n",
        "              label = torch.unsqueeze(label, dim=0)\n",
        "\n",
        "              l = criterion(output, label.float())\n",
        "\n",
        "              # measure accuracy and record loss\n",
        "              acc = compute_acc(output, label)\n",
        "      #        acc_record.update(100 * acc[0].item())\n",
        "              acc_record.update(100*acc[0].item()/data.size(0))\n",
        "              loss_record.update(l.item())\n",
        "              #print('val Step: {}/{} Loss: {:.4f} \\t Acc: {:.4f}'.format(batch_idx,len(dataloader), loss_record(), acc_record()))\n",
        "              progbar.set_description('Val (loss=%.4f)' % (loss_record()))\n",
        "              progbar.update(1)\n",
        "\n",
        "\n",
        "    writer.add_scalar('validation/Loss_epoch', loss_record(), epoch)\n",
        "    writer.add_scalar('validation/Acc_epoch', acc_record(), epoch)\n",
        "    \n",
        "    return loss_record(),acc_record()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY0xdlwRtpBw"
      },
      "outputs": [],
      "source": [
        "class RunningAverage():\n",
        "    \"\"\"A simple class that maintains the running average of a quantity\n",
        "    \n",
        "    Example:\n",
        "    ```\n",
        "    loss_avg = RunningAverage()\n",
        "    loss_avg.update(2)\n",
        "    loss_avg.update(4)\n",
        "    loss_avg() = 3\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.steps = 0\n",
        "        self.total = 0\n",
        "    \n",
        "    def update(self, val):\n",
        "        self.total += val\n",
        "        self.steps += 1\n",
        "    \n",
        "    def __call__(self):\n",
        "        return self.total/float(self.steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzMV-YgJnB2k"
      },
      "outputs": [],
      "source": [
        "# main training cell\n",
        "\n",
        "logs=os.path.join('/content/drive/MyDrive/BYOL-ViT-Hourglass/BYOL/experiments/hourglass_log') #tboard\n",
        "\n",
        "global iter_cnt\n",
        "iter_cnt=0\n",
        "\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "val=0\n",
        "epochs = 600\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    train_loss,train_acc = train(epoch, model, device, dataloader_train, optimizer, criterion,writer)\n",
        "    val_loss,val_acc = validate(epoch, model, device, dataloader_val, criterion, writer)\n",
        "    \n",
        "    val=val+val_acc\n",
        "    #print(f\"Epoch : {epoch+1} - acc: {train_acc:.4f} - loss : {train_loss:.4f}\\n\")\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Saving model at epoch {epoch}\")\n",
        "        torch.save(model.state_dict(), f\"/content/drive/MyDrive/BYOL-ViT-Hourglass/BYOL/experiments/Hourglass_res50_cct{epoch}.pth\")\n",
        "\n",
        "\n",
        "\n",
        "vall=val/epochs\n",
        "writer.add_text('validation accuracy','acc {}'.format(val_acc))\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DMGPF2it9L2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BYOL-ViT-Hourglass.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}